{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70a95c2c",
   "metadata": {},
   "source": [
    "# 3DBall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8ba6fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "\n",
    "# Global Setting\n",
    "cur_dir = os.getcwd()\n",
    "env_dir = os.path.abspath(os.path.join(cur_dir, \"..\", \"Unity6000_Envs\"))\n",
    "output_dir = os.path.abspath(os.path.join(cur_dir, \"temp\", \"mlagents_learn_output\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a62e191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Unity6000_Envs/3DBall_Darwin.app\n"
     ]
    }
   ],
   "source": [
    "# Unity Enviroment\n",
    "game = \"3DBall\"\n",
    "os_name = platform.system()\n",
    "\n",
    "if os_name == 'Linux':\n",
    "    env_name = os.path.join(env_dir, f\"{game}_{os_name}.x86_64\")\n",
    "elif os_name == 'Darwin':\n",
    "    env_name = os.path.join(env_dir, f\"{game}_{os_name}.app\")\n",
    "env_fp = os.path.join(env_dir, env_name)\n",
    "print(env_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a25ba76",
   "metadata": {},
   "source": [
    "## Training PPO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "360531c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/config/3DBall_ppo.yaml\n",
      "3DBall_PPO\n",
      "\n",
      "            ┐  ╖\n",
      "        ╓╖╬│╡  ││╬╖╖\n",
      "    ╓╖╬│││││┘  ╬│││││╬╖\n",
      " ╖╬│││││╬╜        ╙╬│││││╖╖                               ╗╗╗\n",
      " ╬╬╬╬╖││╦╖        ╖╬││╗╣╣╣╬      ╟╣╣╬    ╟╣╣╣             ╜╜╜  ╟╣╣\n",
      " ╬╬╬╬╬╬╬╬╖│╬╖╖╓╬╪│╓╣╣╣╣╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╒╣╣╖╗╣╣╣╗   ╣╣╣ ╣╣╣╣╣╣ ╟╣╣╖   ╣╣╣\n",
      " ╬╬╬╬┐  ╙╬╬╬╬│╓╣╣╣╝╜  ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╣╙ ╙╣╣╣  ╣╣╣ ╙╟╣╣╜╙  ╫╣╣  ╟╣╣\n",
      " ╬╬╬╬┐     ╙╬╬╣╣      ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣     ╣╣╣┌╣╣╜\n",
      " ╬╬╬╜       ╬╬╣╣      ╙╝╣╣╬      ╙╣╣╣╗╖╓╗╣╣╣╜ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣╦╓    ╣╣╣╣╣\n",
      " ╙   ╓╦╖    ╬╬╣╣   ╓╗╗╖            ╙╝╣╣╣╣╝╜   ╘╝╝╜   ╝╝╝  ╝╝╝   ╙╣╣╣    ╟╣╣╣\n",
      "   ╩╬╬╬╬╬╬╦╦╬╬╣╣╗╣╣╣╣╣╣╣╝                                             ╫╣╣╣╣\n",
      "      ╙╬╬╬╬╬╬╬╣╣╣╣╣╣╝╜\n",
      "          ╙╬╬╬╣╣╣╜\n",
      "             ╙\n",
      "        \n",
      " Version information:\n",
      "  ml-agents: 1.1.0,\n",
      "  ml-agents-envs: 1.1.0,\n",
      "  Communicator API: 1.5.0,\n",
      "  PyTorch: 2.8.0\n",
      "[INFO] Connected to Unity environment with package version 4.0.0 and communication version 1.5.0\n",
      "[INFO] Connected new brain: 3DBall?team=0\n",
      "[INFO] Hyperparameters for behavior name 3DBall: \n",
      "\ttrainer_type:\tppo\n",
      "\thyperparameters:\t\n",
      "\t  batch_size:\t64\n",
      "\t  buffer_size:\t12000\n",
      "\t  learning_rate:\t0.0003\n",
      "\t  beta:\t0.001\n",
      "\t  epsilon:\t0.2\n",
      "\t  lambd:\t0.99\n",
      "\t  num_epoch:\t3\n",
      "\t  shared_critic:\tFalse\n",
      "\t  learning_rate_schedule:\tlinear\n",
      "\t  beta_schedule:\tlinear\n",
      "\t  epsilon_schedule:\tlinear\n",
      "\tcheckpoint_interval:\t500000\n",
      "\tnetwork_settings:\t\n",
      "\t  normalize:\tTrue\n",
      "\t  hidden_units:\t128\n",
      "\t  num_layers:\t2\n",
      "\t  vis_encode_type:\tsimple\n",
      "\t  memory:\tNone\n",
      "\t  goal_conditioning_type:\thyper\n",
      "\t  deterministic:\tFalse\n",
      "\treward_signals:\t\n",
      "\t  extrinsic:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t1.0\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t128\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\tinit_path:\tNone\n",
      "\tkeep_checkpoints:\t5\n",
      "\teven_checkpoints:\tFalse\n",
      "\tmax_steps:\t500000\n",
      "\ttime_horizon:\t1000\n",
      "\tsummary_freq:\t12000\n",
      "\tthreaded:\tFalse\n",
      "\tself_play:\tNone\n",
      "\tbehavioral_cloning:\tNone\n",
      "[INFO] 3DBall. Step: 12000. Time Elapsed: 11.775 s. Mean Reward: 1.143. Std of Reward: 0.731. Training.\n",
      "[INFO] 3DBall. Step: 24000. Time Elapsed: 20.216 s. Mean Reward: 1.280. Std of Reward: 0.757. Training.\n",
      "[INFO] 3DBall. Step: 36000. Time Elapsed: 28.108 s. Mean Reward: 2.022. Std of Reward: 1.235. Training.\n",
      "[INFO] 3DBall. Step: 48000. Time Elapsed: 36.028 s. Mean Reward: 2.906. Std of Reward: 2.022. Training.\n",
      "[INFO] 3DBall. Step: 60000. Time Elapsed: 43.615 s. Mean Reward: 5.503. Std of Reward: 5.441. Training.\n",
      "[INFO] 3DBall. Step: 72000. Time Elapsed: 51.313 s. Mean Reward: 12.433. Std of Reward: 12.636. Training.\n",
      "[INFO] 3DBall. Step: 84000. Time Elapsed: 59.111 s. Mean Reward: 35.769. Std of Reward: 32.441. Training.\n",
      "[INFO] 3DBall. Step: 96000. Time Elapsed: 66.982 s. Mean Reward: 71.988. Std of Reward: 30.965. Training.\n",
      "[INFO] 3DBall. Step: 108000. Time Elapsed: 73.677 s. Mean Reward: 97.858. Std of Reward: 7.103. Training.\n",
      "[INFO] 3DBall. Step: 120000. Time Elapsed: 80.794 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 132000. Time Elapsed: 87.583 s. Mean Reward: 97.258. Std of Reward: 9.093. Training.\n",
      "[INFO] 3DBall. Step: 144000. Time Elapsed: 95.111 s. Mean Reward: 93.538. Std of Reward: 22.383. Training.\n",
      "[INFO] 3DBall. Step: 156000. Time Elapsed: 102.127 s. Mean Reward: 95.408. Std of Reward: 15.229. Training.\n",
      "[INFO] 3DBall. Step: 168000. Time Elapsed: 109.277 s. Mean Reward: 92.777. Std of Reward: 25.021. Training.\n",
      "[INFO] 3DBall. Step: 180000. Time Elapsed: 116.477 s. Mean Reward: 94.692. Std of Reward: 18.386. Training.\n",
      "[INFO] 3DBall. Step: 192000. Time Elapsed: 123.919 s. Mean Reward: 94.715. Std of Reward: 18.306. Training.\n",
      "[INFO] 3DBall. Step: 204000. Time Elapsed: 131.018 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 216000. Time Elapsed: 137.843 s. Mean Reward: 89.577. Std of Reward: 27.275. Training.\n",
      "[INFO] 3DBall. Step: 228000. Time Elapsed: 144.893 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 240000. Time Elapsed: 152.044 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 252000. Time Elapsed: 159.226 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 264000. Time Elapsed: 166.293 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 276000. Time Elapsed: 173.393 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 288000. Time Elapsed: 179.042 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 300000. Time Elapsed: 186.043 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 312000. Time Elapsed: 193.895 s. Mean Reward: 96.285. Std of Reward: 12.870. Training.\n",
      "[INFO] 3DBall. Step: 324000. Time Elapsed: 199.893 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 336000. Time Elapsed: 206.676 s. Mean Reward: 94.383. Std of Reward: 18.628. Training.\n",
      "[INFO] 3DBall. Step: 348000. Time Elapsed: 213.659 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 360000. Time Elapsed: 220.676 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 372000. Time Elapsed: 227.544 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 384000. Time Elapsed: 234.359 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 396000. Time Elapsed: 241.376 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 408000. Time Elapsed: 248.159 s. Mean Reward: 97.450. Std of Reward: 8.457. Training.\n",
      "[INFO] 3DBall. Step: 420000. Time Elapsed: 255.076 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 432000. Time Elapsed: 261.975 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 444000. Time Elapsed: 268.809 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 456000. Time Elapsed: 274.308 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 468000. Time Elapsed: 281.142 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 480000. Time Elapsed: 288.025 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 492000. Time Elapsed: 294.859 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/3DBall_PPO/3DBall/3DBall-499017.onnx\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/3DBall_PPO/3DBall/3DBall-500017.onnx\n",
      "[INFO] Copied /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/3DBall_PPO/3DBall/3DBall-500017.onnx to /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/3DBall_PPO/3DBall.onnx.\n"
     ]
    }
   ],
   "source": [
    "config_ppo_fp = os.path.join(cur_dir, \"config\", \"3DBall_ppo.yaml\")\n",
    "run_ppo_id = \"3DBall_PPO\"\n",
    "print(config_ppo_fp)\n",
    "print(run_ppo_id)\n",
    "\n",
    "!mlagents-learn $config_ppo_fp \\\n",
    "               --env=$env_fp \\\n",
    "               --results-dir=$output_dir \\\n",
    "               --run-id=$run_ppo_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c6d53a",
   "metadata": {},
   "source": [
    "## Training PPO with randomization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b2a5c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/config/3DBall_randomize_ppo.yaml\n",
      "3DBall_RND_PPO\n",
      "\n",
      "            ┐  ╖\n",
      "        ╓╖╬│╡  ││╬╖╖\n",
      "    ╓╖╬│││││┘  ╬│││││╬╖\n",
      " ╖╬│││││╬╜        ╙╬│││││╖╖                               ╗╗╗\n",
      " ╬╬╬╬╖││╦╖        ╖╬││╗╣╣╣╬      ╟╣╣╬    ╟╣╣╣             ╜╜╜  ╟╣╣\n",
      " ╬╬╬╬╬╬╬╬╖│╬╖╖╓╬╪│╓╣╣╣╣╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╒╣╣╖╗╣╣╣╗   ╣╣╣ ╣╣╣╣╣╣ ╟╣╣╖   ╣╣╣\n",
      " ╬╬╬╬┐  ╙╬╬╬╬│╓╣╣╣╝╜  ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╣╙ ╙╣╣╣  ╣╣╣ ╙╟╣╣╜╙  ╫╣╣  ╟╣╣\n",
      " ╬╬╬╬┐     ╙╬╬╣╣      ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣     ╣╣╣┌╣╣╜\n",
      " ╬╬╬╜       ╬╬╣╣      ╙╝╣╣╬      ╙╣╣╣╗╖╓╗╣╣╣╜ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣╦╓    ╣╣╣╣╣\n",
      " ╙   ╓╦╖    ╬╬╣╣   ╓╗╗╖            ╙╝╣╣╣╣╝╜   ╘╝╝╜   ╝╝╝  ╝╝╝   ╙╣╣╣    ╟╣╣╣\n",
      "   ╩╬╬╬╬╬╬╦╦╬╬╣╣╗╣╣╣╣╣╣╣╝                                             ╫╣╣╣╣\n",
      "      ╙╬╬╬╬╬╬╬╣╣╣╣╣╣╝╜\n",
      "          ╙╬╬╬╣╣╣╜\n",
      "             ╙\n",
      "        \n",
      " Version information:\n",
      "  ml-agents: 1.1.0,\n",
      "  ml-agents-envs: 1.1.0,\n",
      "  Communicator API: 1.5.0,\n",
      "  PyTorch: 2.8.0\n",
      "[INFO] Connected to Unity environment with package version 4.0.0 and communication version 1.5.0\n",
      "[INFO] Connected new brain: 3DBall?team=0\n",
      "[INFO] Hyperparameters for behavior name 3DBall: \n",
      "\ttrainer_type:\tppo\n",
      "\thyperparameters:\t\n",
      "\t  batch_size:\t64\n",
      "\t  buffer_size:\t12000\n",
      "\t  learning_rate:\t0.0003\n",
      "\t  beta:\t0.001\n",
      "\t  epsilon:\t0.2\n",
      "\t  lambd:\t0.99\n",
      "\t  num_epoch:\t3\n",
      "\t  shared_critic:\tFalse\n",
      "\t  learning_rate_schedule:\tlinear\n",
      "\t  beta_schedule:\tlinear\n",
      "\t  epsilon_schedule:\tlinear\n",
      "\tcheckpoint_interval:\t500000\n",
      "\tnetwork_settings:\t\n",
      "\t  normalize:\tTrue\n",
      "\t  hidden_units:\t128\n",
      "\t  num_layers:\t2\n",
      "\t  vis_encode_type:\tsimple\n",
      "\t  memory:\tNone\n",
      "\t  goal_conditioning_type:\thyper\n",
      "\t  deterministic:\tFalse\n",
      "\treward_signals:\t\n",
      "\t  extrinsic:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t1.0\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t128\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\tinit_path:\tNone\n",
      "\tkeep_checkpoints:\t5\n",
      "\teven_checkpoints:\tFalse\n",
      "\tmax_steps:\t500000\n",
      "\ttime_horizon:\t1000\n",
      "\tsummary_freq:\t12000\n",
      "\tthreaded:\tFalse\n",
      "\tself_play:\tNone\n",
      "\tbehavioral_cloning:\tNone\n",
      "[INFO] Parameter 'mass' is in lesson 'mass' and has value 'Uniform sampler: min=0.5, max=10.0'.\n",
      "[INFO] Parameter 'scale' is in lesson 'scale' and has value 'Uniform sampler: min=0.75, max=3.0'.\n",
      "[INFO] 3DBall. Step: 12000. Time Elapsed: 12.451 s. Mean Reward: 1.053. Std of Reward: 0.799. Training.\n",
      "[INFO] 3DBall. Step: 24000. Time Elapsed: 21.430 s. Mean Reward: 1.152. Std of Reward: 0.866. Training.\n",
      "[INFO] 3DBall. Step: 36000. Time Elapsed: 29.997 s. Mean Reward: 1.821. Std of Reward: 1.377. Training.\n",
      "[INFO] 3DBall. Step: 48000. Time Elapsed: 38.097 s. Mean Reward: 3.101. Std of Reward: 2.612. Training.\n",
      "[INFO] 3DBall. Step: 60000. Time Elapsed: 46.089 s. Mean Reward: 5.264. Std of Reward: 4.702. Training.\n",
      "[INFO] 3DBall. Step: 72000. Time Elapsed: 54.989 s. Mean Reward: 12.062. Std of Reward: 13.826. Training.\n",
      "[INFO] 3DBall. Step: 84000. Time Elapsed: 62.870 s. Mean Reward: 52.905. Std of Reward: 35.494. Training.\n",
      "[INFO] 3DBall. Step: 96000. Time Elapsed: 70.170 s. Mean Reward: 53.573. Std of Reward: 39.093. Training.\n",
      "[INFO] 3DBall. Step: 108000. Time Elapsed: 77.278 s. Mean Reward: 69.082. Std of Reward: 40.975. Training.\n",
      "[INFO] 3DBall. Step: 120000. Time Elapsed: 84.428 s. Mean Reward: 74.594. Std of Reward: 38.461. Training.\n",
      "[INFO] 3DBall. Step: 132000. Time Elapsed: 91.553 s. Mean Reward: 79.273. Std of Reward: 34.470. Training.\n",
      "[INFO] 3DBall. Step: 144000. Time Elapsed: 98.745 s. Mean Reward: 95.762. Std of Reward: 14.682. Training.\n",
      "[INFO] 3DBall. Step: 156000. Time Elapsed: 105.929 s. Mean Reward: 96.283. Std of Reward: 12.327. Training.\n",
      "[INFO] 3DBall. Step: 168000. Time Elapsed: 113.240 s. Mean Reward: 97.169. Std of Reward: 9.806. Training.\n",
      "[INFO] 3DBall. Step: 180000. Time Elapsed: 120.407 s. Mean Reward: 96.200. Std of Reward: 12.603. Training.\n",
      "[INFO] 3DBall. Step: 192000. Time Elapsed: 127.640 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 204000. Time Elapsed: 134.924 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 216000. Time Elapsed: 142.123 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 228000. Time Elapsed: 149.273 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 240000. Time Elapsed: 156.372 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 252000. Time Elapsed: 163.747 s. Mean Reward: 88.714. Std of Reward: 28.669. Training.\n",
      "[INFO] 3DBall. Step: 264000. Time Elapsed: 170.947 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 276000. Time Elapsed: 178.115 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 288000. Time Elapsed: 183.890 s. Mean Reward: 84.885. Std of Reward: 35.449. Training.\n",
      "[INFO] 3DBall. Step: 300000. Time Elapsed: 190.906 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 312000. Time Elapsed: 198.006 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 324000. Time Elapsed: 205.222 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 336000. Time Elapsed: 212.322 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 348000. Time Elapsed: 219.506 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 360000. Time Elapsed: 226.655 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 372000. Time Elapsed: 233.888 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 384000. Time Elapsed: 241.039 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 396000. Time Elapsed: 248.156 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 408000. Time Elapsed: 255.371 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 420000. Time Elapsed: 262.539 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 432000. Time Elapsed: 269.472 s. Mean Reward: 93.746. Std of Reward: 21.664. Training.\n",
      "[INFO] 3DBall. Step: 444000. Time Elapsed: 276.672 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 456000. Time Elapsed: 282.454 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 468000. Time Elapsed: 289.688 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 480000. Time Elapsed: 296.946 s. Mean Reward: 92.454. Std of Reward: 26.141. Training.\n",
      "[INFO] 3DBall. Step: 492000. Time Elapsed: 303.896 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/3DBall_RND_PPO/3DBall/3DBall-499354.onnx\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/3DBall_RND_PPO/3DBall/3DBall-500354.onnx\n",
      "[INFO] Copied /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/3DBall_RND_PPO/3DBall/3DBall-500354.onnx to /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/3DBall_RND_PPO/3DBall.onnx.\n"
     ]
    }
   ],
   "source": [
    "config_ppo_fp = os.path.join(cur_dir, \"config\", \"3DBall_randomize_ppo.yaml\")\n",
    "run_ppo_id = \"3DBall_RND_PPO\"\n",
    "print(config_ppo_fp)\n",
    "print(run_ppo_id)\n",
    "\n",
    "!mlagents-learn $config_ppo_fp \\\n",
    "               --env=$env_fp \\\n",
    "               --results-dir=$output_dir \\\n",
    "               --run-id=$run_ppo_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d8c3c8",
   "metadata": {},
   "source": [
    "## Training SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "590578c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/config/3DBall_randomize_ppo.yaml\n",
      "3DBall_SAC\n",
      "\n",
      "            ┐  ╖\n",
      "        ╓╖╬│╡  ││╬╖╖\n",
      "    ╓╖╬│││││┘  ╬│││││╬╖\n",
      " ╖╬│││││╬╜        ╙╬│││││╖╖                               ╗╗╗\n",
      " ╬╬╬╬╖││╦╖        ╖╬││╗╣╣╣╬      ╟╣╣╬    ╟╣╣╣             ╜╜╜  ╟╣╣\n",
      " ╬╬╬╬╬╬╬╬╖│╬╖╖╓╬╪│╓╣╣╣╣╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╒╣╣╖╗╣╣╣╗   ╣╣╣ ╣╣╣╣╣╣ ╟╣╣╖   ╣╣╣\n",
      " ╬╬╬╬┐  ╙╬╬╬╬│╓╣╣╣╝╜  ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╣╙ ╙╣╣╣  ╣╣╣ ╙╟╣╣╜╙  ╫╣╣  ╟╣╣\n",
      " ╬╬╬╬┐     ╙╬╬╣╣      ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣     ╣╣╣┌╣╣╜\n",
      " ╬╬╬╜       ╬╬╣╣      ╙╝╣╣╬      ╙╣╣╣╗╖╓╗╣╣╣╜ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣╦╓    ╣╣╣╣╣\n",
      " ╙   ╓╦╖    ╬╬╣╣   ╓╗╗╖            ╙╝╣╣╣╣╝╜   ╘╝╝╜   ╝╝╝  ╝╝╝   ╙╣╣╣    ╟╣╣╣\n",
      "   ╩╬╬╬╬╬╬╦╦╬╬╣╣╗╣╣╣╣╣╣╣╝                                             ╫╣╣╣╣\n",
      "      ╙╬╬╬╬╬╬╬╣╣╣╣╣╣╝╜\n",
      "          ╙╬╬╬╣╣╣╜\n",
      "             ╙\n",
      "        \n",
      " Version information:\n",
      "  ml-agents: 1.1.0,\n",
      "  ml-agents-envs: 1.1.0,\n",
      "  Communicator API: 1.5.0,\n",
      "  PyTorch: 2.8.0\n",
      "[INFO] Connected to Unity environment with package version 4.0.0 and communication version 1.5.0\n",
      "[INFO] Connected new brain: 3DBall?team=0\n",
      "[INFO] Hyperparameters for behavior name 3DBall: \n",
      "\ttrainer_type:\tppo\n",
      "\thyperparameters:\t\n",
      "\t  batch_size:\t64\n",
      "\t  buffer_size:\t12000\n",
      "\t  learning_rate:\t0.0003\n",
      "\t  beta:\t0.001\n",
      "\t  epsilon:\t0.2\n",
      "\t  lambd:\t0.99\n",
      "\t  num_epoch:\t3\n",
      "\t  shared_critic:\tFalse\n",
      "\t  learning_rate_schedule:\tlinear\n",
      "\t  beta_schedule:\tlinear\n",
      "\t  epsilon_schedule:\tlinear\n",
      "\tcheckpoint_interval:\t500000\n",
      "\tnetwork_settings:\t\n",
      "\t  normalize:\tTrue\n",
      "\t  hidden_units:\t128\n",
      "\t  num_layers:\t2\n",
      "\t  vis_encode_type:\tsimple\n",
      "\t  memory:\tNone\n",
      "\t  goal_conditioning_type:\thyper\n",
      "\t  deterministic:\tFalse\n",
      "\treward_signals:\t\n",
      "\t  extrinsic:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t1.0\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t128\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\tinit_path:\tNone\n",
      "\tkeep_checkpoints:\t5\n",
      "\teven_checkpoints:\tFalse\n",
      "\tmax_steps:\t500000\n",
      "\ttime_horizon:\t1000\n",
      "\tsummary_freq:\t12000\n",
      "\tthreaded:\tFalse\n",
      "\tself_play:\tNone\n",
      "\tbehavioral_cloning:\tNone\n",
      "[INFO] Parameter 'mass' is in lesson 'mass' and has value 'Uniform sampler: min=0.5, max=10.0'.\n",
      "[INFO] Parameter 'scale' is in lesson 'scale' and has value 'Uniform sampler: min=0.75, max=3.0'.\n",
      "[INFO] 3DBall. Step: 12000. Time Elapsed: 11.707 s. Mean Reward: 1.072. Std of Reward: 0.757. Training.\n",
      "[INFO] 3DBall. Step: 24000. Time Elapsed: 20.428 s. Mean Reward: 1.310. Std of Reward: 0.950. Training.\n",
      "[INFO] 3DBall. Step: 36000. Time Elapsed: 28.549 s. Mean Reward: 1.749. Std of Reward: 1.303. Training.\n",
      "[INFO] 3DBall. Step: 48000. Time Elapsed: 36.665 s. Mean Reward: 2.710. Std of Reward: 2.168. Training.\n",
      "[INFO] 3DBall. Step: 60000. Time Elapsed: 44.310 s. Mean Reward: 5.106. Std of Reward: 4.699. Training.\n",
      "[INFO] 3DBall. Step: 72000. Time Elapsed: 52.658 s. Mean Reward: 16.013. Std of Reward: 15.810. Training.\n",
      "[INFO] 3DBall. Step: 84000. Time Elapsed: 59.607 s. Mean Reward: 54.257. Std of Reward: 33.182. Training.\n",
      "[INFO] 3DBall. Step: 96000. Time Elapsed: 67.999 s. Mean Reward: 70.171. Std of Reward: 36.789. Training.\n",
      "[INFO] 3DBall. Step: 108000. Time Elapsed: 74.832 s. Mean Reward: 89.546. Std of Reward: 23.975. Training.\n",
      "[INFO] 3DBall. Step: 120000. Time Elapsed: 81.826 s. Mean Reward: 91.136. Std of Reward: 25.940. Training.\n",
      "[INFO] 3DBall. Step: 132000. Time Elapsed: 88.693 s. Mean Reward: 96.092. Std of Reward: 12.962. Training.\n",
      "[INFO] 3DBall. Step: 144000. Time Elapsed: 95.643 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 156000. Time Elapsed: 102.448 s. Mean Reward: 97.500. Std of Reward: 8.292. Training.\n",
      "[INFO] 3DBall. Step: 168000. Time Elapsed: 109.315 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 180000. Time Elapsed: 116.276 s. Mean Reward: 96.323. Std of Reward: 12.737. Training.\n",
      "[INFO] 3DBall. Step: 192000. Time Elapsed: 123.242 s. Mean Reward: 85.386. Std of Reward: 29.082. Training.\n",
      "[INFO] 3DBall. Step: 204000. Time Elapsed: 130.192 s. Mean Reward: 93.950. Std of Reward: 20.066. Training.\n",
      "[INFO] 3DBall. Step: 216000. Time Elapsed: 137.209 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 228000. Time Elapsed: 144.258 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 240000. Time Elapsed: 151.225 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 252000. Time Elapsed: 158.125 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 264000. Time Elapsed: 165.142 s. Mean Reward: 99.817. Std of Reward: 0.608. Training.\n",
      "[INFO] 3DBall. Step: 276000. Time Elapsed: 171.958 s. Mean Reward: 92.808. Std of Reward: 24.915. Training.\n",
      "[INFO] 3DBall. Step: 288000. Time Elapsed: 179.042 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 300000. Time Elapsed: 184.724 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 312000. Time Elapsed: 191.808 s. Mean Reward: 92.692. Std of Reward: 25.315. Training.\n",
      "[INFO] 3DBall. Step: 324000. Time Elapsed: 198.757 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 336000. Time Elapsed: 205.824 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 348000. Time Elapsed: 212.891 s. Mean Reward: 90.500. Std of Reward: 26.208. Training.\n",
      "[INFO] 3DBall. Step: 360000. Time Elapsed: 219.994 s. Mean Reward: 92.392. Std of Reward: 26.354. Training.\n",
      "[INFO] 3DBall. Step: 372000. Time Elapsed: 226.909 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 384000. Time Elapsed: 234.060 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 396000. Time Elapsed: 241.027 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 408000. Time Elapsed: 248.057 s. Mean Reward: 98.325. Std of Reward: 5.555. Training.\n",
      "[INFO] 3DBall. Step: 420000. Time Elapsed: 255.109 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 432000. Time Elapsed: 262.040 s. Mean Reward: 92.508. Std of Reward: 25.954. Training.\n",
      "[INFO] 3DBall. Step: 444000. Time Elapsed: 269.124 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 456000. Time Elapsed: 276.057 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 468000. Time Elapsed: 282.924 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 480000. Time Elapsed: 289.958 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 492000. Time Elapsed: 295.522 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/3DBall_SAC/3DBall/3DBall-499731.onnx\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/3DBall_SAC/3DBall/3DBall-500731.onnx\n",
      "[INFO] Copied /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/3DBall_SAC/3DBall/3DBall-500731.onnx to /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/3DBall_SAC/3DBall.onnx.\n"
     ]
    }
   ],
   "source": [
    "config_sac_fp = os.path.join(cur_dir, \"config\", \"3DBall_sac.yaml\")\n",
    "run_sac_id = \"3DBall_SAC\"\n",
    "print(config_ppo_fp)\n",
    "print(run_sac_id)\n",
    "\n",
    "!mlagents-learn $config_ppo_fp \\\n",
    "               --env=$env_fp \\\n",
    "               --results-dir=$output_dir \\\n",
    "               --run-id=$run_sac_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e00cf00",
   "metadata": {},
   "source": [
    "## Training SAC with randomization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "276170ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/config/3DBall_randomize_ppo.yaml\n",
      "3DBall_RND_SAC\n",
      "\n",
      "            ┐  ╖\n",
      "        ╓╖╬│╡  ││╬╖╖\n",
      "    ╓╖╬│││││┘  ╬│││││╬╖\n",
      " ╖╬│││││╬╜        ╙╬│││││╖╖                               ╗╗╗\n",
      " ╬╬╬╬╖││╦╖        ╖╬││╗╣╣╣╬      ╟╣╣╬    ╟╣╣╣             ╜╜╜  ╟╣╣\n",
      " ╬╬╬╬╬╬╬╬╖│╬╖╖╓╬╪│╓╣╣╣╣╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╒╣╣╖╗╣╣╣╗   ╣╣╣ ╣╣╣╣╣╣ ╟╣╣╖   ╣╣╣\n",
      " ╬╬╬╬┐  ╙╬╬╬╬│╓╣╣╣╝╜  ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╣╙ ╙╣╣╣  ╣╣╣ ╙╟╣╣╜╙  ╫╣╣  ╟╣╣\n",
      " ╬╬╬╬┐     ╙╬╬╣╣      ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣     ╣╣╣┌╣╣╜\n",
      " ╬╬╬╜       ╬╬╣╣      ╙╝╣╣╬      ╙╣╣╣╗╖╓╗╣╣╣╜ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣╦╓    ╣╣╣╣╣\n",
      " ╙   ╓╦╖    ╬╬╣╣   ╓╗╗╖            ╙╝╣╣╣╣╝╜   ╘╝╝╜   ╝╝╝  ╝╝╝   ╙╣╣╣    ╟╣╣╣\n",
      "   ╩╬╬╬╬╬╬╦╦╬╬╣╣╗╣╣╣╣╣╣╣╝                                             ╫╣╣╣╣\n",
      "      ╙╬╬╬╬╬╬╬╣╣╣╣╣╣╝╜\n",
      "          ╙╬╬╬╣╣╣╜\n",
      "             ╙\n",
      "        \n",
      " Version information:\n",
      "  ml-agents: 1.1.0,\n",
      "  ml-agents-envs: 1.1.0,\n",
      "  Communicator API: 1.5.0,\n",
      "  PyTorch: 2.8.0\n",
      "[INFO] Connected to Unity environment with package version 4.0.0 and communication version 1.5.0\n",
      "[INFO] Connected new brain: 3DBall?team=0\n",
      "[INFO] Hyperparameters for behavior name 3DBall: \n",
      "\ttrainer_type:\tppo\n",
      "\thyperparameters:\t\n",
      "\t  batch_size:\t64\n",
      "\t  buffer_size:\t12000\n",
      "\t  learning_rate:\t0.0003\n",
      "\t  beta:\t0.001\n",
      "\t  epsilon:\t0.2\n",
      "\t  lambd:\t0.99\n",
      "\t  num_epoch:\t3\n",
      "\t  shared_critic:\tFalse\n",
      "\t  learning_rate_schedule:\tlinear\n",
      "\t  beta_schedule:\tlinear\n",
      "\t  epsilon_schedule:\tlinear\n",
      "\tcheckpoint_interval:\t500000\n",
      "\tnetwork_settings:\t\n",
      "\t  normalize:\tTrue\n",
      "\t  hidden_units:\t128\n",
      "\t  num_layers:\t2\n",
      "\t  vis_encode_type:\tsimple\n",
      "\t  memory:\tNone\n",
      "\t  goal_conditioning_type:\thyper\n",
      "\t  deterministic:\tFalse\n",
      "\treward_signals:\t\n",
      "\t  extrinsic:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t1.0\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t128\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\tinit_path:\tNone\n",
      "\tkeep_checkpoints:\t5\n",
      "\teven_checkpoints:\tFalse\n",
      "\tmax_steps:\t500000\n",
      "\ttime_horizon:\t1000\n",
      "\tsummary_freq:\t12000\n",
      "\tthreaded:\tFalse\n",
      "\tself_play:\tNone\n",
      "\tbehavioral_cloning:\tNone\n",
      "[INFO] Parameter 'mass' is in lesson 'mass' and has value 'Uniform sampler: min=0.5, max=10.0'.\n",
      "[INFO] Parameter 'scale' is in lesson 'scale' and has value 'Uniform sampler: min=0.75, max=3.0'.\n",
      "[INFO] 3DBall. Step: 12000. Time Elapsed: 11.776 s. Mean Reward: 1.060. Std of Reward: 0.739. Training.\n",
      "[INFO] 3DBall. Step: 24000. Time Elapsed: 20.460 s. Mean Reward: 1.310. Std of Reward: 0.971. Training.\n",
      "[INFO] 3DBall. Step: 36000. Time Elapsed: 28.825 s. Mean Reward: 1.836. Std of Reward: 1.338. Training.\n",
      "[INFO] 3DBall. Step: 48000. Time Elapsed: 36.947 s. Mean Reward: 2.995. Std of Reward: 2.626. Training.\n",
      "[INFO] 3DBall. Step: 60000. Time Elapsed: 44.880 s. Mean Reward: 5.789. Std of Reward: 5.713. Training.\n",
      "[INFO] 3DBall. Step: 72000. Time Elapsed: 53.402 s. Mean Reward: 13.320. Std of Reward: 13.205. Training.\n",
      "[INFO] 3DBall. Step: 84000. Time Elapsed: 60.879 s. Mean Reward: 40.700. Std of Reward: 36.694. Training.\n",
      "[INFO] 3DBall. Step: 96000. Time Elapsed: 68.519 s. Mean Reward: 75.875. Std of Reward: 34.715. Training.\n",
      "[INFO] 3DBall. Step: 108000. Time Elapsed: 75.519 s. Mean Reward: 90.931. Std of Reward: 25.017. Training.\n",
      "[INFO] 3DBall. Step: 120000. Time Elapsed: 81.845 s. Mean Reward: 95.692. Std of Reward: 9.782. Training.\n",
      "[INFO] 3DBall. Step: 132000. Time Elapsed: 88.862 s. Mean Reward: 77.987. Std of Reward: 37.015. Training.\n",
      "[INFO] 3DBall. Step: 144000. Time Elapsed: 95.778 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 156000. Time Elapsed: 102.928 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 168000. Time Elapsed: 109.845 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 180000. Time Elapsed: 116.894 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 192000. Time Elapsed: 123.961 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 204000. Time Elapsed: 131.061 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 216000. Time Elapsed: 138.161 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 228000. Time Elapsed: 145.145 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 240000. Time Elapsed: 150.844 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 252000. Time Elapsed: 157.911 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 264000. Time Elapsed: 164.927 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 276000. Time Elapsed: 171.977 s. Mean Reward: 92.462. Std of Reward: 26.114. Training.\n",
      "[INFO] 3DBall. Step: 288000. Time Elapsed: 179.665 s. Mean Reward: 91.879. Std of Reward: 24.748. Training.\n",
      "[INFO] 3DBall. Step: 300000. Time Elapsed: 186.748 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 312000. Time Elapsed: 193.664 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 324000. Time Elapsed: 200.614 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 336000. Time Elapsed: 207.581 s. Mean Reward: 85.786. Std of Reward: 34.818. Training.\n",
      "[INFO] 3DBall. Step: 348000. Time Elapsed: 214.530 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 360000. Time Elapsed: 220.626 s. Mean Reward: 95.250. Std of Reward: 15.754. Training.\n",
      "[INFO] 3DBall. Step: 372000. Time Elapsed: 227.560 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 384000. Time Elapsed: 234.501 s. Mean Reward: 92.362. Std of Reward: 26.460. Training.\n",
      "[INFO] 3DBall. Step: 396000. Time Elapsed: 241.535 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 408000. Time Elapsed: 248.434 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 420000. Time Elapsed: 255.534 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 432000. Time Elapsed: 262.484 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 444000. Time Elapsed: 268.117 s. Mean Reward: 97.617. Std of Reward: 7.905. Training.\n",
      "[INFO] 3DBall. Step: 456000. Time Elapsed: 275.067 s. Mean Reward: 92.546. Std of Reward: 25.821. Training.\n",
      "[INFO] 3DBall. Step: 468000. Time Elapsed: 282.050 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 480000. Time Elapsed: 289.034 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 492000. Time Elapsed: 296.017 s. Mean Reward: 100.000. Std of Reward: 0.000. Training.\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/3DBall_RND_SAC/3DBall/3DBall-499212.onnx\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/3DBall_RND_SAC/3DBall/3DBall-500212.onnx\n",
      "[INFO] Copied /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/3DBall_RND_SAC/3DBall/3DBall-500212.onnx to /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/3DBall_RND_SAC/3DBall.onnx.\n"
     ]
    }
   ],
   "source": [
    "config_sac_fp = os.path.join(cur_dir, \"config\", \"3DBall_randomize_sac.yaml\")\n",
    "run_sac_id = \"3DBall_RND_SAC\"\n",
    "print(config_ppo_fp)\n",
    "print(run_sac_id)\n",
    "\n",
    "!mlagents-learn $config_ppo_fp \\\n",
    "               --env=$env_fp \\\n",
    "               --results-dir=$output_dir \\\n",
    "               --run-id=$run_sac_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a47b032",
   "metadata": {},
   "source": [
    "## Training POCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6886efe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/config/3DBall_poca.yaml\n",
      "3DBall_POCA\n",
      "\n",
      "            ┐  ╖\n",
      "        ╓╖╬│╡  ││╬╖╖\n",
      "    ╓╖╬│││││┘  ╬│││││╬╖\n",
      " ╖╬│││││╬╜        ╙╬│││││╖╖                               ╗╗╗\n",
      " ╬╬╬╬╖││╦╖        ╖╬││╗╣╣╣╬      ╟╣╣╬    ╟╣╣╣             ╜╜╜  ╟╣╣\n",
      " ╬╬╬╬╬╬╬╬╖│╬╖╖╓╬╪│╓╣╣╣╣╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╒╣╣╖╗╣╣╣╗   ╣╣╣ ╣╣╣╣╣╣ ╟╣╣╖   ╣╣╣\n",
      " ╬╬╬╬┐  ╙╬╬╬╬│╓╣╣╣╝╜  ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╣╙ ╙╣╣╣  ╣╣╣ ╙╟╣╣╜╙  ╫╣╣  ╟╣╣\n",
      " ╬╬╬╬┐     ╙╬╬╣╣      ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣     ╣╣╣┌╣╣╜\n",
      " ╬╬╬╜       ╬╬╣╣      ╙╝╣╣╬      ╙╣╣╣╗╖╓╗╣╣╣╜ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣╦╓    ╣╣╣╣╣\n",
      " ╙   ╓╦╖    ╬╬╣╣   ╓╗╗╖            ╙╝╣╣╣╣╝╜   ╘╝╝╜   ╝╝╝  ╝╝╝   ╙╣╣╣    ╟╣╣╣\n",
      "   ╩╬╬╬╬╬╬╦╦╬╬╣╣╗╣╣╣╣╣╣╣╝                                             ╫╣╣╣╣\n",
      "      ╙╬╬╬╬╬╬╬╣╣╣╣╣╣╝╜\n",
      "          ╙╬╬╬╣╣╣╜\n",
      "             ╙\n",
      "        \n",
      " Version information:\n",
      "  ml-agents: 1.1.0,\n",
      "  ml-agents-envs: 1.1.0,\n",
      "  Communicator API: 1.5.0,\n",
      "  PyTorch: 2.8.0\n",
      "[INFO] Connected to Unity environment with package version 4.0.0 and communication version 1.5.0\n",
      "[INFO] Connected new brain: 3DBall?team=0\n",
      "[INFO] Hyperparameters for behavior name 3DBall: \n",
      "\ttrainer_type:\tpoca\n",
      "\thyperparameters:\t\n",
      "\t  batch_size:\t64\n",
      "\t  buffer_size:\t12000\n",
      "\t  learning_rate:\t0.0003\n",
      "\t  beta:\t0.001\n",
      "\t  epsilon:\t0.2\n",
      "\t  lambd:\t0.99\n",
      "\t  num_epoch:\t3\n",
      "\t  learning_rate_schedule:\tlinear\n",
      "\t  beta_schedule:\tlinear\n",
      "\t  epsilon_schedule:\tlinear\n",
      "\tcheckpoint_interval:\t500000\n",
      "\tnetwork_settings:\t\n",
      "\t  normalize:\tTrue\n",
      "\t  hidden_units:\t128\n",
      "\t  num_layers:\t2\n",
      "\t  vis_encode_type:\tsimple\n",
      "\t  memory:\tNone\n",
      "\t  goal_conditioning_type:\thyper\n",
      "\t  deterministic:\tFalse\n",
      "\treward_signals:\t\n",
      "\t  extrinsic:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t1.0\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t128\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\tinit_path:\tNone\n",
      "\tkeep_checkpoints:\t5\n",
      "\teven_checkpoints:\tFalse\n",
      "\tmax_steps:\t500000\n",
      "\ttime_horizon:\t1000\n",
      "\tsummary_freq:\t12000\n",
      "\tthreaded:\tFalse\n",
      "\tself_play:\tNone\n",
      "\tbehavioral_cloning:\tNone\n",
      "[INFO] 3DBall. Step: 12000. Time Elapsed: 12.572 s. Mean Reward: 1.192. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 24000. Time Elapsed: 23.382 s. Mean Reward: 1.395. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 36000. Time Elapsed: 33.621 s. Mean Reward: 1.991. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 48000. Time Elapsed: 43.286 s. Mean Reward: 3.201. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 60000. Time Elapsed: 52.080 s. Mean Reward: 5.528. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 72000. Time Elapsed: 61.436 s. Mean Reward: 11.633. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 84000. Time Elapsed: 70.631 s. Mean Reward: 44.600. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 96000. Time Elapsed: 79.126 s. Mean Reward: 82.887. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 108000. Time Elapsed: 87.115 s. Mean Reward: 84.838. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 120000. Time Elapsed: 95.281 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 132000. Time Elapsed: 103.513 s. Mean Reward: 74.269. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 144000. Time Elapsed: 112.775 s. Mean Reward: 84.500. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 156000. Time Elapsed: 121.058 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 168000. Time Elapsed: 129.509 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 180000. Time Elapsed: 137.858 s. Mean Reward: 92.892. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 192000. Time Elapsed: 146.074 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 204000. Time Elapsed: 155.096 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 216000. Time Elapsed: 164.162 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 228000. Time Elapsed: 172.845 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 240000. Time Elapsed: 181.428 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 252000. Time Elapsed: 189.912 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 264000. Time Elapsed: 198.595 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 276000. Time Elapsed: 207.061 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 288000. Time Elapsed: 212.911 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 300000. Time Elapsed: 221.261 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 312000. Time Elapsed: 229.711 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 324000. Time Elapsed: 237.532 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 336000. Time Elapsed: 245.899 s. Mean Reward: 95.525. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 348000. Time Elapsed: 254.266 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 360000. Time Elapsed: 262.732 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 372000. Time Elapsed: 271.065 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 384000. Time Elapsed: 279.449 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 396000. Time Elapsed: 287.849 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 408000. Time Elapsed: 296.248 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 420000. Time Elapsed: 304.549 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 432000. Time Elapsed: 313.382 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 444000. Time Elapsed: 319.381 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 456000. Time Elapsed: 328.098 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 468000. Time Elapsed: 336.781 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 480000. Time Elapsed: 345.515 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 492000. Time Elapsed: 354.148 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/3DBall_POCA/3DBall/3DBall-499271.onnx\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/3DBall_POCA/3DBall/3DBall-500271.onnx\n",
      "[INFO] Copied /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/3DBall_POCA/3DBall/3DBall-500271.onnx to /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/3DBall_POCA/3DBall.onnx.\n"
     ]
    }
   ],
   "source": [
    "config_poca_fp = os.path.join(cur_dir, \"config\", \"3DBall_poca.yaml\")\n",
    "run_poca_id = \"3DBall_POCA\"\n",
    "print(config_poca_fp)\n",
    "print(run_poca_id)\n",
    "\n",
    "!mlagents-learn $config_poca_fp \\\n",
    "               --env=$env_fp \\\n",
    "               --results-dir=$output_dir \\\n",
    "               --run-id=$run_poca_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f0aa40",
   "metadata": {},
   "source": [
    "## Training POCA with randomization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be52769f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/config/3DBall_randomize_poca.yaml\n",
      "3DBall_RND_POCA\n",
      "\n",
      "            ┐  ╖\n",
      "        ╓╖╬│╡  ││╬╖╖\n",
      "    ╓╖╬│││││┘  ╬│││││╬╖\n",
      " ╖╬│││││╬╜        ╙╬│││││╖╖                               ╗╗╗\n",
      " ╬╬╬╬╖││╦╖        ╖╬││╗╣╣╣╬      ╟╣╣╬    ╟╣╣╣             ╜╜╜  ╟╣╣\n",
      " ╬╬╬╬╬╬╬╬╖│╬╖╖╓╬╪│╓╣╣╣╣╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╒╣╣╖╗╣╣╣╗   ╣╣╣ ╣╣╣╣╣╣ ╟╣╣╖   ╣╣╣\n",
      " ╬╬╬╬┐  ╙╬╬╬╬│╓╣╣╣╝╜  ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╣╙ ╙╣╣╣  ╣╣╣ ╙╟╣╣╜╙  ╫╣╣  ╟╣╣\n",
      " ╬╬╬╬┐     ╙╬╬╣╣      ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣     ╣╣╣┌╣╣╜\n",
      " ╬╬╬╜       ╬╬╣╣      ╙╝╣╣╬      ╙╣╣╣╗╖╓╗╣╣╣╜ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣╦╓    ╣╣╣╣╣\n",
      " ╙   ╓╦╖    ╬╬╣╣   ╓╗╗╖            ╙╝╣╣╣╣╝╜   ╘╝╝╜   ╝╝╝  ╝╝╝   ╙╣╣╣    ╟╣╣╣\n",
      "   ╩╬╬╬╬╬╬╦╦╬╬╣╣╗╣╣╣╣╣╣╣╝                                             ╫╣╣╣╣\n",
      "      ╙╬╬╬╬╬╬╬╣╣╣╣╣╣╝╜\n",
      "          ╙╬╬╬╣╣╣╜\n",
      "             ╙\n",
      "        \n",
      " Version information:\n",
      "  ml-agents: 1.1.0,\n",
      "  ml-agents-envs: 1.1.0,\n",
      "  Communicator API: 1.5.0,\n",
      "  PyTorch: 2.8.0\n",
      "[INFO] Connected to Unity environment with package version 4.0.0 and communication version 1.5.0\n",
      "[INFO] Connected new brain: 3DBall?team=0\n",
      "[INFO] Hyperparameters for behavior name 3DBall: \n",
      "\ttrainer_type:\tpoca\n",
      "\thyperparameters:\t\n",
      "\t  batch_size:\t64\n",
      "\t  buffer_size:\t12000\n",
      "\t  learning_rate:\t0.0003\n",
      "\t  beta:\t0.001\n",
      "\t  epsilon:\t0.2\n",
      "\t  lambd:\t0.99\n",
      "\t  num_epoch:\t3\n",
      "\t  learning_rate_schedule:\tlinear\n",
      "\t  beta_schedule:\tlinear\n",
      "\t  epsilon_schedule:\tlinear\n",
      "\tcheckpoint_interval:\t500000\n",
      "\tnetwork_settings:\t\n",
      "\t  normalize:\tTrue\n",
      "\t  hidden_units:\t128\n",
      "\t  num_layers:\t2\n",
      "\t  vis_encode_type:\tsimple\n",
      "\t  memory:\tNone\n",
      "\t  goal_conditioning_type:\thyper\n",
      "\t  deterministic:\tFalse\n",
      "\treward_signals:\t\n",
      "\t  extrinsic:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t1.0\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t128\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\tinit_path:\tNone\n",
      "\tkeep_checkpoints:\t5\n",
      "\teven_checkpoints:\tFalse\n",
      "\tmax_steps:\t500000\n",
      "\ttime_horizon:\t1000\n",
      "\tsummary_freq:\t12000\n",
      "\tthreaded:\tFalse\n",
      "\tself_play:\tNone\n",
      "\tbehavioral_cloning:\tNone\n",
      "[INFO] Parameter 'mass' is in lesson 'mass' and has value 'Uniform sampler: min=0.5, max=10.0'.\n",
      "[INFO] Parameter 'scale' is in lesson 'scale' and has value 'Uniform sampler: min=0.75, max=3.0'.\n",
      "[INFO] 3DBall. Step: 12000. Time Elapsed: 13.299 s. Mean Reward: 1.047. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 24000. Time Elapsed: 24.503 s. Mean Reward: 1.211. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 36000. Time Elapsed: 35.339 s. Mean Reward: 1.743. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 48000. Time Elapsed: 46.036 s. Mean Reward: 2.943. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 60000. Time Elapsed: 56.448 s. Mean Reward: 4.996. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 72000. Time Elapsed: 66.073 s. Mean Reward: 20.918. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 84000. Time Elapsed: 76.694 s. Mean Reward: 33.408. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 96000. Time Elapsed: 87.451 s. Mean Reward: 69.482. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 108000. Time Elapsed: 96.700 s. Mean Reward: 77.753. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 120000. Time Elapsed: 105.933 s. Mean Reward: 99.067. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 132000. Time Elapsed: 115.466 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 144000. Time Elapsed: 125.150 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 156000. Time Elapsed: 134.633 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 168000. Time Elapsed: 144.216 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 180000. Time Elapsed: 153.766 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 192000. Time Elapsed: 163.449 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 204000. Time Elapsed: 173.283 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 216000. Time Elapsed: 183.366 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 228000. Time Elapsed: 193.550 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 240000. Time Elapsed: 203.433 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 252000. Time Elapsed: 211.282 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 264000. Time Elapsed: 220.999 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 276000. Time Elapsed: 240.400 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 288000. Time Elapsed: 251.267 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 300000. Time Elapsed: 262.033 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 312000. Time Elapsed: 272.282 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 324000. Time Elapsed: 282.982 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 336000. Time Elapsed: 292.948 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 348000. Time Elapsed: 303.131 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 360000. Time Elapsed: 312.898 s. Mean Reward: 92.423. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 372000. Time Elapsed: 322.897 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 384000. Time Elapsed: 332.897 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 396000. Time Elapsed: 342.465 s. Mean Reward: 86.057. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 408000. Time Elapsed: 350.455 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 420000. Time Elapsed: 359.647 s. Mean Reward: 78.587. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 432000. Time Elapsed: 365.814 s. Mean Reward: 99.650. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 444000. Time Elapsed: 374.847 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 456000. Time Elapsed: 383.781 s. Mean Reward: 92.400. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 468000. Time Elapsed: 392.614 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 480000. Time Elapsed: 401.513 s. Mean Reward: 100.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] 3DBall. Step: 492000. Time Elapsed: 410.765 s. Mean Reward: 85.943. Mean Group Reward: 0.000. Training.\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/3DBall_RND_POCA/3DBall/3DBall-499432.onnx\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/3DBall_RND_POCA/3DBall/3DBall-500432.onnx\n",
      "[INFO] Copied /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/3DBall_RND_POCA/3DBall/3DBall-500432.onnx to /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/3DBall_RND_POCA/3DBall.onnx.\n"
     ]
    }
   ],
   "source": [
    "config_poca_fp = os.path.join(cur_dir, \"config\", \"3DBall_randomize_poca.yaml\")\n",
    "run_poca_id = \"3DBall_RND_POCA\"\n",
    "print(config_poca_fp)\n",
    "print(run_poca_id)\n",
    "\n",
    "!mlagents-learn $config_poca_fp \\\n",
    "               --env=$env_fp \\\n",
    "               --results-dir=$output_dir \\\n",
    "               --run-id=$run_poca_id\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
