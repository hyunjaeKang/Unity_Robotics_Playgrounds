{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70a95c2c",
   "metadata": {},
   "source": [
    "# Dron_DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84d197bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import datetime\n",
    "import platform\n",
    "import torch\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import deque\n",
    "from pathlib import Path\n",
    "from mlagents_envs.environment import UnityEnvironment, ActionTuple\n",
    "from mlagents_envs.side_channel.engine_configuration_channel\\\n",
    "                             import EngineConfigurationChannel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2fb148",
   "metadata": {},
   "source": [
    "## Setting environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d8093df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Setting\n",
    "cur_dir = os.getcwd()\n",
    "env_dir = os.path.abspath(os.path.join(cur_dir, \"..\", \"Unity6000_Envs\"))\n",
    "test_dir = os.path.abspath(os.path.join(cur_dir, \"temp\", \"pytorch_output\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d33761",
   "metadata": {},
   "source": [
    "### Pytorch Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "521fc2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "# Pytorch Device\n",
    "if torch.backends.mps.is_available():\n",
    "    g_device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    g_device = torch.device(\"cuda\")\n",
    "else:\n",
    "    g_device = torch.device(\"cpu\")\n",
    "\n",
    "print(g_device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7485c2ea",
   "metadata": {},
   "source": [
    "### Unity Enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94f1e519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unity Enviroment\n",
    "game = \"Drone\"\n",
    "os_name = platform.system()\n",
    "\n",
    "if os_name == 'Linux':\n",
    "    env_name = os.path.join(env_dir, f\"{game}_{os_name}.x86_64\")\n",
    "elif os_name == 'Darwin':\n",
    "    env_name = os.path.join(env_dir, f\"{game}_{os_name}.app\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be65174e",
   "metadata": {},
   "source": [
    "### Seting parameters for DDPG Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30ee7182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seting parameters for DDPG Network\n",
    "state_size = 9\n",
    "action_size = 3\n",
    "\n",
    "load_model = False\n",
    "train_mode = True\n",
    "\n",
    "batch_size = 128\n",
    "mem_maxlen = 30000\n",
    "discount_factor = 0.9\n",
    "actor_lr = 1e-4\n",
    "critic_lr = 5e-4\n",
    "tau = 1e-3\n",
    "\n",
    "# OU noise Parameters\n",
    "mu = 0\n",
    "theta = 1e-3\n",
    "sigma = 2e-3\n",
    "\n",
    "run_step = 50000 if train_mode else 0\n",
    "test_step = 10000\n",
    "train_start_step = 5000\n",
    "\n",
    "print_interval = 10\n",
    "save_interval = 100\n",
    "\n",
    "unity_base_port = 1900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bce97f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN model : Save and Load\n",
    "date_time = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "save_path = os.path.join(test_dir, f\"saved_models/{game}/DDPG/{date_time}\")\n",
    "Path(save_path).mkdir(parents=True, exist_ok=True)\n",
    "save_model_path = os.path.join(save_path, 'Drone_DDPG.ckpt')\n",
    "# print(f\"save_path :{save_path}\")\n",
    "# print(f\"save_model_path :{save_model_path}\")\n",
    "load_path = os.path.join(test_dir, f\"saved_models/{game}/DQN/20210514201212\") # Need to update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75aded1f",
   "metadata": {},
   "source": [
    "## OU_noise sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa3fdab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OU_noise sampling\n",
    "class OU_noise:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.X = np.ones((1, action_size), dtype=np.float32) * mu\n",
    "\n",
    "    def sample(self):\n",
    "        dx = theta * (mu - self.X) + sigma * np.random.randn(len(self.X))\n",
    "        self.X += dx\n",
    "        return self.X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee41c622",
   "metadata": {},
   "source": [
    "## Actor Network : DDPG-Actor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50190506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor Network : DDPG-Actor\n",
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Input : State - 9\n",
    "    Output : Action - 3\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_size, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, 128)\n",
    "        self.mu = torch.nn.Linear(128, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return torch.tanh(self.mu(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdb4bb9",
   "metadata": {},
   "source": [
    "## Critic Network : DDPG -Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe61af2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critic Network : DDPG -Critic\n",
    "class Critic(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Input : State + Action\n",
    "    Output : Q-Value\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(state_size, 128)\n",
    "        self.fc2 = torch.nn.Linear(128+action_size, 128)\n",
    "        self.q = torch.nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.cat((x, action), dim=-1)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.q(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e80e8cb",
   "metadata": {},
   "source": [
    "## DDPG Agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ca8c7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDPGAgent\n",
    "class DDPGAgent():\n",
    "    def __init__(self):\n",
    "        self.actor = Actor().to(g_device)\n",
    "        self.target_actor = copy.deepcopy(self.actor)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "\n",
    "        self.critic = Critic().to(g_device)\n",
    "        self.target_critic = copy.deepcopy(self.critic)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "\n",
    "        self.OU = OU_noise()\n",
    "        self.memory = deque(maxlen=mem_maxlen)\n",
    "        self.writer = SummaryWriter(save_path)\n",
    "\n",
    "        if load_model == True:\n",
    "            print(f\"... Load Model from {load_path}/ckpt ...\")\n",
    "            checkpoint = torch.load(load_path, map_location=g_device)\n",
    "            self.actor.load_state_dict(checkpoint[\"actor\"])\n",
    "            self.target_actor.load_state_dict(checkpoint[\"actor\"])\n",
    "            self.actor_optimizer.load_state_dict(checkpoint[\"actor_optimizer\"])\n",
    "            self.critic.load_state_dict(checkpoint[\"critic\"])\n",
    "            self.target_critic.load_state_dict(checkpoint[\"critic\"])\n",
    "            self.critic_optimizer.load_state_dict(checkpoint[\"critic_optimizer\"])\n",
    "\n",
    "    # Get Action with OU noise\n",
    "    def get_action(self, state, training=True):\n",
    "        self.actor.train(training)\n",
    "\n",
    "        action = self.actor(torch.FloatTensor(state).to(g_device)).cpu().detach().numpy()\n",
    "        return action + self.OU.sample() if training else action\n",
    "\n",
    "    # Add sample data into memory\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    # Training model\n",
    "    def train_model(self):\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        state      = np.stack([b[0] for b in batch], axis=0)\n",
    "        action     = np.stack([b[1] for b in batch], axis=0)\n",
    "        reward     = np.stack([b[2] for b in batch], axis=0)\n",
    "        next_state = np.stack([b[3] for b in batch], axis=0)\n",
    "        done       = np.stack([b[4] for b in batch], axis=0)\n",
    "\n",
    "        state, action, reward, next_state, done = map(lambda x: torch.FloatTensor(x).to(g_device),\n",
    "                                                        [state, action, reward, next_state, done])\n",
    "\n",
    "        # Update Critic\n",
    "        next_actions = self.target_actor(next_state)\n",
    "        next_q = self.target_critic(next_state, next_actions)\n",
    "        target_q = reward + (1 - done) * discount_factor * next_q\n",
    "        q = self.critic(state, action)\n",
    "        critic_loss = F.mse_loss(target_q, q)\n",
    "\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # Update Actor\n",
    "        action_pred = self.actor(state)\n",
    "        actor_loss = -self.critic(state, action_pred).mean()\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        return actor_loss.item(), critic_loss.item()\n",
    "\n",
    "    # Soft Update for target network\n",
    "    def soft_update_target(self):\n",
    "        for target_param, local_param in zip(self.target_actor.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
    "        for target_param, local_param in zip(self.target_critic.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
    "\n",
    "    # Save Model\n",
    "    def save_model(self):\n",
    "        print(f\"... Save Model to {save_model_path} ...\")\n",
    "        torch.save({\n",
    "            \"actor\" : self.actor.state_dict(),\n",
    "            \"actor_optimizer\" : self.actor_optimizer.state_dict(),\n",
    "            \"critic\" : self.critic.state_dict(),\n",
    "            \"critic_optimizer\" : self.critic_optimizer.state_dict(),\n",
    "        }, save_model_path)\n",
    "\n",
    "    # 학습 기록\n",
    "    def write_summray(self, score, actor_loss, critic_loss, step):\n",
    "        self.writer.add_scalar(\"ddpg_run/score\", score, step)\n",
    "        self.writer.add_scalar(\"ddpg_model/actor_loss\", actor_loss, step)\n",
    "        self.writer.add_scalar(\"ddpg_model/critic_loss\", critic_loss, step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b01e40",
   "metadata": {},
   "source": [
    "## Train DQN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc0558bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnityMemory] Configuration Parameters - Can be set up in boot.config\n",
      "    \"memorysetup-allocator-temp-initial-block-size-main=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-worker=262144\"\n",
      "    \"memorysetup-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-cache-allocator-block-size=4194304\"\n",
      "    \"memorysetup-typetree-allocator-block-size=2097152\"\n",
      "    \"memorysetup-profiler-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-profiler-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-profiler-allocator-block-size=16777216\"\n",
      "    \"memorysetup-profiler-editor-allocator-block-size=1048576\"\n",
      "    \"memorysetup-temp-allocator-size-main=4194304\"\n",
      "    \"memorysetup-job-temp-allocator-block-size=2097152\"\n",
      "    \"memorysetup-job-temp-allocator-block-size-background=1048576\"\n",
      "    \"memorysetup-job-temp-allocator-reduction-small-platforms=262144\"\n",
      "    \"memorysetup-temp-allocator-size-background-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-job-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-preload-manager=262144\"\n",
      "    \"memorysetup-temp-allocator-size-nav-mesh-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-audio-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-cloud-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-gfx=262144\"\n",
      "10 Episode / Step: 5756 / Score: -12.50 / Actor loss: -0.20 / Critic loss: 0.0006\n",
      "20 Episode / Step: 6282 / Score: -6.44 / Actor loss: -0.15 / Critic loss: 0.0012\n",
      "30 Episode / Step: 6878 / Score: -7.20 / Actor loss: -0.11 / Critic loss: 0.0009\n",
      "40 Episode / Step: 7415 / Score: -6.64 / Actor loss: -0.05 / Critic loss: 0.0005\n",
      "50 Episode / Step: 7995 / Score: -6.78 / Actor loss: 0.00 / Critic loss: 0.0003\n",
      "60 Episode / Step: 8645 / Score: -7.14 / Actor loss: 0.04 / Critic loss: 0.0001\n",
      "70 Episode / Step: 9324 / Score: -7.28 / Actor loss: 0.08 / Critic loss: 0.0001\n",
      "80 Episode / Step: 10067 / Score: -7.51 / Actor loss: 0.12 / Critic loss: 0.0001\n",
      "90 Episode / Step: 10733 / Score: -6.48 / Actor loss: 0.14 / Critic loss: 0.0001\n",
      "100 Episode / Step: 11485 / Score: -6.73 / Actor loss: 0.16 / Critic loss: 0.0001\n",
      "... Save Model to /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/pytorch_output/saved_models/Drone/DDPG/20250917140120/Drone_DDPG.ckpt ...\n",
      "110 Episode / Step: 12258 / Score: -6.70 / Actor loss: 0.16 / Critic loss: 0.0002\n",
      "120 Episode / Step: 13031 / Score: -7.50 / Actor loss: 0.16 / Critic loss: 0.0003\n",
      "130 Episode / Step: 13843 / Score: -3.95 / Actor loss: 0.15 / Critic loss: 0.0004\n",
      "140 Episode / Step: 14825 / Score: -6.45 / Actor loss: 0.13 / Critic loss: 0.0004\n",
      "150 Episode / Step: 15687 / Score: -5.74 / Actor loss: 0.13 / Critic loss: 0.0004\n",
      "160 Episode / Step: 16451 / Score: -5.20 / Actor loss: 0.13 / Critic loss: 0.0004\n",
      "170 Episode / Step: 17450 / Score: -7.18 / Actor loss: 0.13 / Critic loss: 0.0004\n",
      "180 Episode / Step: 18307 / Score: -7.01 / Actor loss: 0.14 / Critic loss: 0.0004\n",
      "190 Episode / Step: 19386 / Score: -7.81 / Actor loss: 0.15 / Critic loss: 0.0003\n",
      "200 Episode / Step: 20410 / Score: -8.31 / Actor loss: 0.15 / Critic loss: 0.0002\n",
      "... Save Model to /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/pytorch_output/saved_models/Drone/DDPG/20250917140120/Drone_DDPG.ckpt ...\n",
      "210 Episode / Step: 21115 / Score: -5.14 / Actor loss: 0.16 / Critic loss: 0.0003\n",
      "220 Episode / Step: 22137 / Score: -7.34 / Actor loss: 0.16 / Critic loss: 0.0003\n",
      "230 Episode / Step: 23155 / Score: -7.73 / Actor loss: 0.18 / Critic loss: 0.0003\n",
      "240 Episode / Step: 23920 / Score: -4.33 / Actor loss: 0.19 / Critic loss: 0.0003\n",
      "250 Episode / Step: 24857 / Score: -6.38 / Actor loss: 0.19 / Critic loss: 0.0004\n",
      "260 Episode / Step: 25905 / Score: -7.72 / Actor loss: 0.21 / Critic loss: 0.0004\n",
      "270 Episode / Step: 26668 / Score: -4.76 / Actor loss: 0.22 / Critic loss: 0.0004\n",
      "280 Episode / Step: 27657 / Score: -5.61 / Actor loss: 0.23 / Critic loss: 0.0005\n",
      "290 Episode / Step: 28666 / Score: -7.14 / Actor loss: 0.24 / Critic loss: 0.0005\n",
      "300 Episode / Step: 29626 / Score: -6.85 / Actor loss: 0.25 / Critic loss: 0.0005\n",
      "... Save Model to /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/pytorch_output/saved_models/Drone/DDPG/20250917140120/Drone_DDPG.ckpt ...\n",
      "310 Episode / Step: 30649 / Score: -6.27 / Actor loss: 0.26 / Critic loss: 0.0006\n",
      "320 Episode / Step: 31796 / Score: -7.69 / Actor loss: 0.31 / Critic loss: 0.0005\n",
      "330 Episode / Step: 32548 / Score: 0.93 / Actor loss: 0.32 / Critic loss: 0.0006\n",
      "340 Episode / Step: 32971 / Score: 4.52 / Actor loss: 0.29 / Critic loss: 0.0010\n",
      "350 Episode / Step: 33429 / Score: 4.38 / Actor loss: 0.26 / Critic loss: 0.0009\n",
      "360 Episode / Step: 33913 / Score: 4.93 / Actor loss: 0.23 / Critic loss: 0.0012\n",
      "370 Episode / Step: 34457 / Score: 4.60 / Actor loss: 0.20 / Critic loss: 0.0012\n",
      "380 Episode / Step: 34999 / Score: 4.37 / Actor loss: 0.17 / Critic loss: 0.0015\n",
      "390 Episode / Step: 35747 / Score: 5.17 / Actor loss: 0.14 / Critic loss: 0.0016\n",
      "400 Episode / Step: 36241 / Score: 5.09 / Actor loss: 0.10 / Critic loss: 0.0014\n",
      "... Save Model to /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/pytorch_output/saved_models/Drone/DDPG/20250917140120/Drone_DDPG.ckpt ...\n",
      "410 Episode / Step: 36790 / Score: 4.84 / Actor loss: 0.07 / Critic loss: 0.0016\n",
      "420 Episode / Step: 37855 / Score: 3.78 / Actor loss: 0.02 / Critic loss: 0.0018\n",
      "430 Episode / Step: 38277 / Score: 4.73 / Actor loss: -0.02 / Critic loss: 0.0018\n",
      "440 Episode / Step: 38693 / Score: 5.13 / Actor loss: -0.05 / Critic loss: 0.0018\n",
      "450 Episode / Step: 39088 / Score: 5.12 / Actor loss: -0.09 / Critic loss: 0.0019\n",
      "460 Episode / Step: 39574 / Score: 5.32 / Actor loss: -0.12 / Critic loss: 0.0020\n",
      "470 Episode / Step: 40080 / Score: 4.97 / Actor loss: -0.16 / Critic loss: 0.0020\n",
      "480 Episode / Step: 40535 / Score: 5.13 / Actor loss: -0.19 / Critic loss: 0.0019\n",
      "490 Episode / Step: 41043 / Score: 5.22 / Actor loss: -0.23 / Critic loss: 0.0021\n",
      "500 Episode / Step: 41589 / Score: 4.62 / Actor loss: -0.26 / Critic loss: 0.0021\n",
      "... Save Model to /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/pytorch_output/saved_models/Drone/DDPG/20250917140120/Drone_DDPG.ckpt ...\n",
      "510 Episode / Step: 42013 / Score: 4.54 / Actor loss: -0.30 / Critic loss: 0.0020\n",
      "520 Episode / Step: 42506 / Score: 5.14 / Actor loss: -0.33 / Critic loss: 0.0019\n",
      "530 Episode / Step: 42987 / Score: 5.41 / Actor loss: -0.36 / Critic loss: 0.0020\n",
      "540 Episode / Step: 43433 / Score: 4.90 / Actor loss: -0.39 / Critic loss: 0.0021\n",
      "550 Episode / Step: 43817 / Score: 4.82 / Actor loss: -0.41 / Critic loss: 0.0019\n",
      "560 Episode / Step: 44240 / Score: 4.72 / Actor loss: -0.43 / Critic loss: 0.0019\n",
      "570 Episode / Step: 44683 / Score: 5.42 / Actor loss: -0.46 / Critic loss: 0.0019\n",
      "580 Episode / Step: 45089 / Score: 5.28 / Actor loss: -0.48 / Critic loss: 0.0019\n",
      "590 Episode / Step: 45502 / Score: 5.23 / Actor loss: -0.51 / Critic loss: 0.0019\n",
      "600 Episode / Step: 45961 / Score: 5.46 / Actor loss: -0.54 / Critic loss: 0.0017\n",
      "... Save Model to /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/pytorch_output/saved_models/Drone/DDPG/20250917140120/Drone_DDPG.ckpt ...\n",
      "610 Episode / Step: 46302 / Score: 4.14 / Actor loss: -0.55 / Critic loss: 0.0018\n",
      "620 Episode / Step: 46708 / Score: 5.05 / Actor loss: -0.57 / Critic loss: 0.0017\n",
      "630 Episode / Step: 47073 / Score: 4.17 / Actor loss: -0.59 / Critic loss: 0.0016\n",
      "640 Episode / Step: 47442 / Score: 4.87 / Actor loss: -0.60 / Critic loss: 0.0016\n",
      "650 Episode / Step: 47824 / Score: 4.20 / Actor loss: -0.61 / Critic loss: 0.0015\n",
      "660 Episode / Step: 48202 / Score: 4.68 / Actor loss: -0.63 / Critic loss: 0.0014\n",
      "670 Episode / Step: 48581 / Score: 4.76 / Actor loss: -0.65 / Critic loss: 0.0014\n",
      "680 Episode / Step: 48981 / Score: 5.14 / Actor loss: -0.67 / Critic loss: 0.0013\n",
      "690 Episode / Step: 49326 / Score: 4.40 / Actor loss: -0.68 / Critic loss: 0.0013\n",
      "700 Episode / Step: 49761 / Score: 5.62 / Actor loss: -0.70 / Critic loss: 0.0012\n",
      "... Save Model to /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/pytorch_output/saved_models/Drone/DDPG/20250917140120/Drone_DDPG.ckpt ...\n",
      "... Save Model to /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/pytorch_output/saved_models/Drone/DDPG/20250917140120/Drone_DDPG.ckpt ...\n",
      "TEST START\n",
      "710 Episode / Step: 50091 / Score: 3.91 / Actor loss: -0.71 / Critic loss: 0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720 Episode / Step: 50493 / Score: 4.79 / Actor loss: nan / Critic loss: nan\n",
      "730 Episode / Step: 50783 / Score: 4.30 / Actor loss: nan / Critic loss: nan\n",
      "740 Episode / Step: 51178 / Score: 4.94 / Actor loss: nan / Critic loss: nan\n",
      "750 Episode / Step: 51495 / Score: 4.31 / Actor loss: nan / Critic loss: nan\n",
      "760 Episode / Step: 51854 / Score: 4.99 / Actor loss: nan / Critic loss: nan\n",
      "770 Episode / Step: 52156 / Score: 4.31 / Actor loss: nan / Critic loss: nan\n",
      "780 Episode / Step: 52521 / Score: 4.73 / Actor loss: nan / Critic loss: nan\n",
      "790 Episode / Step: 52897 / Score: 4.68 / Actor loss: nan / Critic loss: nan\n",
      "800 Episode / Step: 53346 / Score: 5.67 / Actor loss: nan / Critic loss: nan\n",
      "810 Episode / Step: 53722 / Score: 5.01 / Actor loss: nan / Critic loss: nan\n",
      "820 Episode / Step: 54062 / Score: 4.57 / Actor loss: nan / Critic loss: nan\n",
      "830 Episode / Step: 54465 / Score: 5.25 / Actor loss: nan / Critic loss: nan\n",
      "840 Episode / Step: 54869 / Score: 5.74 / Actor loss: nan / Critic loss: nan\n",
      "850 Episode / Step: 55257 / Score: 5.09 / Actor loss: nan / Critic loss: nan\n",
      "860 Episode / Step: 55654 / Score: 5.23 / Actor loss: nan / Critic loss: nan\n",
      "870 Episode / Step: 56026 / Score: 4.84 / Actor loss: nan / Critic loss: nan\n",
      "880 Episode / Step: 56354 / Score: 4.50 / Actor loss: nan / Critic loss: nan\n",
      "890 Episode / Step: 56757 / Score: 5.25 / Actor loss: nan / Critic loss: nan\n",
      "900 Episode / Step: 57138 / Score: 4.50 / Actor loss: nan / Critic loss: nan\n",
      "910 Episode / Step: 57504 / Score: 5.12 / Actor loss: nan / Critic loss: nan\n",
      "920 Episode / Step: 57890 / Score: 5.31 / Actor loss: nan / Critic loss: nan\n",
      "930 Episode / Step: 58231 / Score: 4.47 / Actor loss: nan / Critic loss: nan\n",
      "940 Episode / Step: 58610 / Score: 4.48 / Actor loss: nan / Critic loss: nan\n",
      "950 Episode / Step: 58950 / Score: 4.47 / Actor loss: nan / Critic loss: nan\n",
      "960 Episode / Step: 59308 / Score: 5.43 / Actor loss: nan / Critic loss: nan\n",
      "970 Episode / Step: 59735 / Score: 5.43 / Actor loss: nan / Critic loss: nan\n"
     ]
    }
   ],
   "source": [
    "engine_configuration_channel = EngineConfigurationChannel()\n",
    "env = UnityEnvironment(file_name=env_name,\n",
    "                        side_channels=[engine_configuration_channel], base_port=unity_base_port)\n",
    "env.reset()\n",
    "\n",
    "# Setup Unitu MLAgent\n",
    "behavior_name = list(env.behavior_specs.keys())[0]\n",
    "spec = env.behavior_specs[behavior_name]\n",
    "engine_configuration_channel.set_configuration_parameters(time_scale=12.0)\n",
    "# engine_configuration_channel.set_configuration_parameters(time_scale=1.0)\n",
    "dec, term = env.get_steps(behavior_name)\n",
    "\n",
    "#\n",
    "agent = DDPGAgent()\n",
    "\n",
    "actor_losses, critic_losses, scores, episode, score = [], [], [], 0, 0\n",
    "for step in range(run_step + test_step):\n",
    "    if step == run_step:\n",
    "        if train_mode:\n",
    "            agent.save_model()\n",
    "        print(\"TEST START\")\n",
    "        train_mode = False\n",
    "        engine_configuration_channel.set_configuration_parameters(time_scale=1.0)\n",
    "\n",
    "    state = dec.obs[0]\n",
    "    action = agent.get_action(state, train_mode)\n",
    "    action_tuple = ActionTuple()\n",
    "    action_tuple.add_continuous(action)\n",
    "    env.set_actions(behavior_name, action_tuple)\n",
    "    env.step()\n",
    "\n",
    "    dec, term = env.get_steps(behavior_name)\n",
    "    done = len(term.agent_id) > 0\n",
    "    reward = term.reward if done else dec.reward\n",
    "    next_state = term.obs[0] if done else dec.obs[0]\n",
    "    score += reward[0]\n",
    "\n",
    "    if train_mode:\n",
    "        agent.append_sample(state[0], action[0], reward, next_state[0], [done])\n",
    "\n",
    "    if train_mode and step > max(batch_size, train_start_step):\n",
    "        # Traing Agent's networks\n",
    "        actor_loss, critic_loss = agent.train_model()\n",
    "        actor_losses.append(actor_loss)\n",
    "        critic_losses.append(critic_loss)\n",
    "\n",
    "        # Update the target network\n",
    "        agent.soft_update_target()\n",
    "\n",
    "    if done:\n",
    "        episode += 1\n",
    "        scores.append(score)\n",
    "        score = 0\n",
    "\n",
    "        # logging tensorboard\n",
    "        if episode % print_interval == 0:\n",
    "            mean_score = np.mean(scores)\n",
    "            mean_actor_loss = np.mean(actor_losses)\n",
    "            mean_critic_loss = np.mean(critic_losses)\n",
    "            agent.write_summray(mean_score, mean_actor_loss, mean_critic_loss, step)\n",
    "            actor_losses, critic_losses, scores = [], [], []\n",
    "\n",
    "            print(f\"{episode} Episode / Step: {step} / Score: {mean_score:.2f} / \" +\\\n",
    "                    f\"Actor loss: {mean_actor_loss:.2f} / Critic loss: {mean_critic_loss:.4f}\")\n",
    "\n",
    "        # Saveing model\n",
    "        if train_mode and episode % save_interval == 0:\n",
    "            agent.save_model()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac3f52b",
   "metadata": {},
   "source": [
    "## Test the pretrained DDPG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "395f904e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnityMemory] Configuration Parameters - Can be set up in boot.config\n",
      "    \"memorysetup-allocator-temp-initial-block-size-main=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-worker=262144\"\n",
      "    \"memorysetup-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-cache-allocator-block-size=4194304\"\n",
      "    \"memorysetup-typetree-allocator-block-size=2097152\"\n",
      "    \"memorysetup-profiler-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-profiler-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-profiler-allocator-block-size=16777216\"\n",
      "    \"memorysetup-profiler-editor-allocator-block-size=1048576\"\n",
      "    \"memorysetup-temp-allocator-size-main=4194304\"\n",
      "    \"memorysetup-job-temp-allocator-block-size=2097152\"\n",
      "    \"memorysetup-job-temp-allocator-block-size-background=1048576\"\n",
      "    \"memorysetup-job-temp-allocator-reduction-small-platforms=262144\"\n",
      "    \"memorysetup-temp-allocator-size-background-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-job-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-preload-manager=262144\"\n",
      "    \"memorysetup-temp-allocator-size-nav-mesh-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-audio-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-cloud-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-gfx=262144\"\n",
      "... Load Model from /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/pytorch_output/saved_models/Drone/DDPG/20250917140120/Drone_DDPG.ckpt/ckpt ...\n",
      "1 - episode (18 steps) is done with the score of 2.3828252386301756\n",
      "2 - episode (25 steps) is done with the score of 3.8067673798650503\n",
      "3 - episode (17 steps) is done with the score of 2.9235613103955984\n",
      "4 - episode (40 steps) is done with the score of 5.490142455324531\n",
      "5 - episode (26 steps) is done with the score of 4.2979933470487595\n",
      "6 - episode (19 steps) is done with the score of 3.2384345065802336\n",
      "7 - episode (43 steps) is done with the score of 5.746401933953166\n",
      "8 - episode (28 steps) is done with the score of 3.586203671991825\n",
      "9 - episode (47 steps) is done with the score of 6.157242467626929\n",
      "10 - episode (35 steps) is done with the score of 5.698632104322314\n",
      "11 - episode (44 steps) is done with the score of 4.775944149121642\n",
      "12 - episode (50 steps) is done with the score of 6.968776527792215\n",
      "13 - episode (37 steps) is done with the score of 5.0371911730617285\n",
      "14 - episode (27 steps) is done with the score of 3.630731387063861\n",
      "15 - episode (47 steps) is done with the score of 7.673298737034202\n",
      "16 - episode (34 steps) is done with the score of 4.0265098717063665\n",
      "17 - episode (48 steps) is done with the score of 7.062089605256915\n",
      "18 - episode (5 steps) is done with the score of 1.3097351491451263\n",
      "19 - episode (48 steps) is done with the score of 6.751673569902778\n",
      "20 - episode (34 steps) is done with the score of 3.913325121626258\n",
      "21 - episode (15 steps) is done with the score of 1.7538037411868572\n",
      "22 - episode (44 steps) is done with the score of 4.758567430078983\n",
      "23 - episode (34 steps) is done with the score of 4.397446205839515\n",
      "24 - episode (50 steps) is done with the score of 7.763181222602725\n",
      "25 - episode (37 steps) is done with the score of 4.271806683391333\n",
      "26 - episode (17 steps) is done with the score of 2.0989271383732557\n",
      "27 - episode (24 steps) is done with the score of 3.217771491035819\n",
      "28 - episode (35 steps) is done with the score of 5.870501710101962\n",
      "29 - episode (33 steps) is done with the score of 3.8548699896782637\n",
      "30 - episode (46 steps) is done with the score of 6.320618776604533\n",
      "31 - episode (24 steps) is done with the score of 3.9783944990485907\n",
      "32 - episode (26 steps) is done with the score of 4.2650923486799\n",
      "33 - episode (48 steps) is done with the score of 5.64634925685823\n",
      "34 - episode (45 steps) is done with the score of 6.774726456031203\n",
      "35 - episode (44 steps) is done with the score of 5.449243206530809\n",
      "36 - episode (45 steps) is done with the score of 5.8535984214395285\n",
      "37 - episode (28 steps) is done with the score of 4.85792713612318\n",
      "38 - episode (21 steps) is done with the score of 3.7896655574440956\n",
      "39 - episode (36 steps) is done with the score of 4.1022323705255985\n",
      "40 - episode (39 steps) is done with the score of 4.086685432121158\n",
      "41 - episode (28 steps) is done with the score of 4.137756358832121\n",
      "42 - episode (38 steps) is done with the score of 4.33986010402441\n",
      "43 - episode (36 steps) is done with the score of 4.564908305183053\n",
      "44 - episode (51 steps) is done with the score of 6.161633361130953\n",
      "45 - episode (48 steps) is done with the score of 6.3082149010151625\n",
      "46 - episode (18 steps) is done with the score of 2.432352317497134\n",
      "47 - episode (28 steps) is done with the score of 4.21087851934135\n",
      "48 - episode (38 steps) is done with the score of 4.931664055213332\n",
      "49 - episode (41 steps) is done with the score of 4.547872422263026\n",
      "50 - episode (44 steps) is done with the score of 6.7369617354124784\n",
      "51 - episode (25 steps) is done with the score of 4.501956021413207\n",
      "52 - episode (30 steps) is done with the score of 4.814278166741133\n",
      "53 - episode (48 steps) is done with the score of 6.409122360870242\n",
      "54 - episode (35 steps) is done with the score of 6.014598844572902\n",
      "55 - episode (46 steps) is done with the score of 5.865656038746238\n",
      "56 - episode (21 steps) is done with the score of 3.1073319148272276\n",
      "57 - episode (26 steps) is done with the score of 2.991884818300605\n",
      "58 - episode (43 steps) is done with the score of 4.513049341738224\n",
      "59 - episode (35 steps) is done with the score of 3.391875009983778\n",
      "60 - episode (22 steps) is done with the score of 3.64115153811872\n",
      "61 - episode (46 steps) is done with the score of 4.669862290844321\n",
      "62 - episode (41 steps) is done with the score of 4.179192122071981\n",
      "63 - episode (39 steps) is done with the score of 4.912420189008117\n",
      "64 - episode (21 steps) is done with the score of 3.257411604747176\n",
      "65 - episode (53 steps) is done with the score of 5.6514392755925655\n",
      "66 - episode (46 steps) is done with the score of 5.086986003443599\n",
      "67 - episode (28 steps) is done with the score of 3.877305781468749\n",
      "68 - episode (17 steps) is done with the score of 2.5489890351891518\n",
      "69 - episode (40 steps) is done with the score of 5.928712969645858\n",
      "70 - episode (23 steps) is done with the score of 4.059161961078644\n",
      "71 - episode (8 steps) is done with the score of 1.264134231954813\n",
      "72 - episode (45 steps) is done with the score of 6.614039493724704\n",
      "73 - episode (46 steps) is done with the score of 4.66164549253881\n",
      "74 - episode (24 steps) is done with the score of 4.585644580423832\n",
      "75 - episode (29 steps) is done with the score of 4.118904989212751\n",
      "76 - episode (41 steps) is done with the score of 4.166135726496577\n",
      "77 - episode (40 steps) is done with the score of 4.901130801066756\n",
      "78 - episode (12 steps) is done with the score of 2.397318882867694\n",
      "79 - episode (47 steps) is done with the score of 4.978976979851723\n",
      "80 - episode (44 steps) is done with the score of 5.061995958909392\n",
      "81 - episode (33 steps) is done with the score of 4.665994836017489\n",
      "82 - episode (28 steps) is done with the score of 3.6753427051007748\n",
      "83 - episode (33 steps) is done with the score of 4.849048433825374\n",
      "84 - episode (41 steps) is done with the score of 4.7625390868633986\n",
      "85 - episode (48 steps) is done with the score of 5.726502878591418\n",
      "86 - episode (47 steps) is done with the score of 5.863883350044489\n",
      "87 - episode (45 steps) is done with the score of 5.793626306578517\n",
      "88 - episode (25 steps) is done with the score of 4.0755307506769896\n",
      "89 - episode (50 steps) is done with the score of 6.894748969003558\n",
      "90 - episode (48 steps) is done with the score of 5.1830099280923605\n",
      "91 - episode (39 steps) is done with the score of 4.2436949126422405\n",
      "92 - episode (41 steps) is done with the score of 4.868528425693512\n",
      "93 - episode (37 steps) is done with the score of 3.9206986110657454\n",
      "94 - episode (47 steps) is done with the score of 5.436260174959898\n",
      "95 - episode (45 steps) is done with the score of 4.906106559559703\n",
      "96 - episode (25 steps) is done with the score of 4.392352377995849\n",
      "97 - episode (41 steps) is done with the score of 7.213250823318958\n",
      "98 - episode (28 steps) is done with the score of 2.9733157474547625\n",
      "99 - episode (47 steps) is done with the score of 5.862498266622424\n",
      "100 - episode (49 steps) is done with the score of 6.271435325965285\n",
      "101 - episode (44 steps) is done with the score of 4.594351863488555\n",
      "102 - episode (33 steps) is done with the score of 5.2018801011145115\n",
      "103 - episode (41 steps) is done with the score of 4.633784828707576\n",
      "104 - episode (37 steps) is done with the score of 6.060656575486064\n",
      "105 - episode (49 steps) is done with the score of 6.834079748019576\n",
      "106 - episode (39 steps) is done with the score of 5.9567293878644705\n",
      "107 - episode (38 steps) is done with the score of 5.37452688626945\n",
      "108 - episode (38 steps) is done with the score of 5.489263281226158\n",
      "109 - episode (20 steps) is done with the score of 2.3126733656972647\n",
      "110 - episode (35 steps) is done with the score of 4.346412984654307\n",
      "111 - episode (22 steps) is done with the score of 4.022658726200461\n",
      "112 - episode (20 steps) is done with the score of 2.286105427891016\n",
      "113 - episode (34 steps) is done with the score of 3.9530912693589926\n",
      "114 - episode (50 steps) is done with the score of 5.1792911775410175\n",
      "115 - episode (28 steps) is done with the score of 3.1292978264391422\n",
      "116 - episode (32 steps) is done with the score of 5.31607705168426\n",
      "117 - episode (34 steps) is done with the score of 4.9407749976962805\n",
      "118 - episode (44 steps) is done with the score of 5.719085289165378\n",
      "119 - episode (31 steps) is done with the score of 4.2158677112311125\n",
      "120 - episode (42 steps) is done with the score of 4.7580887116491795\n",
      "121 - episode (11 steps) is done with the score of 1.689981622621417\n",
      "122 - episode (10 steps) is done with the score of 1.4396720416843891\n",
      "123 - episode (38 steps) is done with the score of 5.078512620180845\n",
      "124 - episode (45 steps) is done with the score of 5.8697758093476295\n",
      "125 - episode (34 steps) is done with the score of 4.095533778890967\n",
      "126 - episode (47 steps) is done with the score of 6.047024412080646\n",
      "127 - episode (32 steps) is done with the score of 3.2756121195852757\n",
      "128 - episode (50 steps) is done with the score of 6.582668332383037\n",
      "129 - episode (46 steps) is done with the score of 4.936709007248282\n",
      "130 - episode (42 steps) is done with the score of 5.743313981220126\n",
      "131 - episode (33 steps) is done with the score of 4.745214639231563\n",
      "132 - episode (44 steps) is done with the score of 6.689854731783271\n",
      "133 - episode (33 steps) is done with the score of 4.345158724114299\n",
      "134 - episode (36 steps) is done with the score of 6.159011563286185\n",
      "135 - episode (30 steps) is done with the score of 4.798401324078441\n",
      "136 - episode (20 steps) is done with the score of 2.939769996330142\n",
      "137 - episode (46 steps) is done with the score of 5.997441468760371\n",
      "138 - episode (47 steps) is done with the score of 6.302228523418307\n",
      "139 - episode (43 steps) is done with the score of 6.791688797995448\n",
      "140 - episode (48 steps) is done with the score of 7.1936843525618315\n",
      "141 - episode (49 steps) is done with the score of 6.711156679317355\n",
      "142 - episode (24 steps) is done with the score of 4.341311035677791\n",
      "143 - episode (32 steps) is done with the score of 3.9005001224577427\n",
      "144 - episode (35 steps) is done with the score of 5.745270315557718\n",
      "145 - episode (56 steps) is done with the score of 7.14772898517549\n",
      "146 - episode (32 steps) is done with the score of 4.993235787376761\n",
      "147 - episode (24 steps) is done with the score of 3.090615663677454\n",
      "148 - episode (35 steps) is done with the score of 5.8643351923674345\n",
      "149 - episode (21 steps) is done with the score of 3.781650884076953\n",
      "150 - episode (35 steps) is done with the score of 4.83281066082418\n",
      "151 - episode (49 steps) is done with the score of 5.9971511866897345\n",
      "152 - episode (27 steps) is done with the score of 4.314190229400992\n",
      "153 - episode (42 steps) is done with the score of 4.38523506373167\n",
      "154 - episode (23 steps) is done with the score of 2.8683532010763884\n",
      "155 - episode (39 steps) is done with the score of 5.77518779784441\n",
      "156 - episode (52 steps) is done with the score of 5.276855502277613\n",
      "157 - episode (52 steps) is done with the score of 6.825959151610732\n",
      "158 - episode (47 steps) is done with the score of 7.221206387504935\n",
      "159 - episode (30 steps) is done with the score of 4.778090229257941\n",
      "160 - episode (46 steps) is done with the score of 6.61527157202363\n",
      "161 - episode (45 steps) is done with the score of 6.45483416877687\n",
      "162 - episode (38 steps) is done with the score of 4.851888796314597\n",
      "163 - episode (35 steps) is done with the score of 3.8957527093589306\n",
      "164 - episode (54 steps) is done with the score of 6.125403011217713\n",
      "165 - episode (51 steps) is done with the score of 6.451379304751754\n",
      "166 - episode (35 steps) is done with the score of 4.0641009621322155\n",
      "167 - episode (24 steps) is done with the score of 2.8120453394949436\n",
      "168 - episode (38 steps) is done with the score of 4.174081226810813\n",
      "169 - episode (34 steps) is done with the score of 4.986025778576732\n",
      "170 - episode (33 steps) is done with the score of 4.721388569101691\n",
      "171 - episode (33 steps) is done with the score of 4.6392386965453625\n",
      "172 - episode (47 steps) is done with the score of 4.567765545099974\n",
      "173 - episode (41 steps) is done with the score of 6.087094796821475\n",
      "174 - episode (47 steps) is done with the score of 5.345200968906283\n",
      "175 - episode (43 steps) is done with the score of 6.084118824452162\n",
      "176 - episode (35 steps) is done with the score of 5.968202060088515\n",
      "177 - episode (40 steps) is done with the score of 5.161834022030234\n",
      "178 - episode (31 steps) is done with the score of 3.565450206398964\n",
      "179 - episode (44 steps) is done with the score of 4.8135670237243176\n",
      "180 - episode (23 steps) is done with the score of 2.4397097919136286\n",
      "181 - episode (37 steps) is done with the score of 4.525271682068706\n",
      "182 - episode (38 steps) is done with the score of 4.408335529267788\n",
      "183 - episode (28 steps) is done with the score of 3.9593470748513937\n",
      "184 - episode (18 steps) is done with the score of 2.4696431066840887\n",
      "185 - episode (42 steps) is done with the score of 5.514140194281936\n",
      "186 - episode (26 steps) is done with the score of 3.0602856762707233\n",
      "187 - episode (21 steps) is done with the score of 3.1354957055300474\n",
      "188 - episode (47 steps) is done with the score of 6.008779343217611\n",
      "189 - episode (30 steps) is done with the score of 4.168867154046893\n",
      "190 - episode (43 steps) is done with the score of 6.318540511652827\n",
      "191 - episode (25 steps) is done with the score of 2.841002522036433\n",
      "192 - episode (18 steps) is done with the score of 2.801583817228675\n",
      "193 - episode (31 steps) is done with the score of 4.964147781953216\n",
      "194 - episode (35 steps) is done with the score of 4.433654770255089\n",
      "195 - episode (32 steps) is done with the score of 3.6668706573545933\n",
      "196 - episode (34 steps) is done with the score of 4.34108174405992\n",
      "197 - episode (35 steps) is done with the score of 4.268598835915327\n",
      "198 - episode (34 steps) is done with the score of 5.158800018951297\n",
      "199 - episode (41 steps) is done with the score of 4.323091683909297\n",
      "200 - episode (10 steps) is done with the score of 1.679122081026435\n",
      "201 - episode (47 steps) is done with the score of 5.8044581320136786\n",
      "202 - episode (25 steps) is done with the score of 3.836659735068679\n",
      "203 - episode (36 steps) is done with the score of 6.151429167017341\n",
      "204 - episode (41 steps) is done with the score of 4.819342730566859\n",
      "205 - episode (38 steps) is done with the score of 5.853987826034427\n",
      "206 - episode (48 steps) is done with the score of 5.6439765598624945\n",
      "207 - episode (40 steps) is done with the score of 4.6780591662973166\n",
      "208 - episode (37 steps) is done with the score of 5.018437264487147\n",
      "209 - episode (43 steps) is done with the score of 5.729835992679\n",
      "210 - episode (49 steps) is done with the score of 6.735716855153441\n",
      "211 - episode (28 steps) is done with the score of 4.012296138331294\n",
      "212 - episode (48 steps) is done with the score of 4.889397922903299\n",
      "213 - episode (36 steps) is done with the score of 6.065690694376826\n",
      "214 - episode (42 steps) is done with the score of 4.24600868485868\n",
      "215 - episode (31 steps) is done with the score of 5.645885167643428\n",
      "216 - episode (20 steps) is done with the score of 3.4560160767287016\n",
      "217 - episode (41 steps) is done with the score of 6.63278385810554\n",
      "218 - episode (45 steps) is done with the score of 6.04736558906734\n",
      "219 - episode (23 steps) is done with the score of 3.1899622287601233\n",
      "220 - episode (23 steps) is done with the score of 3.445893755182624\n",
      "221 - episode (27 steps) is done with the score of 4.581366604194045\n",
      "222 - episode (30 steps) is done with the score of 3.0824976712465286\n",
      "223 - episode (37 steps) is done with the score of 3.6789762023836374\n",
      "224 - episode (34 steps) is done with the score of 4.190912475809455\n",
      "225 - episode (47 steps) is done with the score of 5.211930111050606\n",
      "226 - episode (32 steps) is done with the score of 5.370464159175754\n",
      "227 - episode (50 steps) is done with the score of 5.213478375226259\n",
      "228 - episode (42 steps) is done with the score of 5.269254697486758\n",
      "229 - episode (45 steps) is done with the score of 4.47468688711524\n",
      "230 - episode (12 steps) is done with the score of 2.338757798075676\n",
      "231 - episode (43 steps) is done with the score of 5.457780979573727\n",
      "232 - episode (42 steps) is done with the score of 6.0574188362807035\n",
      "233 - episode (43 steps) is done with the score of 6.329315511509776\n",
      "234 - episode (31 steps) is done with the score of 5.223749255761504\n",
      "235 - episode (37 steps) is done with the score of 5.538762100040913\n",
      "236 - episode (26 steps) is done with the score of 4.342186963185668\n",
      "237 - episode (11 steps) is done with the score of 1.4539634995162487\n",
      "238 - episode (46 steps) is done with the score of 6.262550866231322\n",
      "239 - episode (46 steps) is done with the score of 4.616295715793967\n",
      "240 - episode (43 steps) is done with the score of 6.691015496850014\n",
      "241 - episode (40 steps) is done with the score of 5.6957957446575165\n",
      "242 - episode (33 steps) is done with the score of 5.66193768940866\n",
      "243 - episode (30 steps) is done with the score of 4.383202387019992\n",
      "244 - episode (28 steps) is done with the score of 4.048660269007087\n",
      "245 - episode (42 steps) is done with the score of 5.704117117449641\n",
      "246 - episode (39 steps) is done with the score of 4.908100640401244\n",
      "247 - episode (29 steps) is done with the score of 4.868311382830143\n",
      "248 - episode (28 steps) is done with the score of 3.6286919470876455\n",
      "249 - episode (23 steps) is done with the score of 2.6768699437379837\n",
      "250 - episode (16 steps) is done with the score of 3.2409274950623512\n",
      "251 - episode (24 steps) is done with the score of 3.834227552637458\n",
      "252 - episode (32 steps) is done with the score of 3.5991510786116123\n",
      "253 - episode (40 steps) is done with the score of 5.7195370476692915\n",
      "254 - episode (38 steps) is done with the score of 5.156650457531214\n",
      "255 - episode (24 steps) is done with the score of 3.60834901034832\n",
      "256 - episode (40 steps) is done with the score of 3.445939600467682\n",
      "257 - episode (42 steps) is done with the score of 6.202140128239989\n",
      "258 - episode (26 steps) is done with the score of 3.9971839021891356\n",
      "259 - episode (31 steps) is done with the score of 3.8230227418243885\n",
      "260 - episode (34 steps) is done with the score of 4.575093453750014\n",
      "261 - episode (26 steps) is done with the score of 3.212656546384096\n",
      "262 - episode (48 steps) is done with the score of 6.331121666356921\n",
      "263 - episode (30 steps) is done with the score of 4.4978189673274755\n",
      "264 - episode (33 steps) is done with the score of 4.673510020598769\n",
      "265 - episode (49 steps) is done with the score of 5.240428565070033\n",
      "266 - episode (53 steps) is done with the score of 6.477962095290422\n",
      "267 - episode (28 steps) is done with the score of 3.49472395144403\n",
      "268 - episode (44 steps) is done with the score of 5.160471415147185\n",
      "269 - episode (38 steps) is done with the score of 4.158605773001909\n",
      "270 - episode (27 steps) is done with the score of 4.3773554395884275\n",
      "271 - episode (52 steps) is done with the score of 5.829577993601561\n",
      "272 - episode (38 steps) is done with the score of 5.817731460556388\n",
      "273 - episode (33 steps) is done with the score of 4.730953915044665\n",
      "274 - episode (34 steps) is done with the score of 3.626554124057293\n",
      "275 - episode (19 steps) is done with the score of 2.45767305418849\n",
      "276 - episode (25 steps) is done with the score of 4.494243426248431\n",
      "277 - episode (49 steps) is done with the score of 6.769412023946643\n",
      "278 - episode (48 steps) is done with the score of 7.188861375674605\n",
      "279 - episode (48 steps) is done with the score of 5.217967038974166\n",
      "{1: 2.3828252386301756, 2: 3.8067673798650503, 3: 2.9235613103955984, 4: 5.490142455324531, 5: 4.2979933470487595, 6: 3.2384345065802336, 7: 5.746401933953166, 8: 3.586203671991825, 9: 6.157242467626929, 10: 5.698632104322314, 11: 4.775944149121642, 12: 6.968776527792215, 13: 5.0371911730617285, 14: 3.630731387063861, 15: 7.673298737034202, 16: 4.0265098717063665, 17: 7.062089605256915, 18: 1.3097351491451263, 19: 6.751673569902778, 20: 3.913325121626258, 21: 1.7538037411868572, 22: 4.758567430078983, 23: 4.397446205839515, 24: 7.763181222602725, 25: 4.271806683391333, 26: 2.0989271383732557, 27: 3.217771491035819, 28: 5.870501710101962, 29: 3.8548699896782637, 30: 6.320618776604533, 31: 3.9783944990485907, 32: 4.2650923486799, 33: 5.64634925685823, 34: 6.774726456031203, 35: 5.449243206530809, 36: 5.8535984214395285, 37: 4.85792713612318, 38: 3.7896655574440956, 39: 4.1022323705255985, 40: 4.086685432121158, 41: 4.137756358832121, 42: 4.33986010402441, 43: 4.564908305183053, 44: 6.161633361130953, 45: 6.3082149010151625, 46: 2.432352317497134, 47: 4.21087851934135, 48: 4.931664055213332, 49: 4.547872422263026, 50: 6.7369617354124784, 51: 4.501956021413207, 52: 4.814278166741133, 53: 6.409122360870242, 54: 6.014598844572902, 55: 5.865656038746238, 56: 3.1073319148272276, 57: 2.991884818300605, 58: 4.513049341738224, 59: 3.391875009983778, 60: 3.64115153811872, 61: 4.669862290844321, 62: 4.179192122071981, 63: 4.912420189008117, 64: 3.257411604747176, 65: 5.6514392755925655, 66: 5.086986003443599, 67: 3.877305781468749, 68: 2.5489890351891518, 69: 5.928712969645858, 70: 4.059161961078644, 71: 1.264134231954813, 72: 6.614039493724704, 73: 4.66164549253881, 74: 4.585644580423832, 75: 4.118904989212751, 76: 4.166135726496577, 77: 4.901130801066756, 78: 2.397318882867694, 79: 4.978976979851723, 80: 5.061995958909392, 81: 4.665994836017489, 82: 3.6753427051007748, 83: 4.849048433825374, 84: 4.7625390868633986, 85: 5.726502878591418, 86: 5.863883350044489, 87: 5.793626306578517, 88: 4.0755307506769896, 89: 6.894748969003558, 90: 5.1830099280923605, 91: 4.2436949126422405, 92: 4.868528425693512, 93: 3.9206986110657454, 94: 5.436260174959898, 95: 4.906106559559703, 96: 4.392352377995849, 97: 7.213250823318958, 98: 2.9733157474547625, 99: 5.862498266622424, 100: 6.271435325965285, 101: 4.594351863488555, 102: 5.2018801011145115, 103: 4.633784828707576, 104: 6.060656575486064, 105: 6.834079748019576, 106: 5.9567293878644705, 107: 5.37452688626945, 108: 5.489263281226158, 109: 2.3126733656972647, 110: 4.346412984654307, 111: 4.022658726200461, 112: 2.286105427891016, 113: 3.9530912693589926, 114: 5.1792911775410175, 115: 3.1292978264391422, 116: 5.31607705168426, 117: 4.9407749976962805, 118: 5.719085289165378, 119: 4.2158677112311125, 120: 4.7580887116491795, 121: 1.689981622621417, 122: 1.4396720416843891, 123: 5.078512620180845, 124: 5.8697758093476295, 125: 4.095533778890967, 126: 6.047024412080646, 127: 3.2756121195852757, 128: 6.582668332383037, 129: 4.936709007248282, 130: 5.743313981220126, 131: 4.745214639231563, 132: 6.689854731783271, 133: 4.345158724114299, 134: 6.159011563286185, 135: 4.798401324078441, 136: 2.939769996330142, 137: 5.997441468760371, 138: 6.302228523418307, 139: 6.791688797995448, 140: 7.1936843525618315, 141: 6.711156679317355, 142: 4.341311035677791, 143: 3.9005001224577427, 144: 5.745270315557718, 145: 7.14772898517549, 146: 4.993235787376761, 147: 3.090615663677454, 148: 5.8643351923674345, 149: 3.781650884076953, 150: 4.83281066082418, 151: 5.9971511866897345, 152: 4.314190229400992, 153: 4.38523506373167, 154: 2.8683532010763884, 155: 5.77518779784441, 156: 5.276855502277613, 157: 6.825959151610732, 158: 7.221206387504935, 159: 4.778090229257941, 160: 6.61527157202363, 161: 6.45483416877687, 162: 4.851888796314597, 163: 3.8957527093589306, 164: 6.125403011217713, 165: 6.451379304751754, 166: 4.0641009621322155, 167: 2.8120453394949436, 168: 4.174081226810813, 169: 4.986025778576732, 170: 4.721388569101691, 171: 4.6392386965453625, 172: 4.567765545099974, 173: 6.087094796821475, 174: 5.345200968906283, 175: 6.084118824452162, 176: 5.968202060088515, 177: 5.161834022030234, 178: 3.565450206398964, 179: 4.8135670237243176, 180: 2.4397097919136286, 181: 4.525271682068706, 182: 4.408335529267788, 183: 3.9593470748513937, 184: 2.4696431066840887, 185: 5.514140194281936, 186: 3.0602856762707233, 187: 3.1354957055300474, 188: 6.008779343217611, 189: 4.168867154046893, 190: 6.318540511652827, 191: 2.841002522036433, 192: 2.801583817228675, 193: 4.964147781953216, 194: 4.433654770255089, 195: 3.6668706573545933, 196: 4.34108174405992, 197: 4.268598835915327, 198: 5.158800018951297, 199: 4.323091683909297, 200: 1.679122081026435, 201: 5.8044581320136786, 202: 3.836659735068679, 203: 6.151429167017341, 204: 4.819342730566859, 205: 5.853987826034427, 206: 5.6439765598624945, 207: 4.6780591662973166, 208: 5.018437264487147, 209: 5.729835992679, 210: 6.735716855153441, 211: 4.012296138331294, 212: 4.889397922903299, 213: 6.065690694376826, 214: 4.24600868485868, 215: 5.645885167643428, 216: 3.4560160767287016, 217: 6.63278385810554, 218: 6.04736558906734, 219: 3.1899622287601233, 220: 3.445893755182624, 221: 4.581366604194045, 222: 3.0824976712465286, 223: 3.6789762023836374, 224: 4.190912475809455, 225: 5.211930111050606, 226: 5.370464159175754, 227: 5.213478375226259, 228: 5.269254697486758, 229: 4.47468688711524, 230: 2.338757798075676, 231: 5.457780979573727, 232: 6.0574188362807035, 233: 6.329315511509776, 234: 5.223749255761504, 235: 5.538762100040913, 236: 4.342186963185668, 237: 1.4539634995162487, 238: 6.262550866231322, 239: 4.616295715793967, 240: 6.691015496850014, 241: 5.6957957446575165, 242: 5.66193768940866, 243: 4.383202387019992, 244: 4.048660269007087, 245: 5.704117117449641, 246: 4.908100640401244, 247: 4.868311382830143, 248: 3.6286919470876455, 249: 2.6768699437379837, 250: 3.2409274950623512, 251: 3.834227552637458, 252: 3.5991510786116123, 253: 5.7195370476692915, 254: 5.156650457531214, 255: 3.60834901034832, 256: 3.445939600467682, 257: 6.202140128239989, 258: 3.9971839021891356, 259: 3.8230227418243885, 260: 4.575093453750014, 261: 3.212656546384096, 262: 6.331121666356921, 263: 4.4978189673274755, 264: 4.673510020598769, 265: 5.240428565070033, 266: 6.477962095290422, 267: 3.49472395144403, 268: 5.160471415147185, 269: 4.158605773001909, 270: 4.3773554395884275, 271: 5.829577993601561, 272: 5.817731460556388, 273: 4.730953915044665, 274: 3.626554124057293, 275: 2.45767305418849, 276: 4.494243426248431, 277: 6.769412023946643, 278: 7.188861375674605, 279: 5.217967038974166}\n"
     ]
    }
   ],
   "source": [
    "load_model = True\n",
    "train_mode = False\n",
    "\n",
    "load_path = save_model_path\n",
    "\n",
    "engine_configuration_channel = EngineConfigurationChannel()\n",
    "env = UnityEnvironment(file_name=env_name,\n",
    "                        side_channels=[engine_configuration_channel], base_port=unity_base_port)\n",
    "env.reset()\n",
    "\n",
    "# Setup Unitu MLAgent\n",
    "behavior_name = list(env.behavior_specs.keys())[0]\n",
    "# spec = env.behavior_specs[behavior_name]\n",
    "engine_configuration_channel.set_configuration_parameters(time_scale=1.0)\n",
    "dec, term = env.get_steps(behavior_name)\n",
    "\n",
    "# DQNAgent\n",
    "agent = DDPGAgent()\n",
    "score = 0\n",
    "score_lst = {}\n",
    "episode = 0\n",
    "pre_step = 0\n",
    "for step in range(test_step):\n",
    "    state = dec.obs[0]\n",
    "    action = agent.get_action(state, train_mode)\n",
    "    action_tuple = ActionTuple()\n",
    "    action_tuple.add_continuous(action)\n",
    "    env.set_actions(behavior_name, action_tuple)\n",
    "    env.step()\n",
    "    dec, term = env.get_steps(behavior_name)\n",
    "    done = len(term.agent_id) > 0\n",
    "    reward = term.reward if done else dec.reward\n",
    "    score += reward[0]\n",
    "    if done:\n",
    "        episode +=1\n",
    "        score_lst[episode] = score\n",
    "        print(f\"{episode} - episode ({step - pre_step} steps) is done with the score of {score}\")\n",
    "        pre_step = step\n",
    "        score = 0\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(score_lst)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
