{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70a95c2c",
   "metadata": {},
   "source": [
    "# Drone_PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8ba6fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "\n",
    "# Global Setting\n",
    "cur_dir = os.getcwd()\n",
    "env_dir = os.path.abspath(os.path.join(cur_dir, \"..\", \"Unity6000_Envs\"))\n",
    "output_dir = os.path.abspath(os.path.join(cur_dir, \"temp\", \"mlagents_learn_output\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a62e191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Unity6000_Envs/Drone_Darwin.app\n"
     ]
    }
   ],
   "source": [
    "# Unity Enviroment\n",
    "game = \"Drone\"\n",
    "os_name = platform.system()\n",
    "\n",
    "if os_name == 'Linux':\n",
    "    env_name = os.path.join(env_dir, f\"{game}_{os_name}.x86_64\")\n",
    "elif os_name == 'Darwin':\n",
    "    env_name = os.path.join(env_dir, f\"{game}_{os_name}.app\")\n",
    "env_fp = os.path.join(env_dir, env_name)\n",
    "print(env_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9e227b",
   "metadata": {},
   "source": [
    "## Training PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01b4cca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/config/Drone_ppo.yaml\n",
      "Drone_PPO\n",
      "\n",
      "            ┐  ╖\n",
      "        ╓╖╬│╡  ││╬╖╖\n",
      "    ╓╖╬│││││┘  ╬│││││╬╖\n",
      " ╖╬│││││╬╜        ╙╬│││││╖╖                               ╗╗╗\n",
      " ╬╬╬╬╖││╦╖        ╖╬││╗╣╣╣╬      ╟╣╣╬    ╟╣╣╣             ╜╜╜  ╟╣╣\n",
      " ╬╬╬╬╬╬╬╬╖│╬╖╖╓╬╪│╓╣╣╣╣╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╒╣╣╖╗╣╣╣╗   ╣╣╣ ╣╣╣╣╣╣ ╟╣╣╖   ╣╣╣\n",
      " ╬╬╬╬┐  ╙╬╬╬╬│╓╣╣╣╝╜  ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╣╙ ╙╣╣╣  ╣╣╣ ╙╟╣╣╜╙  ╫╣╣  ╟╣╣\n",
      " ╬╬╬╬┐     ╙╬╬╣╣      ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣     ╣╣╣┌╣╣╜\n",
      " ╬╬╬╜       ╬╬╣╣      ╙╝╣╣╬      ╙╣╣╣╗╖╓╗╣╣╣╜ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣╦╓    ╣╣╣╣╣\n",
      " ╙   ╓╦╖    ╬╬╣╣   ╓╗╗╖            ╙╝╣╣╣╣╝╜   ╘╝╝╜   ╝╝╝  ╝╝╝   ╙╣╣╣    ╟╣╣╣\n",
      "   ╩╬╬╬╬╬╬╦╦╬╬╣╣╗╣╣╣╣╣╣╣╝                                             ╫╣╣╣╣\n",
      "      ╙╬╬╬╬╬╬╬╣╣╣╣╣╣╝╜\n",
      "          ╙╬╬╬╣╣╣╜\n",
      "             ╙\n",
      "        \n",
      " Version information:\n",
      "  ml-agents: 1.1.0,\n",
      "  ml-agents-envs: 1.1.0,\n",
      "  Communicator API: 1.5.0,\n",
      "  PyTorch: 2.8.0\n",
      "[INFO] Connected to Unity environment with package version 4.0.0 and communication version 1.5.0\n",
      "[INFO] Connected new brain: My Behavior?team=0\n",
      "[INFO] Hyperparameters for behavior name My Behavior: \n",
      "\ttrainer_type:\tppo\n",
      "\thyperparameters:\t\n",
      "\t  batch_size:\t128\n",
      "\t  buffer_size:\t1024\n",
      "\t  learning_rate:\t0.0003\n",
      "\t  beta:\t0.01\n",
      "\t  epsilon:\t0.2\n",
      "\t  lambd:\t0.95\n",
      "\t  num_epoch:\t3\n",
      "\t  shared_critic:\tFalse\n",
      "\t  learning_rate_schedule:\tlinear\n",
      "\t  beta_schedule:\tlinear\n",
      "\t  epsilon_schedule:\tlinear\n",
      "\tcheckpoint_interval:\t500000\n",
      "\tnetwork_settings:\t\n",
      "\t  normalize:\tFalse\n",
      "\t  hidden_units:\t256\n",
      "\t  num_layers:\t2\n",
      "\t  vis_encode_type:\tsimple\n",
      "\t  memory:\tNone\n",
      "\t  goal_conditioning_type:\thyper\n",
      "\t  deterministic:\tFalse\n",
      "\treward_signals:\t\n",
      "\t  extrinsic:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t1.0\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t128\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\tinit_path:\tNone\n",
      "\tkeep_checkpoints:\t5\n",
      "\teven_checkpoints:\tFalse\n",
      "\tmax_steps:\t500000\n",
      "\ttime_horizon:\t64\n",
      "\tsummary_freq:\t15000\n",
      "\tthreaded:\tFalse\n",
      "\tself_play:\tNone\n",
      "\tbehavioral_cloning:\tNone\n",
      "[INFO] My Behavior. Step: 15000. Time Elapsed: 30.325 s. Mean Reward: -1.977. Std of Reward: 11.062. Training.\n",
      "[INFO] My Behavior. Step: 30000. Time Elapsed: 55.943 s. Mean Reward: 4.439. Std of Reward: 1.363. Training.\n",
      "[INFO] My Behavior. Step: 45000. Time Elapsed: 81.702 s. Mean Reward: 4.831. Std of Reward: 1.285. Training.\n",
      "[INFO] My Behavior. Step: 60000. Time Elapsed: 107.463 s. Mean Reward: 4.880. Std of Reward: 1.391. Training.\n",
      "[INFO] My Behavior. Step: 75000. Time Elapsed: 133.329 s. Mean Reward: 4.814. Std of Reward: 1.324. Training.\n",
      "[INFO] My Behavior. Step: 90000. Time Elapsed: 159.278 s. Mean Reward: 4.765. Std of Reward: 1.283. Training.\n",
      "[INFO] My Behavior. Step: 105000. Time Elapsed: 185.113 s. Mean Reward: 4.839. Std of Reward: 1.352. Training.\n",
      "[INFO] My Behavior. Step: 120000. Time Elapsed: 211.049 s. Mean Reward: 4.848. Std of Reward: 1.297. Training.\n",
      "[INFO] My Behavior. Step: 135000. Time Elapsed: 236.853 s. Mean Reward: 4.898. Std of Reward: 1.354. Training.\n",
      "[INFO] My Behavior. Step: 150000. Time Elapsed: 262.738 s. Mean Reward: 4.994. Std of Reward: 1.183. Training.\n",
      "[INFO] My Behavior. Step: 165000. Time Elapsed: 288.515 s. Mean Reward: 4.952. Std of Reward: 1.273. Training.\n",
      "[INFO] My Behavior. Step: 180000. Time Elapsed: 314.291 s. Mean Reward: 4.726. Std of Reward: 1.339. Training.\n",
      "[INFO] My Behavior. Step: 195000. Time Elapsed: 340.170 s. Mean Reward: 4.765. Std of Reward: 1.369. Training.\n",
      "[INFO] My Behavior. Step: 210000. Time Elapsed: 365.983 s. Mean Reward: 4.902. Std of Reward: 1.328. Training.\n",
      "[INFO] My Behavior. Step: 225000. Time Elapsed: 391.791 s. Mean Reward: 5.008. Std of Reward: 1.305. Training.\n",
      "[INFO] My Behavior. Step: 240000. Time Elapsed: 417.595 s. Mean Reward: 4.981. Std of Reward: 1.300. Training.\n",
      "[INFO] My Behavior. Step: 255000. Time Elapsed: 443.496 s. Mean Reward: 4.810. Std of Reward: 1.323. Training.\n",
      "[INFO] My Behavior. Step: 270000. Time Elapsed: 469.389 s. Mean Reward: 4.884. Std of Reward: 1.233. Training.\n",
      "[INFO] My Behavior. Step: 285000. Time Elapsed: 495.172 s. Mean Reward: 4.805. Std of Reward: 1.304. Training.\n",
      "[INFO] My Behavior. Step: 300000. Time Elapsed: 520.848 s. Mean Reward: 4.924. Std of Reward: 1.266. Training.\n",
      "[INFO] My Behavior. Step: 315000. Time Elapsed: 546.654 s. Mean Reward: 4.983. Std of Reward: 1.298. Training.\n",
      "[INFO] My Behavior. Step: 330000. Time Elapsed: 572.429 s. Mean Reward: 4.772. Std of Reward: 1.304. Training.\n",
      "[INFO] My Behavior. Step: 345000. Time Elapsed: 598.298 s. Mean Reward: 4.824. Std of Reward: 1.304. Training.\n",
      "[INFO] My Behavior. Step: 360000. Time Elapsed: 624.063 s. Mean Reward: 4.847. Std of Reward: 1.385. Training.\n",
      "[INFO] My Behavior. Step: 375000. Time Elapsed: 649.788 s. Mean Reward: 4.905. Std of Reward: 1.326. Training.\n",
      "[INFO] My Behavior. Step: 390000. Time Elapsed: 675.588 s. Mean Reward: 4.883. Std of Reward: 1.308. Training.\n",
      "[INFO] My Behavior. Step: 405000. Time Elapsed: 701.333 s. Mean Reward: 4.942. Std of Reward: 1.197. Training.\n",
      "[INFO] My Behavior. Step: 420000. Time Elapsed: 727.078 s. Mean Reward: 5.002. Std of Reward: 1.216. Training.\n",
      "[INFO] My Behavior. Step: 435000. Time Elapsed: 753.002 s. Mean Reward: 4.897. Std of Reward: 1.331. Training.\n",
      "[INFO] My Behavior. Step: 450000. Time Elapsed: 778.769 s. Mean Reward: 4.915. Std of Reward: 1.306. Training.\n",
      "[INFO] My Behavior. Step: 465000. Time Elapsed: 804.503 s. Mean Reward: 4.886. Std of Reward: 1.358. Training.\n",
      "[INFO] My Behavior. Step: 480000. Time Elapsed: 830.354 s. Mean Reward: 4.847. Std of Reward: 1.321. Training.\n",
      "[INFO] My Behavior. Step: 495000. Time Elapsed: 856.131 s. Mean Reward: 4.914. Std of Reward: 1.318. Training.\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Drone_PPO/My Behavior/My Behavior-499977.onnx\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Drone_PPO/My Behavior/My Behavior-500013.onnx\n",
      "[INFO] Copied /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Drone_PPO/My Behavior/My Behavior-500013.onnx to /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Drone_PPO/My Behavior.onnx.\n"
     ]
    }
   ],
   "source": [
    "config_ppo_fp = os.path.join(cur_dir, \"config\", \"Drone_ppo.yaml\")\n",
    "run_ppo_id = \"Drone_PPO\"\n",
    "print(config_ppo_fp)\n",
    "print(run_ppo_id)\n",
    "\n",
    "!mlagents-learn $config_ppo_fp \\\n",
    "               --env=$env_fp \\\n",
    "               --results-dir=$output_dir \\\n",
    "               --run-id=$run_ppo_id --base-port=1990\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a47b032",
   "metadata": {},
   "source": [
    "## Training SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6886efe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/config/Drone_ppo.yaml\n",
      "Drone_SAC\n",
      "\n",
      "            ┐  ╖\n",
      "        ╓╖╬│╡  ││╬╖╖\n",
      "    ╓╖╬│││││┘  ╬│││││╬╖\n",
      " ╖╬│││││╬╜        ╙╬│││││╖╖                               ╗╗╗\n",
      " ╬╬╬╬╖││╦╖        ╖╬││╗╣╣╣╬      ╟╣╣╬    ╟╣╣╣             ╜╜╜  ╟╣╣\n",
      " ╬╬╬╬╬╬╬╬╖│╬╖╖╓╬╪│╓╣╣╣╣╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╒╣╣╖╗╣╣╣╗   ╣╣╣ ╣╣╣╣╣╣ ╟╣╣╖   ╣╣╣\n",
      " ╬╬╬╬┐  ╙╬╬╬╬│╓╣╣╣╝╜  ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╣╙ ╙╣╣╣  ╣╣╣ ╙╟╣╣╜╙  ╫╣╣  ╟╣╣\n",
      " ╬╬╬╬┐     ╙╬╬╣╣      ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣     ╣╣╣┌╣╣╜\n",
      " ╬╬╬╜       ╬╬╣╣      ╙╝╣╣╬      ╙╣╣╣╗╖╓╗╣╣╣╜ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣╦╓    ╣╣╣╣╣\n",
      " ╙   ╓╦╖    ╬╬╣╣   ╓╗╗╖            ╙╝╣╣╣╣╝╜   ╘╝╝╜   ╝╝╝  ╝╝╝   ╙╣╣╣    ╟╣╣╣\n",
      "   ╩╬╬╬╬╬╬╦╦╬╬╣╣╗╣╣╣╣╣╣╣╝                                             ╫╣╣╣╣\n",
      "      ╙╬╬╬╬╬╬╬╣╣╣╣╣╣╝╜\n",
      "          ╙╬╬╬╣╣╣╜\n",
      "             ╙\n",
      "        \n",
      " Version information:\n",
      "  ml-agents: 1.1.0,\n",
      "  ml-agents-envs: 1.1.0,\n",
      "  Communicator API: 1.5.0,\n",
      "  PyTorch: 2.8.0\n",
      "[INFO] Connected to Unity environment with package version 4.0.0 and communication version 1.5.0\n",
      "[INFO] Connected new brain: My Behavior?team=0\n",
      "[INFO] Hyperparameters for behavior name My Behavior: \n",
      "\ttrainer_type:\tppo\n",
      "\thyperparameters:\t\n",
      "\t  batch_size:\t128\n",
      "\t  buffer_size:\t1024\n",
      "\t  learning_rate:\t0.0003\n",
      "\t  beta:\t0.01\n",
      "\t  epsilon:\t0.2\n",
      "\t  lambd:\t0.95\n",
      "\t  num_epoch:\t3\n",
      "\t  shared_critic:\tFalse\n",
      "\t  learning_rate_schedule:\tlinear\n",
      "\t  beta_schedule:\tlinear\n",
      "\t  epsilon_schedule:\tlinear\n",
      "\tcheckpoint_interval:\t500000\n",
      "\tnetwork_settings:\t\n",
      "\t  normalize:\tFalse\n",
      "\t  hidden_units:\t256\n",
      "\t  num_layers:\t2\n",
      "\t  vis_encode_type:\tsimple\n",
      "\t  memory:\tNone\n",
      "\t  goal_conditioning_type:\thyper\n",
      "\t  deterministic:\tFalse\n",
      "\treward_signals:\t\n",
      "\t  extrinsic:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t1.0\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t128\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\tinit_path:\tNone\n",
      "\tkeep_checkpoints:\t5\n",
      "\teven_checkpoints:\tFalse\n",
      "\tmax_steps:\t500000\n",
      "\ttime_horizon:\t64\n",
      "\tsummary_freq:\t15000\n",
      "\tthreaded:\tFalse\n",
      "\tself_play:\tNone\n",
      "\tbehavioral_cloning:\tNone\n",
      "[INFO] My Behavior. Step: 15000. Time Elapsed: 30.420 s. Mean Reward: 0.712. Std of Reward: 10.026. Training.\n",
      "[INFO] My Behavior. Step: 30000. Time Elapsed: 56.010 s. Mean Reward: 4.644. Std of Reward: 1.304. Training.\n",
      "[INFO] My Behavior. Step: 45000. Time Elapsed: 81.805 s. Mean Reward: 4.759. Std of Reward: 1.318. Training.\n",
      "[INFO] My Behavior. Step: 60000. Time Elapsed: 107.429 s. Mean Reward: 4.792. Std of Reward: 1.364. Training.\n",
      "[INFO] My Behavior. Step: 75000. Time Elapsed: 133.276 s. Mean Reward: 4.763. Std of Reward: 1.269. Training.\n",
      "[INFO] My Behavior. Step: 90000. Time Elapsed: 159.117 s. Mean Reward: 4.988. Std of Reward: 1.321. Training.\n",
      "[INFO] My Behavior. Step: 105000. Time Elapsed: 185.046 s. Mean Reward: 4.790. Std of Reward: 1.322. Training.\n",
      "[INFO] My Behavior. Step: 120000. Time Elapsed: 210.759 s. Mean Reward: 4.744. Std of Reward: 1.383. Training.\n",
      "[INFO] My Behavior. Step: 135000. Time Elapsed: 236.506 s. Mean Reward: 4.871. Std of Reward: 1.288. Training.\n",
      "[INFO] My Behavior. Step: 150000. Time Elapsed: 262.378 s. Mean Reward: 4.842. Std of Reward: 1.424. Training.\n",
      "[INFO] My Behavior. Step: 165000. Time Elapsed: 288.190 s. Mean Reward: 4.734. Std of Reward: 1.301. Training.\n",
      "[INFO] My Behavior. Step: 180000. Time Elapsed: 314.050 s. Mean Reward: 4.921. Std of Reward: 1.302. Training.\n",
      "[INFO] My Behavior. Step: 195000. Time Elapsed: 339.912 s. Mean Reward: 4.778. Std of Reward: 1.328. Training.\n",
      "[INFO] My Behavior. Step: 210000. Time Elapsed: 365.653 s. Mean Reward: 4.817. Std of Reward: 1.313. Training.\n",
      "[INFO] My Behavior. Step: 225000. Time Elapsed: 391.421 s. Mean Reward: 4.849. Std of Reward: 1.240. Training.\n",
      "[INFO] My Behavior. Step: 240000. Time Elapsed: 417.521 s. Mean Reward: 4.829. Std of Reward: 1.339. Training.\n",
      "[INFO] My Behavior. Step: 255000. Time Elapsed: 443.440 s. Mean Reward: 5.057. Std of Reward: 1.282. Training.\n",
      "[INFO] My Behavior. Step: 270000. Time Elapsed: 469.529 s. Mean Reward: 4.842. Std of Reward: 1.406. Training.\n",
      "[INFO] My Behavior. Step: 285000. Time Elapsed: 495.653 s. Mean Reward: 4.947. Std of Reward: 1.311. Training.\n",
      "[INFO] My Behavior. Step: 300000. Time Elapsed: 521.589 s. Mean Reward: 4.908. Std of Reward: 1.313. Training.\n",
      "[INFO] My Behavior. Step: 315000. Time Elapsed: 547.604 s. Mean Reward: 4.887. Std of Reward: 1.240. Training.\n",
      "[INFO] My Behavior. Step: 330000. Time Elapsed: 573.391 s. Mean Reward: 4.909. Std of Reward: 1.290. Training.\n",
      "[INFO] My Behavior. Step: 345000. Time Elapsed: 599.262 s. Mean Reward: 4.907. Std of Reward: 1.213. Training.\n",
      "[INFO] My Behavior. Step: 360000. Time Elapsed: 625.285 s. Mean Reward: 4.780. Std of Reward: 1.377. Training.\n",
      "[INFO] My Behavior. Step: 375000. Time Elapsed: 651.212 s. Mean Reward: 4.941. Std of Reward: 1.266. Training.\n",
      "[INFO] My Behavior. Step: 390000. Time Elapsed: 677.172 s. Mean Reward: 4.880. Std of Reward: 1.246. Training.\n",
      "[INFO] My Behavior. Step: 405000. Time Elapsed: 703.191 s. Mean Reward: 4.893. Std of Reward: 1.345. Training.\n",
      "[INFO] My Behavior. Step: 420000. Time Elapsed: 729.129 s. Mean Reward: 4.915. Std of Reward: 1.286. Training.\n",
      "[INFO] My Behavior. Step: 435000. Time Elapsed: 755.022 s. Mean Reward: 4.898. Std of Reward: 1.270. Training.\n",
      "[INFO] My Behavior. Step: 450000. Time Elapsed: 781.023 s. Mean Reward: 4.895. Std of Reward: 1.304. Training.\n",
      "[INFO] My Behavior. Step: 465000. Time Elapsed: 806.926 s. Mean Reward: 4.985. Std of Reward: 1.275. Training.\n",
      "[INFO] My Behavior. Step: 480000. Time Elapsed: 832.794 s. Mean Reward: 4.931. Std of Reward: 1.320. Training.\n",
      "[INFO] My Behavior. Step: 495000. Time Elapsed: 858.833 s. Mean Reward: 5.015. Std of Reward: 1.345. Training.\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Drone_SAC/My Behavior/My Behavior-499966.onnx\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Drone_SAC/My Behavior/My Behavior-500006.onnx\n",
      "[INFO] Copied /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Drone_SAC/My Behavior/My Behavior-500006.onnx to /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Drone_SAC/My Behavior.onnx.\n"
     ]
    }
   ],
   "source": [
    "config_sac_fp = os.path.join(cur_dir, \"config\", \"Drone_sac.yaml\")\n",
    "run_sac_id = \"Drone_SAC\"\n",
    "print(config_ppo_fp)\n",
    "print(run_sac_id)\n",
    "\n",
    "!mlagents-learn $config_ppo_fp \\\n",
    "               --env=$env_fp \\\n",
    "               --results-dir=$output_dir \\\n",
    "               --run-id=$run_sac_id --base-port=1990"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40c13bd",
   "metadata": {},
   "source": [
    "## Training POCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "694c6520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/config/Drone_poca.yaml\n",
      "Drone_POCA\n",
      "\n",
      "            ┐  ╖\n",
      "        ╓╖╬│╡  ││╬╖╖\n",
      "    ╓╖╬│││││┘  ╬│││││╬╖\n",
      " ╖╬│││││╬╜        ╙╬│││││╖╖                               ╗╗╗\n",
      " ╬╬╬╬╖││╦╖        ╖╬││╗╣╣╣╬      ╟╣╣╬    ╟╣╣╣             ╜╜╜  ╟╣╣\n",
      " ╬╬╬╬╬╬╬╬╖│╬╖╖╓╬╪│╓╣╣╣╣╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╒╣╣╖╗╣╣╣╗   ╣╣╣ ╣╣╣╣╣╣ ╟╣╣╖   ╣╣╣\n",
      " ╬╬╬╬┐  ╙╬╬╬╬│╓╣╣╣╝╜  ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╣╙ ╙╣╣╣  ╣╣╣ ╙╟╣╣╜╙  ╫╣╣  ╟╣╣\n",
      " ╬╬╬╬┐     ╙╬╬╣╣      ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣     ╣╣╣┌╣╣╜\n",
      " ╬╬╬╜       ╬╬╣╣      ╙╝╣╣╬      ╙╣╣╣╗╖╓╗╣╣╣╜ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣╦╓    ╣╣╣╣╣\n",
      " ╙   ╓╦╖    ╬╬╣╣   ╓╗╗╖            ╙╝╣╣╣╣╝╜   ╘╝╝╜   ╝╝╝  ╝╝╝   ╙╣╣╣    ╟╣╣╣\n",
      "   ╩╬╬╬╬╬╬╦╦╬╬╣╣╗╣╣╣╣╣╣╣╝                                             ╫╣╣╣╣\n",
      "      ╙╬╬╬╬╬╬╬╣╣╣╣╣╣╝╜\n",
      "          ╙╬╬╬╣╣╣╜\n",
      "             ╙\n",
      "        \n",
      " Version information:\n",
      "  ml-agents: 1.1.0,\n",
      "  ml-agents-envs: 1.1.0,\n",
      "  Communicator API: 1.5.0,\n",
      "  PyTorch: 2.8.0\n",
      "[INFO] Connected to Unity environment with package version 4.0.0 and communication version 1.5.0\n",
      "[INFO] Connected new brain: My Behavior?team=0\n",
      "[INFO] Hyperparameters for behavior name My Behavior: \n",
      "\ttrainer_type:\tpoca\n",
      "\thyperparameters:\t\n",
      "\t  batch_size:\t128\n",
      "\t  buffer_size:\t1024\n",
      "\t  learning_rate:\t0.0003\n",
      "\t  beta:\t0.01\n",
      "\t  epsilon:\t0.2\n",
      "\t  lambd:\t0.95\n",
      "\t  num_epoch:\t3\n",
      "\t  learning_rate_schedule:\tlinear\n",
      "\t  beta_schedule:\tlinear\n",
      "\t  epsilon_schedule:\tlinear\n",
      "\tcheckpoint_interval:\t500000\n",
      "\tnetwork_settings:\t\n",
      "\t  normalize:\tFalse\n",
      "\t  hidden_units:\t256\n",
      "\t  num_layers:\t2\n",
      "\t  vis_encode_type:\tsimple\n",
      "\t  memory:\tNone\n",
      "\t  goal_conditioning_type:\thyper\n",
      "\t  deterministic:\tFalse\n",
      "\treward_signals:\t\n",
      "\t  extrinsic:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t1.0\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t128\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\tinit_path:\tNone\n",
      "\tkeep_checkpoints:\t5\n",
      "\teven_checkpoints:\tFalse\n",
      "\tmax_steps:\t500000\n",
      "\ttime_horizon:\t64\n",
      "\tsummary_freq:\t15000\n",
      "\tthreaded:\tFalse\n",
      "\tself_play:\tNone\n",
      "\tbehavioral_cloning:\tNone\n",
      "[INFO] My Behavior. Step: 15000. Time Elapsed: 32.911 s. Mean Reward: 0.421. Mean Group Reward: 0.000. Training.\n",
      "[INFO] My Behavior. Step: 30000. Time Elapsed: 61.662 s. Mean Reward: 4.213. Mean Group Reward: 0.000. Training.\n",
      "[INFO] My Behavior. Step: 45000. Time Elapsed: 90.724 s. Mean Reward: 4.682. Mean Group Reward: 0.000. Training.\n",
      "[INFO] My Behavior. Step: 60000. Time Elapsed: 119.642 s. Mean Reward: 4.608. Mean Group Reward: 0.000. Training.\n",
      "[INFO] My Behavior. Step: 75000. Time Elapsed: 148.450 s. Mean Reward: 4.755. Mean Group Reward: 0.000. Training.\n",
      "[INFO] My Behavior. Step: 90000. Time Elapsed: 177.772 s. Mean Reward: 4.906. Mean Group Reward: 0.000. Training.\n",
      "[INFO] My Behavior. Step: 105000. Time Elapsed: 206.734 s. Mean Reward: 4.775. Mean Group Reward: 0.000. Training.\n",
      "[INFO] My Behavior. Step: 120000. Time Elapsed: 235.990 s. Mean Reward: 4.927. Mean Group Reward: 0.000. Training.\n",
      "[INFO] My Behavior. Step: 135000. Time Elapsed: 265.079 s. Mean Reward: 4.905. Mean Group Reward: 0.000. Training.\n",
      "[INFO] My Behavior. Step: 150000. Time Elapsed: 294.193 s. Mean Reward: 4.849. Mean Group Reward: 0.000. Training.\n",
      "[INFO] My Behavior. Step: 165000. Time Elapsed: 323.621 s. Mean Reward: 5.003. Mean Group Reward: 0.000. Training.\n",
      "[INFO] My Behavior. Step: 180000. Time Elapsed: 352.668 s. Mean Reward: 4.846. Mean Group Reward: 0.000. Training.\n",
      "[INFO] My Behavior. Step: 195000. Time Elapsed: 381.726 s. Mean Reward: 4.964. Mean Group Reward: 0.000. Training.\n",
      "[INFO] My Behavior. Step: 210000. Time Elapsed: 411.057 s. Mean Reward: 4.940. Mean Group Reward: 0.000. Training.\n",
      "[INFO] My Behavior. Step: 225000. Time Elapsed: 440.190 s. Mean Reward: 4.839. Mean Group Reward: 0.000. Training.\n",
      "[INFO] My Behavior. Step: 240000. Time Elapsed: 469.252 s. Mean Reward: 4.806. Mean Group Reward: 0.000. Training.\n",
      "[INFO] My Behavior. Step: 255000. Time Elapsed: 498.563 s. Mean Reward: 4.926. Mean Group Reward: 0.000. Training.\n",
      "[INFO] My Behavior. Step: 270000. Time Elapsed: 527.533 s. Mean Reward: 4.893. Mean Group Reward: 0.000. Training.\n",
      "[INFO] My Behavior. Step: 285000. Time Elapsed: 556.876 s. Mean Reward: 4.891. Mean Group Reward: 0.000. Training.\n",
      "[INFO] My Behavior. Step: 300000. Time Elapsed: 585.985 s. Mean Reward: 4.854. Mean Group Reward: 0.000. Training.\n",
      "[INFO] My Behavior. Step: 315000. Time Elapsed: 615.044 s. Mean Reward: 4.910. Mean Group Reward: 0.000. Training.\n",
      "[INFO] My Behavior. Step: 330000. Time Elapsed: 644.452 s. Mean Reward: 4.882. Mean Group Reward: 0.000. Training.\n",
      "[INFO] My Behavior. Step: 345000. Time Elapsed: 673.458 s. Mean Reward: 4.902. Mean Group Reward: 0.000. Training.\n",
      "[INFO] My Behavior. Step: 360000. Time Elapsed: 702.524 s. Mean Reward: 4.972. Mean Group Reward: 0.000. Training.\n",
      "[INFO] My Behavior. Step: 375000. Time Elapsed: 731.881 s. Mean Reward: 4.952. Mean Group Reward: 0.000. Training.\n",
      "[INFO] My Behavior. Step: 390000. Time Elapsed: 760.982 s. Mean Reward: 4.861. Mean Group Reward: 0.000. Training.\n",
      "[INFO] My Behavior. Step: 405000. Time Elapsed: 790.251 s. Mean Reward: 5.002. Mean Group Reward: 0.000. Training.\n",
      "[INFO] My Behavior. Step: 420000. Time Elapsed: 819.454 s. Mean Reward: 4.950. Mean Group Reward: 0.000. Training.\n",
      "[INFO] My Behavior. Step: 435000. Time Elapsed: 848.918 s. Mean Reward: 4.920. Mean Group Reward: 0.000. Training.\n",
      "[INFO] My Behavior. Step: 450000. Time Elapsed: 877.947 s. Mean Reward: 5.038. Mean Group Reward: 0.000. Training.\n",
      "[INFO] My Behavior. Step: 465000. Time Elapsed: 907.061 s. Mean Reward: 4.828. Mean Group Reward: 0.000. Training.\n",
      "[INFO] My Behavior. Step: 480000. Time Elapsed: 936.435 s. Mean Reward: 4.911. Mean Group Reward: 0.000. Training.\n",
      "[INFO] My Behavior. Step: 495000. Time Elapsed: 965.547 s. Mean Reward: 4.926. Mean Group Reward: 0.000. Training.\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Drone_POCA/My Behavior/My Behavior-499976.onnx\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Drone_POCA/My Behavior/My Behavior-500006.onnx\n",
      "[INFO] Copied /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Drone_POCA/My Behavior/My Behavior-500006.onnx to /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Drone_POCA/My Behavior.onnx.\n"
     ]
    }
   ],
   "source": [
    "config_poca_fp = os.path.join(cur_dir, \"config\", \"Drone_poca.yaml\")\n",
    "run_poca_id = \"Drone_POCA\"\n",
    "print(config_poca_fp)\n",
    "print(run_poca_id)\n",
    "\n",
    "!mlagents-learn $config_poca_fp \\\n",
    "               --env=$env_fp \\\n",
    "               --results-dir=$output_dir \\\n",
    "               --run-id=$run_poca_id --base-port=1990"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad33878",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f5c2e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mlagents-learn $config_fp \\\n",
    "#                --env=$env_fp \\\n",
    "#                --results-dir=$test_dir \\\n",
    "#                --run-id=$run_id \\\n",
    "#                --resume --inference\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
