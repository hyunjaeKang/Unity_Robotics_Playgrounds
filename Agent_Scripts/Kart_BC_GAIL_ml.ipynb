{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70a95c2c",
   "metadata": {},
   "source": [
    "# Kart_BC_GAIL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8c6c95",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8ba6fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "\n",
    "# Global Setting\n",
    "cur_dir = os.getcwd()\n",
    "env_dir = os.path.abspath(os.path.join(cur_dir, \"..\", \"Unity6000_Envs\"))\n",
    "output_dir = os.path.abspath(os.path.join(cur_dir, \"temp\", \"mlagents_learn_output\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a62e191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Unity6000_Envs/Kart_Darwin.app\n"
     ]
    }
   ],
   "source": [
    "# Unity Enviroment\n",
    "game = \"Kart\"\n",
    "os_name = platform.system()\n",
    "\n",
    "if os_name == 'Linux':\n",
    "    env_name = os.path.join(env_dir, f\"{game}_{os_name}.x86_64\")\n",
    "elif os_name == 'Darwin':\n",
    "    env_name = os.path.join(env_dir, f\"{game}_{os_name}.app\")\n",
    "env_fp = os.path.join(env_dir, env_name)\n",
    "print(env_fp)\n",
    "baseport = 1091"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9e227b",
   "metadata": {},
   "source": [
    "## Training PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fb8ef05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/config/Kart_ppo_BC_GAIL.yaml\n",
      "Kart_PPO_BC_GAIL\n",
      "\n",
      "            ┐  ╖\n",
      "        ╓╖╬│╡  ││╬╖╖\n",
      "    ╓╖╬│││││┘  ╬│││││╬╖\n",
      " ╖╬│││││╬╜        ╙╬│││││╖╖                               ╗╗╗\n",
      " ╬╬╬╬╖││╦╖        ╖╬││╗╣╣╣╬      ╟╣╣╬    ╟╣╣╣             ╜╜╜  ╟╣╣\n",
      " ╬╬╬╬╬╬╬╬╖│╬╖╖╓╬╪│╓╣╣╣╣╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╒╣╣╖╗╣╣╣╗   ╣╣╣ ╣╣╣╣╣╣ ╟╣╣╖   ╣╣╣\n",
      " ╬╬╬╬┐  ╙╬╬╬╬│╓╣╣╣╝╜  ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╣╙ ╙╣╣╣  ╣╣╣ ╙╟╣╣╜╙  ╫╣╣  ╟╣╣\n",
      " ╬╬╬╬┐     ╙╬╬╣╣      ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣     ╣╣╣┌╣╣╜\n",
      " ╬╬╬╜       ╬╬╣╣      ╙╝╣╣╬      ╙╣╣╣╗╖╓╗╣╣╣╜ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣╦╓    ╣╣╣╣╣\n",
      " ╙   ╓╦╖    ╬╬╣╣   ╓╗╗╖            ╙╝╣╣╣╣╝╜   ╘╝╝╜   ╝╝╝  ╝╝╝   ╙╣╣╣    ╟╣╣╣\n",
      "   ╩╬╬╬╬╬╬╦╦╬╬╣╣╗╣╣╣╣╣╣╣╝                                             ╫╣╣╣╣\n",
      "      ╙╬╬╬╬╬╬╬╣╣╣╣╣╣╝╜\n",
      "          ╙╬╬╬╣╣╣╜\n",
      "             ╙\n",
      "        \n",
      " Version information:\n",
      "  ml-agents: 1.1.0,\n",
      "  ml-agents-envs: 1.1.0,\n",
      "  Communicator API: 1.5.0,\n",
      "  PyTorch: 2.8.0\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/bin/mlagents-learn\", line 7, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/site-packages/mlagents/trainers/learn.py\", line 270, in main\n",
      "    run_cli(parse_command_line())\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/site-packages/mlagents/trainers/learn.py\", line 266, in run_cli\n",
      "    run_training(run_seed, options, num_areas)\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/site-packages/mlagents/trainers/learn.py\", line 75, in run_training\n",
      "    validate_existing_directories(\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/site-packages/mlagents/trainers/directory_utils.py\", line 25, in validate_existing_directories\n",
      "    raise UnityTrainerException(\n",
      "mlagents.trainers.exception.UnityTrainerException: Previous data from this run ID was found. Either specify a new run ID, use --resume to resume this run, or use the --force parameter to overwrite existing data.\n"
     ]
    }
   ],
   "source": [
    "config_ppo_fp = os.path.join(cur_dir, \"config\", \"Kart_ppo_BC_GAIL.yaml\")\n",
    "run_ppo_id = \"Kart_PPO_BC_GAIL\"\n",
    "print(config_ppo_fp)\n",
    "print(run_ppo_id)\n",
    "\n",
    "# !mlagents-learn $config_ppo_fp \\\n",
    "#                --env=$env_fp \\\n",
    "#                --results-dir=$output_dir \\\n",
    "#                --run-id=$run_ppo_id --base-port=$baseport\n",
    "\n",
    "!mlagents-learn $config_ppo_fp \\\n",
    "               --results-dir=$output_dir \\\n",
    "               --run-id=$run_ppo_id --base-port=$baseport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31a4a2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            ┐  ╖\n",
      "        ╓╖╬│╡  ││╬╖╖\n",
      "    ╓╖╬│││││┘  ╬│││││╬╖\n",
      " ╖╬│││││╬╜        ╙╬│││││╖╖                               ╗╗╗\n",
      " ╬╬╬╬╖││╦╖        ╖╬││╗╣╣╣╬      ╟╣╣╬    ╟╣╣╣             ╜╜╜  ╟╣╣\n",
      " ╬╬╬╬╬╬╬╬╖│╬╖╖╓╬╪│╓╣╣╣╣╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╒╣╣╖╗╣╣╣╗   ╣╣╣ ╣╣╣╣╣╣ ╟╣╣╖   ╣╣╣\n",
      " ╬╬╬╬┐  ╙╬╬╬╬│╓╣╣╣╝╜  ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╣╙ ╙╣╣╣  ╣╣╣ ╙╟╣╣╜╙  ╫╣╣  ╟╣╣\n",
      " ╬╬╬╬┐     ╙╬╬╣╣      ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣     ╣╣╣┌╣╣╜\n",
      " ╬╬╬╜       ╬╬╣╣      ╙╝╣╣╬      ╙╣╣╣╗╖╓╗╣╣╣╜ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣╦╓    ╣╣╣╣╣\n",
      " ╙   ╓╦╖    ╬╬╣╣   ╓╗╗╖            ╙╝╣╣╣╣╝╜   ╘╝╝╜   ╝╝╝  ╝╝╝   ╙╣╣╣    ╟╣╣╣\n",
      "   ╩╬╬╬╬╬╬╦╦╬╬╣╣╗╣╣╣╣╣╣╣╝                                             ╫╣╣╣╣\n",
      "      ╙╬╬╬╬╬╬╬╣╣╣╣╣╣╝╜\n",
      "          ╙╬╬╬╣╣╣╜\n",
      "             ╙\n",
      "        \n",
      " Version information:\n",
      "  ml-agents: 1.1.0,\n",
      "  ml-agents-envs: 1.1.0,\n",
      "  Communicator API: 1.5.0,\n",
      "  PyTorch: 2.8.0\n",
      "[INFO] Listening on port 5004. Start training by pressing the Play button in the Unity Editor.\n",
      "^C\n",
      "Exception ignored in atexit callback: <function _exit_function at 0x15330eb90>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/multiprocessing/util.py\", line 357, in _exit_function\n",
      "    p.join()\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/multiprocessing/popen_fork.py\", line 43, in wait\n",
      "    return self.poll(os.WNOHANG if timeout == 0.0 else 0)\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/multiprocessing/popen_fork.py\", line 27, in poll\n",
      "    pid, sts = os.waitpid(self.pid, flag)\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "!mlagents-learn $config_ppo_fp \\\n",
    "               --results-dir=$output_dir \\\n",
    "               --run-id=$run_ppo_id --base-port=$baseport --resume --inference --time-scale=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a47b032",
   "metadata": {},
   "source": [
    "## Training SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6886efe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/config/Kart_sac_BC_GAIL.yaml\n",
      "Kart_SAC_BC_GAIL\n",
      "\n",
      "            ┐  ╖\n",
      "        ╓╖╬│╡  ││╬╖╖\n",
      "    ╓╖╬│││││┘  ╬│││││╬╖\n",
      " ╖╬│││││╬╜        ╙╬│││││╖╖                               ╗╗╗\n",
      " ╬╬╬╬╖││╦╖        ╖╬││╗╣╣╣╬      ╟╣╣╬    ╟╣╣╣             ╜╜╜  ╟╣╣\n",
      " ╬╬╬╬╬╬╬╬╖│╬╖╖╓╬╪│╓╣╣╣╣╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╒╣╣╖╗╣╣╣╗   ╣╣╣ ╣╣╣╣╣╣ ╟╣╣╖   ╣╣╣\n",
      " ╬╬╬╬┐  ╙╬╬╬╬│╓╣╣╣╝╜  ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╣╙ ╙╣╣╣  ╣╣╣ ╙╟╣╣╜╙  ╫╣╣  ╟╣╣\n",
      " ╬╬╬╬┐     ╙╬╬╣╣      ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣     ╣╣╣┌╣╣╜\n",
      " ╬╬╬╜       ╬╬╣╣      ╙╝╣╣╬      ╙╣╣╣╗╖╓╗╣╣╣╜ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣╦╓    ╣╣╣╣╣\n",
      " ╙   ╓╦╖    ╬╬╣╣   ╓╗╗╖            ╙╝╣╣╣╣╝╜   ╘╝╝╜   ╝╝╝  ╝╝╝   ╙╣╣╣    ╟╣╣╣\n",
      "   ╩╬╬╬╬╬╬╦╦╬╬╣╣╗╣╣╣╣╣╣╣╝                                             ╫╣╣╣╣\n",
      "      ╙╬╬╬╬╬╬╬╣╣╣╣╣╣╝╜\n",
      "          ╙╬╬╬╣╣╣╜\n",
      "             ╙\n",
      "        \n",
      " Version information:\n",
      "  ml-agents: 1.1.0,\n",
      "  ml-agents-envs: 1.1.0,\n",
      "  Communicator API: 1.5.0,\n",
      "  PyTorch: 2.8.0\n",
      "[INFO] Listening on port 5004. Start training by pressing the Play button in the Unity Editor.\n",
      "[INFO] Connected to Unity environment with package version 4.0.0 and communication version 1.5.0\n",
      "[INFO] Connected new brain: ArcadeDriver?team=0\n",
      "[INFO] Hyperparameters for behavior name ArcadeDriver: \n",
      "\ttrainer_type:\tsac\n",
      "\thyperparameters:\t\n",
      "\t  learning_rate:\t0.0003\n",
      "\t  learning_rate_schedule:\tlinear\n",
      "\t  batch_size:\t128\n",
      "\t  buffer_size:\t1024\n",
      "\t  buffer_init_steps:\t0\n",
      "\t  tau:\t0.005\n",
      "\t  steps_per_update:\t10.0\n",
      "\t  save_replay_buffer:\tFalse\n",
      "\t  init_entcoef:\t0.5\n",
      "\t  reward_signal_steps_per_update:\t10.0\n",
      "\tcheckpoint_interval:\t500000\n",
      "\tnetwork_settings:\t\n",
      "\t  normalize:\tTrue\n",
      "\t  hidden_units:\t256\n",
      "\t  num_layers:\t5\n",
      "\t  vis_encode_type:\tsimple\n",
      "\t  memory:\tNone\n",
      "\t  goal_conditioning_type:\thyper\n",
      "\t  deterministic:\tFalse\n",
      "\treward_signals:\t\n",
      "\t  extrinsic:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t1.0\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t128\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\t  gail:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t0.05\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t64\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\t    learning_rate:\t0.0003\n",
      "\t    encoding_size:\tNone\n",
      "\t    use_actions:\tTrue\n",
      "\t    use_vail:\tFalse\n",
      "\t    demo_path:\tdemo/KartAgent.demo\n",
      "\tinit_path:\tNone\n",
      "\tkeep_checkpoints:\t5\n",
      "\teven_checkpoints:\tFalse\n",
      "\tmax_steps:\t500000\n",
      "\ttime_horizon:\t64\n",
      "\tsummary_freq:\t15000\n",
      "\tthreaded:\tFalse\n",
      "\tself_play:\tNone\n",
      "\tbehavioral_cloning:\t\n",
      "\t  demo_path:\tdemo/KartAgent.demo\n",
      "\t  steps:\t0\n",
      "\t  strength:\t0.05\n",
      "\t  samples_per_update:\t512\n",
      "\t  num_epoch:\tNone\n",
      "\t  batch_size:\tNone\n",
      "[INFO] ArcadeDriver. Step: 15000. Time Elapsed: 156.648 s. Mean Reward: -5.930. Std of Reward: 0.561. Training.\n",
      "[INFO] ArcadeDriver. Step: 30000. Time Elapsed: 308.290 s. Mean Reward: -6.001. Std of Reward: 0.081. Training.\n",
      "[INFO] ArcadeDriver. Step: 45000. Time Elapsed: 459.509 s. Mean Reward: -6.011. Std of Reward: 0.111. Training.\n",
      "[INFO] ArcadeDriver. Step: 60000. Time Elapsed: 610.676 s. Mean Reward: -6.001. Std of Reward: 0.079. Training.\n",
      "[INFO] ArcadeDriver. Step: 75000. Time Elapsed: 761.871 s. Mean Reward: -6.002. Std of Reward: 0.072. Training.\n",
      "[INFO] ArcadeDriver. Step: 90000. Time Elapsed: 912.896 s. Mean Reward: -5.995. Std of Reward: 0.108. Training.\n",
      "[INFO] ArcadeDriver. Step: 105000. Time Elapsed: 1064.247 s. Mean Reward: -6.006. Std of Reward: 0.041. Training.\n",
      "[INFO] ArcadeDriver. Step: 120000. Time Elapsed: 1215.323 s. Mean Reward: -5.999. Std of Reward: 0.094. Training.\n",
      "[INFO] ArcadeDriver. Step: 135000. Time Elapsed: 1366.571 s. Mean Reward: -5.999. Std of Reward: 0.085. Training.\n",
      "[INFO] ArcadeDriver. Step: 150000. Time Elapsed: 1517.720 s. Mean Reward: -5.983. Std of Reward: 0.143. Training.\n",
      "[INFO] ArcadeDriver. Step: 165000. Time Elapsed: 1668.922 s. Mean Reward: -5.975. Std of Reward: 0.167. Training.\n",
      "[INFO] ArcadeDriver. Step: 180000. Time Elapsed: 1820.160 s. Mean Reward: -6.003. Std of Reward: 0.030. Training.\n",
      "[INFO] ArcadeDriver. Step: 195000. Time Elapsed: 1971.279 s. Mean Reward: -6.000. Std of Reward: 0.030. Training.\n",
      "[INFO] ArcadeDriver. Step: 210000. Time Elapsed: 2122.334 s. Mean Reward: -5.996. Std of Reward: 0.029. Training.\n",
      "[INFO] ArcadeDriver. Step: 225000. Time Elapsed: 2273.302 s. Mean Reward: -5.996. Std of Reward: 0.028. Training.\n",
      "[INFO] ArcadeDriver. Step: 240000. Time Elapsed: 2424.387 s. Mean Reward: -5.996. Std of Reward: 0.024. Training.\n",
      "[INFO] ArcadeDriver. Step: 255000. Time Elapsed: 2575.706 s. Mean Reward: -5.996. Std of Reward: 0.031. Training.\n",
      "[INFO] ArcadeDriver. Step: 270000. Time Elapsed: 2727.387 s. Mean Reward: -6.012. Std of Reward: 0.047. Training.\n",
      "[INFO] ArcadeDriver. Step: 285000. Time Elapsed: 2878.932 s. Mean Reward: -6.001. Std of Reward: 0.132. Training.\n",
      "[INFO] ArcadeDriver. Step: 300000. Time Elapsed: 3030.375 s. Mean Reward: -6.015. Std of Reward: 0.023. Training.\n",
      "[INFO] ArcadeDriver. Step: 315000. Time Elapsed: 3181.998 s. Mean Reward: -6.020. Std of Reward: 0.034. Training.\n",
      "[INFO] ArcadeDriver. Step: 330000. Time Elapsed: 3333.536 s. Mean Reward: -6.019. Std of Reward: 0.047. Training.\n",
      "[INFO] ArcadeDriver. Step: 345000. Time Elapsed: 3485.103 s. Mean Reward: -6.013. Std of Reward: 0.027. Training.\n",
      "[INFO] ArcadeDriver. Step: 360000. Time Elapsed: 3636.517 s. Mean Reward: -6.017. Std of Reward: 0.027. Training.\n",
      "[INFO] ArcadeDriver. Step: 375000. Time Elapsed: 3788.050 s. Mean Reward: -6.012. Std of Reward: 0.018. Training.\n",
      "[INFO] ArcadeDriver. Step: 390000. Time Elapsed: 3939.806 s. Mean Reward: -6.009. Std of Reward: 0.019. Training.\n",
      "[INFO] ArcadeDriver. Step: 405000. Time Elapsed: 4091.442 s. Mean Reward: -6.008. Std of Reward: 0.115. Training.\n",
      "[INFO] ArcadeDriver. Step: 420000. Time Elapsed: 4242.934 s. Mean Reward: -6.013. Std of Reward: 0.074. Training.\n",
      "[INFO] ArcadeDriver. Step: 435000. Time Elapsed: 4394.484 s. Mean Reward: -6.010. Std of Reward: 0.014. Training.\n",
      "[INFO] ArcadeDriver. Step: 450000. Time Elapsed: 4545.962 s. Mean Reward: -6.010. Std of Reward: 0.013. Training.\n",
      "[INFO] ArcadeDriver. Step: 465000. Time Elapsed: 4697.431 s. Mean Reward: -6.013. Std of Reward: 0.023. Training.\n",
      "[INFO] ArcadeDriver. Step: 480000. Time Elapsed: 4848.876 s. Mean Reward: -6.010. Std of Reward: 0.012. Training.\n",
      "[INFO] ArcadeDriver. Step: 495000. Time Elapsed: 5000.295 s. Mean Reward: -6.016. Std of Reward: 0.013. Training.\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_SAC_BC_GAIL/ArcadeDriver/ArcadeDriver-499984.onnx\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_SAC_BC_GAIL/ArcadeDriver/ArcadeDriver-500001.onnx\n",
      "[INFO] Copied /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_SAC_BC_GAIL/ArcadeDriver/ArcadeDriver-500001.onnx to /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_SAC_BC_GAIL/ArcadeDriver.onnx.\n"
     ]
    }
   ],
   "source": [
    "config_sac_fp = os.path.join(cur_dir, \"config\", \"Kart_sac_BC_GAIL.yaml\")\n",
    "run_sac_id = \"Kart_SAC_BC_GAIL\"\n",
    "print(config_sac_fp)\n",
    "print(run_sac_id)\n",
    "\n",
    "# !mlagents-learn $config_sac_fp \\\n",
    "#                --env=$env_fp \\\n",
    "#                --results-dir=$output_dir \\\n",
    "#                --run-id=$run_sac_id --base-port=$baseport\n",
    "\n",
    "!mlagents-learn $config_sac_fp \\\n",
    "               --results-dir=$output_dir \\\n",
    "               --run-id=$run_sac_id --base-port=$baseport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f38a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            ┐  ╖\n",
      "        ╓╖╬│╡  ││╬╖╖\n",
      "    ╓╖╬│││││┘  ╬│││││╬╖\n",
      " ╖╬│││││╬╜        ╙╬│││││╖╖                               ╗╗╗\n",
      " ╬╬╬╬╖││╦╖        ╖╬││╗╣╣╣╬      ╟╣╣╬    ╟╣╣╣             ╜╜╜  ╟╣╣\n",
      " ╬╬╬╬╬╬╬╬╖│╬╖╖╓╬╪│╓╣╣╣╣╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╒╣╣╖╗╣╣╣╗   ╣╣╣ ╣╣╣╣╣╣ ╟╣╣╖   ╣╣╣\n",
      " ╬╬╬╬┐  ╙╬╬╬╬│╓╣╣╣╝╜  ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╣╙ ╙╣╣╣  ╣╣╣ ╙╟╣╣╜╙  ╫╣╣  ╟╣╣\n",
      " ╬╬╬╬┐     ╙╬╬╣╣      ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣     ╣╣╣┌╣╣╜\n",
      " ╬╬╬╜       ╬╬╣╣      ╙╝╣╣╬      ╙╣╣╣╗╖╓╗╣╣╣╜ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣╦╓    ╣╣╣╣╣\n",
      " ╙   ╓╦╖    ╬╬╣╣   ╓╗╗╖            ╙╝╣╣╣╣╝╜   ╘╝╝╜   ╝╝╝  ╝╝╝   ╙╣╣╣    ╟╣╣╣\n",
      "   ╩╬╬╬╬╬╬╦╦╬╬╣╣╗╣╣╣╣╣╣╣╝                                             ╫╣╣╣╣\n",
      "      ╙╬╬╬╬╬╬╬╣╣╣╣╣╣╝╜\n",
      "          ╙╬╬╬╣╣╣╜\n",
      "             ╙\n",
      "        \n",
      " Version information:\n",
      "  ml-agents: 1.1.0,\n",
      "  ml-agents-envs: 1.1.0,\n",
      "  Communicator API: 1.5.0,\n",
      "  PyTorch: 2.8.0\n",
      "[INFO] Listening on port 5004. Start training by pressing the Play button in the Unity Editor.\n",
      "[INFO] Connected to Unity environment with package version 4.0.0 and communication version 1.5.0\n",
      "[INFO] Connected new brain: ArcadeDriver?team=0\n",
      "[INFO] Hyperparameters for behavior name ArcadeDriver: \n",
      "\ttrainer_type:\tsac\n",
      "\thyperparameters:\t\n",
      "\t  learning_rate:\t0.0003\n",
      "\t  learning_rate_schedule:\tlinear\n",
      "\t  batch_size:\t128\n",
      "\t  buffer_size:\t1024\n",
      "\t  buffer_init_steps:\t0\n",
      "\t  tau:\t0.005\n",
      "\t  steps_per_update:\t10.0\n",
      "\t  save_replay_buffer:\tFalse\n",
      "\t  init_entcoef:\t0.5\n",
      "\t  reward_signal_steps_per_update:\t10.0\n",
      "\tcheckpoint_interval:\t500000\n",
      "\tnetwork_settings:\t\n",
      "\t  normalize:\tTrue\n",
      "\t  hidden_units:\t256\n",
      "\t  num_layers:\t5\n",
      "\t  vis_encode_type:\tsimple\n",
      "\t  memory:\tNone\n",
      "\t  goal_conditioning_type:\thyper\n",
      "\t  deterministic:\tFalse\n",
      "\treward_signals:\t\n",
      "\t  extrinsic:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t1.0\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t128\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\t  gail:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t0.05\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t64\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\t    learning_rate:\t0.0003\n",
      "\t    encoding_size:\tNone\n",
      "\t    use_actions:\tTrue\n",
      "\t    use_vail:\tFalse\n",
      "\t    demo_path:\tdemo/KartAgent.demo\n",
      "\tinit_path:\tNone\n",
      "\tkeep_checkpoints:\t5\n",
      "\teven_checkpoints:\tFalse\n",
      "\tmax_steps:\t500000\n",
      "\ttime_horizon:\t64\n",
      "\tsummary_freq:\t15000\n",
      "\tthreaded:\tFalse\n",
      "\tself_play:\tNone\n",
      "\tbehavioral_cloning:\t\n",
      "\t  demo_path:\tdemo/KartAgent.demo\n",
      "\t  steps:\t0\n",
      "\t  strength:\t0.05\n",
      "\t  samples_per_update:\t512\n",
      "\t  num_epoch:\tNone\n",
      "\t  batch_size:\tNone\n",
      "[INFO] Resuming from /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_SAC_BC_GAIL/ArcadeDriver.\n",
      "[INFO] Resuming training from step 500001.\n",
      "[INFO] ArcadeDriver. Step: 510000. Time Elapsed: 42.658 s. Mean Reward: 6.981. Std of Reward: 0.014. Not Training.\n",
      "[INFO] ArcadeDriver. Step: 525000. Time Elapsed: 87.872 s. Mean Reward: 6.979. Std of Reward: 0.000. Not Training.\n",
      "[INFO] ArcadeDriver. Step: 540000. Time Elapsed: 133.448 s. Mean Reward: 6.979. Std of Reward: 0.000. Not Training.\n",
      "^C\n",
      "Exception ignored in atexit callback: <function _exit_function at 0x136d0eb90>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/multiprocessing/util.py\", line 357, in _exit_function\n",
      "    p.join()\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/multiprocessing/popen_fork.py\", line 43, in wait\n",
      "    return self.poll(os.WNOHANG if timeout == 0.0 else 0)\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/multiprocessing/popen_fork.py\", line 27, in poll\n",
      "    pid, sts = os.waitpid(self.pid, flag)\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "# !mlagents-learn $config_ppo_fp \\\n",
    "#                --results-dir=$output_dir \\\n",
    "#                --run-id=$run_sac_id --base-port=$baseport --resume --inference --time-scale=1\n",
    "\n",
    "\n",
    "!mlagents-learn $config_sac_fp \\\n",
    "               --results-dir=$output_dir \\\n",
    "               --run-id=$run_sac_id --base-port=$baseport --resume --inference --time-scale=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40c13bd",
   "metadata": {},
   "source": [
    "## Training POCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "694c6520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/config/Kart_poca_BC_GAIL.yaml\n",
      "Kart_POCA_BC_GAIL\n",
      "\n",
      "            ┐  ╖\n",
      "        ╓╖╬│╡  ││╬╖╖\n",
      "    ╓╖╬│││││┘  ╬│││││╬╖\n",
      " ╖╬│││││╬╜        ╙╬│││││╖╖                               ╗╗╗\n",
      " ╬╬╬╬╖││╦╖        ╖╬││╗╣╣╣╬      ╟╣╣╬    ╟╣╣╣             ╜╜╜  ╟╣╣\n",
      " ╬╬╬╬╬╬╬╬╖│╬╖╖╓╬╪│╓╣╣╣╣╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╒╣╣╖╗╣╣╣╗   ╣╣╣ ╣╣╣╣╣╣ ╟╣╣╖   ╣╣╣\n",
      " ╬╬╬╬┐  ╙╬╬╬╬│╓╣╣╣╝╜  ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╣╙ ╙╣╣╣  ╣╣╣ ╙╟╣╣╜╙  ╫╣╣  ╟╣╣\n",
      " ╬╬╬╬┐     ╙╬╬╣╣      ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣     ╣╣╣┌╣╣╜\n",
      " ╬╬╬╜       ╬╬╣╣      ╙╝╣╣╬      ╙╣╣╣╗╖╓╗╣╣╣╜ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣╦╓    ╣╣╣╣╣\n",
      " ╙   ╓╦╖    ╬╬╣╣   ╓╗╗╖            ╙╝╣╣╣╣╝╜   ╘╝╝╜   ╝╝╝  ╝╝╝   ╙╣╣╣    ╟╣╣╣\n",
      "   ╩╬╬╬╬╬╬╦╦╬╬╣╣╗╣╣╣╣╣╣╣╝                                             ╫╣╣╣╣\n",
      "      ╙╬╬╬╬╬╬╬╣╣╣╣╣╣╝╜\n",
      "          ╙╬╬╬╣╣╣╜\n",
      "             ╙\n",
      "        \n",
      " Version information:\n",
      "  ml-agents: 1.1.0,\n",
      "  ml-agents-envs: 1.1.0,\n",
      "  Communicator API: 1.5.0,\n",
      "  PyTorch: 2.8.0\n",
      "[INFO] Listening on port 5004. Start training by pressing the Play button in the Unity Editor.\n",
      "[INFO] Connected to Unity environment with package version 4.0.0 and communication version 1.5.0\n",
      "[INFO] Connected new brain: ArcadeDriver?team=0\n",
      "[WARNING] Deleting TensorBoard data events.out.tfevents.1758376153.mac.lan.32438.0 that was left over from a previous run.\n",
      "[INFO] Hyperparameters for behavior name ArcadeDriver: \n",
      "\ttrainer_type:\tpoca\n",
      "\thyperparameters:\t\n",
      "\t  batch_size:\t128\n",
      "\t  buffer_size:\t1024\n",
      "\t  learning_rate:\t0.0003\n",
      "\t  beta:\t0.01\n",
      "\t  epsilon:\t0.2\n",
      "\t  lambd:\t0.95\n",
      "\t  num_epoch:\t3\n",
      "\t  learning_rate_schedule:\tlinear\n",
      "\t  beta_schedule:\tlinear\n",
      "\t  epsilon_schedule:\tlinear\n",
      "\tcheckpoint_interval:\t500000\n",
      "\tnetwork_settings:\t\n",
      "\t  normalize:\tTrue\n",
      "\t  hidden_units:\t256\n",
      "\t  num_layers:\t5\n",
      "\t  vis_encode_type:\tsimple\n",
      "\t  memory:\tNone\n",
      "\t  goal_conditioning_type:\thyper\n",
      "\t  deterministic:\tFalse\n",
      "\treward_signals:\t\n",
      "\t  extrinsic:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t1.0\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t128\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\t  gail:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t0.05\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t64\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\t    learning_rate:\t0.0003\n",
      "\t    encoding_size:\tNone\n",
      "\t    use_actions:\tTrue\n",
      "\t    use_vail:\tFalse\n",
      "\t    demo_path:\tdemo/KartAgent.demo\n",
      "\tinit_path:\tNone\n",
      "\tkeep_checkpoints:\t5\n",
      "\teven_checkpoints:\tFalse\n",
      "\tmax_steps:\t500000\n",
      "\ttime_horizon:\t64\n",
      "\tsummary_freq:\t15000\n",
      "\tthreaded:\tFalse\n",
      "\tself_play:\tNone\n",
      "\tbehavioral_cloning:\t\n",
      "\t  demo_path:\tdemo/KartAgent.demo\n",
      "\t  steps:\t0\n",
      "\t  strength:\t0.05\n",
      "\t  samples_per_update:\t512\n",
      "\t  num_epoch:\tNone\n",
      "\t  batch_size:\tNone\n",
      "[WARNING] Reward signal Gail is not supported with the POCA trainer; results may be unexpected.\n",
      "[INFO] ArcadeDriver. Step: 15000. Time Elapsed: 60.623 s. Mean Reward: 0.639. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 30000. Time Elapsed: 112.512 s. Mean Reward: 28.891. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 45000. Time Elapsed: 164.362 s. Mean Reward: 20.818. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 60000. Time Elapsed: 216.725 s. Mean Reward: 18.872. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 75000. Time Elapsed: 268.693 s. Mean Reward: 6.768. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 90000. Time Elapsed: 321.142 s. Mean Reward: 12.377. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 105000. Time Elapsed: 374.186 s. Mean Reward: 16.871. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 120000. Time Elapsed: 427.414 s. Mean Reward: 20.456. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 135000. Time Elapsed: 481.418 s. Mean Reward: 17.374. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 150000. Time Elapsed: 535.105 s. Mean Reward: 35.260. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 165000. Time Elapsed: 589.016 s. Mean Reward: 14.427. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 180000. Time Elapsed: 643.113 s. Mean Reward: 19.976. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 195000. Time Elapsed: 697.344 s. Mean Reward: 35.380. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 210000. Time Elapsed: 751.627 s. Mean Reward: 34.778. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 225000. Time Elapsed: 806.187 s. Mean Reward: 31.858. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 240000. Time Elapsed: 860.555 s. Mean Reward: 24.065. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 255000. Time Elapsed: 915.036 s. Mean Reward: 13.101. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 270000. Time Elapsed: 969.344 s. Mean Reward: 11.375. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 285000. Time Elapsed: 1023.828 s. Mean Reward: 13.255. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 300000. Time Elapsed: 1078.412 s. Mean Reward: 29.216. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 315000. Time Elapsed: 1132.821 s. Mean Reward: 13.477. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 330000. Time Elapsed: 1187.532 s. Mean Reward: 16.093. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 345000. Time Elapsed: 1242.092 s. Mean Reward: 11.560. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 360000. Time Elapsed: 1296.830 s. Mean Reward: 9.382. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 375000. Time Elapsed: 1351.273 s. Mean Reward: 11.866. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 390000. Time Elapsed: 1405.752 s. Mean Reward: 17.994. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 405000. Time Elapsed: 1460.033 s. Mean Reward: 19.612. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 420000. Time Elapsed: 1514.640 s. Mean Reward: 13.610. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 435000. Time Elapsed: 1569.590 s. Mean Reward: 16.728. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 450000. Time Elapsed: 1624.057 s. Mean Reward: 29.732. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 465000. Time Elapsed: 1678.395 s. Mean Reward: 10.328. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 480000. Time Elapsed: 1732.996 s. Mean Reward: 16.424. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 495000. Time Elapsed: 1787.288 s. Mean Reward: 8.342. Mean Group Reward: 0.000. Training.\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_POCA_BC_GAIL/ArcadeDriver/ArcadeDriver-499941.onnx\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_POCA_BC_GAIL/ArcadeDriver/ArcadeDriver-500005.onnx\n",
      "[INFO] Copied /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_POCA_BC_GAIL/ArcadeDriver/ArcadeDriver-500005.onnx to /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_POCA_BC_GAIL/ArcadeDriver.onnx.\n"
     ]
    }
   ],
   "source": [
    "config_poca_fp = os.path.join(cur_dir, \"config\", \"Kart_poca_BC_GAIL.yaml\")\n",
    "run_poca_id = \"Kart_POCA_BC_GAIL\"\n",
    "print(config_poca_fp)\n",
    "print(run_poca_id)\n",
    "\n",
    "# !mlagents-learn $config_poca_fp \\\n",
    "#                --env=$env_fp \\\n",
    "#                --results-dir=$output_dir \\\n",
    "#                --run-id=$run_poca_id --base-port=$baseport\n",
    "\n",
    "!mlagents-learn $config_poca_fp \\\n",
    "               --results-dir=$output_dir \\\n",
    "               --run-id=$run_poca_id --base-port=$baseport  --time-scale=1 --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35e109ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            ┐  ╖\n",
      "        ╓╖╬│╡  ││╬╖╖\n",
      "    ╓╖╬│││││┘  ╬│││││╬╖\n",
      " ╖╬│││││╬╜        ╙╬│││││╖╖                               ╗╗╗\n",
      " ╬╬╬╬╖││╦╖        ╖╬││╗╣╣╣╬      ╟╣╣╬    ╟╣╣╣             ╜╜╜  ╟╣╣\n",
      " ╬╬╬╬╬╬╬╬╖│╬╖╖╓╬╪│╓╣╣╣╣╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╒╣╣╖╗╣╣╣╗   ╣╣╣ ╣╣╣╣╣╣ ╟╣╣╖   ╣╣╣\n",
      " ╬╬╬╬┐  ╙╬╬╬╬│╓╣╣╣╝╜  ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╣╙ ╙╣╣╣  ╣╣╣ ╙╟╣╣╜╙  ╫╣╣  ╟╣╣\n",
      " ╬╬╬╬┐     ╙╬╬╣╣      ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣     ╣╣╣┌╣╣╜\n",
      " ╬╬╬╜       ╬╬╣╣      ╙╝╣╣╬      ╙╣╣╣╗╖╓╗╣╣╣╜ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣╦╓    ╣╣╣╣╣\n",
      " ╙   ╓╦╖    ╬╬╣╣   ╓╗╗╖            ╙╝╣╣╣╣╝╜   ╘╝╝╜   ╝╝╝  ╝╝╝   ╙╣╣╣    ╟╣╣╣\n",
      "   ╩╬╬╬╬╬╬╦╦╬╬╣╣╗╣╣╣╣╣╣╣╝                                             ╫╣╣╣╣\n",
      "      ╙╬╬╬╬╬╬╬╣╣╣╣╣╣╝╜\n",
      "          ╙╬╬╬╣╣╣╜\n",
      "             ╙\n",
      "        \n",
      " Version information:\n",
      "  ml-agents: 1.1.0,\n",
      "  ml-agents-envs: 1.1.0,\n",
      "  Communicator API: 1.5.0,\n",
      "  PyTorch: 2.8.0\n",
      "[INFO] Listening on port 5004. Start training by pressing the Play button in the Unity Editor.\n",
      "[INFO] Connected to Unity environment with package version 4.0.0 and communication version 1.5.0\n",
      "[INFO] Connected new brain: ArcadeDriver?team=0\n",
      "[INFO] Hyperparameters for behavior name ArcadeDriver: \n",
      "\ttrainer_type:\tpoca\n",
      "\thyperparameters:\t\n",
      "\t  batch_size:\t128\n",
      "\t  buffer_size:\t1024\n",
      "\t  learning_rate:\t0.0003\n",
      "\t  beta:\t0.01\n",
      "\t  epsilon:\t0.2\n",
      "\t  lambd:\t0.95\n",
      "\t  num_epoch:\t3\n",
      "\t  learning_rate_schedule:\tlinear\n",
      "\t  beta_schedule:\tlinear\n",
      "\t  epsilon_schedule:\tlinear\n",
      "\tcheckpoint_interval:\t500000\n",
      "\tnetwork_settings:\t\n",
      "\t  normalize:\tTrue\n",
      "\t  hidden_units:\t256\n",
      "\t  num_layers:\t5\n",
      "\t  vis_encode_type:\tsimple\n",
      "\t  memory:\tNone\n",
      "\t  goal_conditioning_type:\thyper\n",
      "\t  deterministic:\tFalse\n",
      "\treward_signals:\t\n",
      "\t  extrinsic:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t1.0\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t128\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\t  gail:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t0.05\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t64\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\t    learning_rate:\t0.0003\n",
      "\t    encoding_size:\tNone\n",
      "\t    use_actions:\tTrue\n",
      "\t    use_vail:\tFalse\n",
      "\t    demo_path:\tdemo/KartAgent.demo\n",
      "\tinit_path:\tNone\n",
      "\tkeep_checkpoints:\t5\n",
      "\teven_checkpoints:\tFalse\n",
      "\tmax_steps:\t500000\n",
      "\ttime_horizon:\t64\n",
      "\tsummary_freq:\t15000\n",
      "\tthreaded:\tFalse\n",
      "\tself_play:\tNone\n",
      "\tbehavioral_cloning:\t\n",
      "\t  demo_path:\tdemo/KartAgent.demo\n",
      "\t  steps:\t0\n",
      "\t  strength:\t0.05\n",
      "\t  samples_per_update:\t512\n",
      "\t  num_epoch:\tNone\n",
      "\t  batch_size:\tNone\n",
      "[WARNING] Reward signal Gail is not supported with the POCA trainer; results may be unexpected.\n",
      "[INFO] Resuming from /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_POCA_BC_GAIL/ArcadeDriver.\n",
      "[INFO] Resuming training from step 500005.\n",
      "[INFO] ArcadeDriver. Step: 510000. Time Elapsed: 40.377 s. Mean Reward: 1.029. Mean Group Reward: 0.000. Not Training.\n",
      "[INFO] ArcadeDriver. Step: 525000. Time Elapsed: 86.482 s. Mean Reward: 1.231. Mean Group Reward: 0.000. Not Training.\n",
      "[INFO] ArcadeDriver. Step: 540000. Time Elapsed: 133.045 s. Mean Reward: 0.592. Mean Group Reward: 0.000. Not Training.\n",
      "[INFO] ArcadeDriver. Step: 555000. Time Elapsed: 180.778 s. Mean Reward: 1.405. Mean Group Reward: 0.000. Not Training.\n",
      "[INFO] ArcadeDriver. Step: 570000. Time Elapsed: 228.020 s. Mean Reward: 1.297. Mean Group Reward: 0.000. Not Training.\n",
      "[INFO] ArcadeDriver. Step: 585000. Time Elapsed: 274.524 s. Mean Reward: 2.154. Mean Group Reward: 0.000. Not Training.\n",
      "[INFO] ArcadeDriver. Step: 600000. Time Elapsed: 321.385 s. Mean Reward: 0.941. Mean Group Reward: 0.000. Not Training.\n",
      "[INFO] ArcadeDriver. Step: 615000. Time Elapsed: 369.348 s. Mean Reward: 0.593. Mean Group Reward: 0.000. Not Training.\n",
      "[INFO] ArcadeDriver. Step: 630000. Time Elapsed: 417.454 s. Mean Reward: 2.115. Mean Group Reward: 0.000. Not Training.\n",
      "[INFO] ArcadeDriver. Step: 645000. Time Elapsed: 465.920 s. Mean Reward: 0.804. Mean Group Reward: 0.000. Not Training.\n",
      "[INFO] ArcadeDriver. Step: 660000. Time Elapsed: 514.552 s. Mean Reward: 1.066. Mean Group Reward: 0.000. Not Training.\n",
      "[INFO] ArcadeDriver. Step: 675000. Time Elapsed: 563.006 s. Mean Reward: 2.502. Mean Group Reward: 0.000. Not Training.\n",
      "[INFO] ArcadeDriver. Step: 690000. Time Elapsed: 611.780 s. Mean Reward: 1.184. Mean Group Reward: 0.000. Not Training.\n",
      "[INFO] ArcadeDriver. Step: 705000. Time Elapsed: 660.350 s. Mean Reward: 0.963. Mean Group Reward: 0.000. Not Training.\n",
      "[INFO] ArcadeDriver. Step: 720000. Time Elapsed: 709.061 s. Mean Reward: 1.143. Mean Group Reward: 0.000. Not Training.\n",
      "[INFO] ArcadeDriver. Step: 735000. Time Elapsed: 757.893 s. Mean Reward: 1.506. Mean Group Reward: 0.000. Not Training.\n",
      "[INFO] ArcadeDriver. Step: 750000. Time Elapsed: 806.690 s. Mean Reward: 0.961. Mean Group Reward: 0.000. Not Training.\n",
      "[INFO] ArcadeDriver. Step: 765000. Time Elapsed: 855.200 s. Mean Reward: 0.656. Mean Group Reward: 0.000. Not Training.\n",
      "[INFO] ArcadeDriver. Step: 780000. Time Elapsed: 903.833 s. Mean Reward: 1.713. Mean Group Reward: 0.000. Not Training.\n",
      "[INFO] ArcadeDriver. Step: 795000. Time Elapsed: 952.548 s. Mean Reward: 1.161. Mean Group Reward: 0.000. Not Training.\n",
      "[INFO] ArcadeDriver. Step: 810000. Time Elapsed: 1001.370 s. Mean Reward: 2.011. Mean Group Reward: 0.000. Not Training.\n",
      "[INFO] ArcadeDriver. Step: 825000. Time Elapsed: 1049.986 s. Mean Reward: 2.399. Mean Group Reward: 0.000. Not Training.\n",
      "[INFO] ArcadeDriver. Step: 840000. Time Elapsed: 1098.654 s. Mean Reward: 0.735. Mean Group Reward: 0.000. Not Training.\n",
      "[INFO] ArcadeDriver. Step: 855000. Time Elapsed: 1147.372 s. Mean Reward: 1.932. Mean Group Reward: 0.000. Not Training.\n",
      "[INFO] ArcadeDriver. Step: 870000. Time Elapsed: 1195.941 s. Mean Reward: 1.188. Mean Group Reward: 0.000. Not Training.\n",
      "[INFO] ArcadeDriver. Step: 885000. Time Elapsed: 1244.743 s. Mean Reward: 1.377. Mean Group Reward: 0.000. Not Training.\n",
      "[INFO] ArcadeDriver. Step: 900000. Time Elapsed: 1293.352 s. Mean Reward: 2.429. Mean Group Reward: 0.000. Not Training.\n",
      "[WARNING] Restarting worker[0] after 'Communicator has exited.'\n",
      "[INFO] Listening on port 5004. Start training by pressing the Play button in the Unity Editor.\n",
      "^C\n",
      "Exception ignored in atexit callback: <function _exit_function at 0x13050eb90>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/multiprocessing/util.py\", line 357, in _exit_function\n",
      "    p.join()\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/multiprocessing/popen_fork.py\", line 43, in wait\n",
      "    return self.poll(os.WNOHANG if timeout == 0.0 else 0)\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/multiprocessing/popen_fork.py\", line 27, in poll\n",
      "    pid, sts = os.waitpid(self.pid, flag)\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "!mlagents-learn $config_poca_fp \\\n",
    "               --results-dir=$output_dir \\\n",
    "               --run-id=$run_poca_id --base-port=$baseport --resume --inference --time-scale=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad33878",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5c2e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mlagents-learn $config_fp \\\n",
    "#                --env=$env_fp \\\n",
    "#                --results-dir=$test_dir \\\n",
    "#                --run-id=$run_id \\\n",
    "#                --resume --inference\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
