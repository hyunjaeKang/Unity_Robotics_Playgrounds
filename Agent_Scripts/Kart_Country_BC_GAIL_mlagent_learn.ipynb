{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70a95c2c",
   "metadata": {},
   "source": [
    "# Kart_BC_GAIL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8c6c95",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8ba6fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "\n",
    "# Global Setting\n",
    "cur_dir = os.getcwd()\n",
    "env_dir = os.path.abspath(os.path.join(cur_dir, \"..\", \"Unity6000_Envs\"))\n",
    "output_dir = os.path.abspath(os.path.join(cur_dir, \"temp\", \"mlagents_learn_output\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a62e191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Unity6000_Envs/Kart_Darwin.app\n"
     ]
    }
   ],
   "source": [
    "# Unity Enviroment\n",
    "game = \"Kart\"\n",
    "os_name = platform.system()\n",
    "\n",
    "if os_name == 'Linux':\n",
    "    env_name = os.path.join(env_dir, f\"{game}_{os_name}.x86_64\")\n",
    "elif os_name == 'Darwin':\n",
    "    env_name = os.path.join(env_dir, f\"{game}_{os_name}.app\")\n",
    "env_fp = os.path.join(env_dir, env_name)\n",
    "print(env_fp)\n",
    "baseport = 1091"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9e227b",
   "metadata": {},
   "source": [
    "## Training PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fb8ef05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/config/Kart_ppo_BC_GAIL.yaml\n",
      "Kart_Country_PPO_BC_GAIL\n",
      "\n",
      "            ┐  ╖\n",
      "        ╓╖╬│╡  ││╬╖╖\n",
      "    ╓╖╬│││││┘  ╬│││││╬╖\n",
      " ╖╬│││││╬╜        ╙╬│││││╖╖                               ╗╗╗\n",
      " ╬╬╬╬╖││╦╖        ╖╬││╗╣╣╣╬      ╟╣╣╬    ╟╣╣╣             ╜╜╜  ╟╣╣\n",
      " ╬╬╬╬╬╬╬╬╖│╬╖╖╓╬╪│╓╣╣╣╣╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╒╣╣╖╗╣╣╣╗   ╣╣╣ ╣╣╣╣╣╣ ╟╣╣╖   ╣╣╣\n",
      " ╬╬╬╬┐  ╙╬╬╬╬│╓╣╣╣╝╜  ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╣╙ ╙╣╣╣  ╣╣╣ ╙╟╣╣╜╙  ╫╣╣  ╟╣╣\n",
      " ╬╬╬╬┐     ╙╬╬╣╣      ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣     ╣╣╣┌╣╣╜\n",
      " ╬╬╬╜       ╬╬╣╣      ╙╝╣╣╬      ╙╣╣╣╗╖╓╗╣╣╣╜ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣╦╓    ╣╣╣╣╣\n",
      " ╙   ╓╦╖    ╬╬╣╣   ╓╗╗╖            ╙╝╣╣╣╣╝╜   ╘╝╝╜   ╝╝╝  ╝╝╝   ╙╣╣╣    ╟╣╣╣\n",
      "   ╩╬╬╬╬╬╬╦╦╬╬╣╣╗╣╣╣╣╣╣╣╝                                             ╫╣╣╣╣\n",
      "      ╙╬╬╬╬╬╬╬╣╣╣╣╣╣╝╜\n",
      "          ╙╬╬╬╣╣╣╜\n",
      "             ╙\n",
      "        \n",
      " Version information:\n",
      "  ml-agents: 1.1.0,\n",
      "  ml-agents-envs: 1.1.0,\n",
      "  Communicator API: 1.5.0,\n",
      "  PyTorch: 2.8.0\n",
      "[INFO] Listening on port 5004. Start training by pressing the Play button in the Unity Editor.\n",
      "[INFO] Connected to Unity environment with package version 4.0.0 and communication version 1.5.0\n",
      "[INFO] Connected new brain: ArcadeDriver?team=0\n",
      "[WARNING] Deleting TensorBoard data events.out.tfevents.1758381704.mac.lan.37321.0 that was left over from a previous run.\n",
      "[INFO] Hyperparameters for behavior name ArcadeDriver: \n",
      "\ttrainer_type:\tppo\n",
      "\thyperparameters:\t\n",
      "\t  batch_size:\t128\n",
      "\t  buffer_size:\t1024\n",
      "\t  learning_rate:\t0.0003\n",
      "\t  beta:\t0.01\n",
      "\t  epsilon:\t0.2\n",
      "\t  lambd:\t0.95\n",
      "\t  num_epoch:\t3\n",
      "\t  shared_critic:\tFalse\n",
      "\t  learning_rate_schedule:\tlinear\n",
      "\t  beta_schedule:\tlinear\n",
      "\t  epsilon_schedule:\tlinear\n",
      "\tcheckpoint_interval:\t500000\n",
      "\tnetwork_settings:\t\n",
      "\t  normalize:\tTrue\n",
      "\t  hidden_units:\t256\n",
      "\t  num_layers:\t5\n",
      "\t  vis_encode_type:\tsimple\n",
      "\t  memory:\tNone\n",
      "\t  goal_conditioning_type:\thyper\n",
      "\t  deterministic:\tFalse\n",
      "\treward_signals:\t\n",
      "\t  extrinsic:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t1.0\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t128\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\t  gail:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t0.05\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t64\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\t    learning_rate:\t0.0003\n",
      "\t    encoding_size:\tNone\n",
      "\t    use_actions:\tTrue\n",
      "\t    use_vail:\tFalse\n",
      "\t    demo_path:\tdemo/KartAgent.demo\n",
      "\tinit_path:\tNone\n",
      "\tkeep_checkpoints:\t5\n",
      "\teven_checkpoints:\tFalse\n",
      "\tmax_steps:\t500000\n",
      "\ttime_horizon:\t64\n",
      "\tsummary_freq:\t15000\n",
      "\tthreaded:\tFalse\n",
      "\tself_play:\tNone\n",
      "\tbehavioral_cloning:\t\n",
      "\t  demo_path:\tdemo/KartAgent.demo\n",
      "\t  steps:\t0\n",
      "\t  strength:\t0.05\n",
      "\t  samples_per_update:\t512\n",
      "\t  num_epoch:\tNone\n",
      "\t  batch_size:\tNone\n",
      "[INFO] ArcadeDriver. Step: 15000. Time Elapsed: 55.982 s. Mean Reward: 1.065. Std of Reward: 5.542. Training.\n",
      "[INFO] ArcadeDriver. Step: 30000. Time Elapsed: 104.928 s. Mean Reward: 6.072. Std of Reward: 11.613. Training.\n",
      "[INFO] ArcadeDriver. Step: 45000. Time Elapsed: 154.233 s. Mean Reward: 5.855. Std of Reward: 12.148. Training.\n",
      "[INFO] ArcadeDriver. Step: 60000. Time Elapsed: 204.383 s. Mean Reward: 12.574. Std of Reward: 18.765. Training.\n",
      "[INFO] ArcadeDriver. Step: 75000. Time Elapsed: 254.698 s. Mean Reward: 17.111. Std of Reward: 23.003. Training.\n",
      "[INFO] ArcadeDriver. Step: 90000. Time Elapsed: 305.205 s. Mean Reward: 8.794. Std of Reward: 17.646. Training.\n",
      "[INFO] ArcadeDriver. Step: 105000. Time Elapsed: 356.668 s. Mean Reward: 25.809. Std of Reward: 23.456. Training.\n",
      "[INFO] ArcadeDriver. Step: 120000. Time Elapsed: 408.200 s. Mean Reward: 30.500. Std of Reward: 21.805. Training.\n",
      "[INFO] ArcadeDriver. Step: 135000. Time Elapsed: 459.414 s. Mean Reward: 14.178. Std of Reward: 21.609. Training.\n",
      "[INFO] ArcadeDriver. Step: 150000. Time Elapsed: 510.755 s. Mean Reward: 21.275. Std of Reward: 23.806. Training.\n",
      "[INFO] ArcadeDriver. Step: 165000. Time Elapsed: 562.285 s. Mean Reward: 30.116. Std of Reward: 23.744. Training.\n",
      "[INFO] ArcadeDriver. Step: 180000. Time Elapsed: 613.569 s. Mean Reward: 20.545. Std of Reward: 24.506. Training.\n",
      "[INFO] ArcadeDriver. Step: 195000. Time Elapsed: 665.017 s. Mean Reward: 21.591. Std of Reward: 22.739. Training.\n",
      "[INFO] ArcadeDriver. Step: 210000. Time Elapsed: 716.300 s. Mean Reward: 21.554. Std of Reward: 23.824. Training.\n",
      "[INFO] ArcadeDriver. Step: 225000. Time Elapsed: 767.800 s. Mean Reward: 14.895. Std of Reward: 22.110. Training.\n",
      "[INFO] ArcadeDriver. Step: 240000. Time Elapsed: 819.093 s. Mean Reward: 18.944. Std of Reward: 22.087. Training.\n",
      "[INFO] ArcadeDriver. Step: 255000. Time Elapsed: 870.405 s. Mean Reward: 24.706. Std of Reward: 22.825. Training.\n",
      "[INFO] ArcadeDriver. Step: 270000. Time Elapsed: 921.645 s. Mean Reward: 17.612. Std of Reward: 22.558. Training.\n",
      "[INFO] ArcadeDriver. Step: 285000. Time Elapsed: 973.028 s. Mean Reward: 16.819. Std of Reward: 23.032. Training.\n",
      "[INFO] ArcadeDriver. Step: 300000. Time Elapsed: 1024.516 s. Mean Reward: 13.591. Std of Reward: 21.129. Training.\n",
      "[INFO] ArcadeDriver. Step: 315000. Time Elapsed: 1075.791 s. Mean Reward: 14.228. Std of Reward: 21.642. Training.\n",
      "[INFO] ArcadeDriver. Step: 330000. Time Elapsed: 1127.376 s. Mean Reward: 15.303. Std of Reward: 22.772. Training.\n",
      "[INFO] ArcadeDriver. Step: 345000. Time Elapsed: 1178.776 s. Mean Reward: 18.170. Std of Reward: 23.435. Training.\n",
      "[INFO] ArcadeDriver. Step: 360000. Time Elapsed: 1230.514 s. Mean Reward: 15.480. Std of Reward: 21.522. Training.\n",
      "[INFO] ArcadeDriver. Step: 375000. Time Elapsed: 1281.730 s. Mean Reward: 15.451. Std of Reward: 22.100. Training.\n",
      "[INFO] ArcadeDriver. Step: 390000. Time Elapsed: 1333.116 s. Mean Reward: 21.600. Std of Reward: 24.859. Training.\n",
      "[INFO] ArcadeDriver. Step: 405000. Time Elapsed: 1384.407 s. Mean Reward: 15.497. Std of Reward: 22.225. Training.\n",
      "[INFO] ArcadeDriver. Step: 420000. Time Elapsed: 1435.919 s. Mean Reward: 11.696. Std of Reward: 20.771. Training.\n",
      "[INFO] ArcadeDriver. Step: 435000. Time Elapsed: 1487.166 s. Mean Reward: 12.206. Std of Reward: 19.968. Training.\n",
      "[INFO] ArcadeDriver. Step: 450000. Time Elapsed: 1538.726 s. Mean Reward: 13.692. Std of Reward: 22.296. Training.\n",
      "[INFO] ArcadeDriver. Step: 465000. Time Elapsed: 1589.964 s. Mean Reward: 5.605. Std of Reward: 15.086. Training.\n",
      "[INFO] ArcadeDriver. Step: 480000. Time Elapsed: 1641.606 s. Mean Reward: 7.670. Std of Reward: 15.352. Training.\n",
      "[INFO] ArcadeDriver. Step: 495000. Time Elapsed: 1693.201 s. Mean Reward: 3.091. Std of Reward: 9.447. Training.\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_Country_PPO_BC_GAIL/ArcadeDriver/ArcadeDriver-499945.onnx\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_Country_PPO_BC_GAIL/ArcadeDriver/ArcadeDriver-500009.onnx\n",
      "[INFO] Copied /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_Country_PPO_BC_GAIL/ArcadeDriver/ArcadeDriver-500009.onnx to /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_Country_PPO_BC_GAIL/ArcadeDriver.onnx.\n"
     ]
    }
   ],
   "source": [
    "config_ppo_fp = os.path.join(cur_dir, \"config\", \"Kart_ppo_BC_GAIL.yaml\")\n",
    "run_ppo_id = \"Kart_Country_PPO_BC_GAIL\"\n",
    "print(config_ppo_fp)\n",
    "print(run_ppo_id)\n",
    "\n",
    "# !mlagents-learn $config_ppo_fp \\\n",
    "#                --env=$env_fp \\\n",
    "#                --results-dir=$output_dir \\\n",
    "#                --run-id=$run_ppo_id --base-port=$baseport\n",
    "\n",
    "!mlagents-learn $config_ppo_fp \\\n",
    "               --results-dir=$output_dir \\\n",
    "               --run-id=$run_ppo_id --base-port=$baseport --time-scale=1 --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31a4a2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            ┐  ╖\n",
      "        ╓╖╬│╡  ││╬╖╖\n",
      "    ╓╖╬│││││┘  ╬│││││╬╖\n",
      " ╖╬│││││╬╜        ╙╬│││││╖╖                               ╗╗╗\n",
      " ╬╬╬╬╖││╦╖        ╖╬││╗╣╣╣╬      ╟╣╣╬    ╟╣╣╣             ╜╜╜  ╟╣╣\n",
      " ╬╬╬╬╬╬╬╬╖│╬╖╖╓╬╪│╓╣╣╣╣╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╒╣╣╖╗╣╣╣╗   ╣╣╣ ╣╣╣╣╣╣ ╟╣╣╖   ╣╣╣\n",
      " ╬╬╬╬┐  ╙╬╬╬╬│╓╣╣╣╝╜  ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╣╙ ╙╣╣╣  ╣╣╣ ╙╟╣╣╜╙  ╫╣╣  ╟╣╣\n",
      " ╬╬╬╬┐     ╙╬╬╣╣      ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣     ╣╣╣┌╣╣╜\n",
      " ╬╬╬╜       ╬╬╣╣      ╙╝╣╣╬      ╙╣╣╣╗╖╓╗╣╣╣╜ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣╦╓    ╣╣╣╣╣\n",
      " ╙   ╓╦╖    ╬╬╣╣   ╓╗╗╖            ╙╝╣╣╣╣╝╜   ╘╝╝╜   ╝╝╝  ╝╝╝   ╙╣╣╣    ╟╣╣╣\n",
      "   ╩╬╬╬╬╬╬╦╦╬╬╣╣╗╣╣╣╣╣╣╣╝                                             ╫╣╣╣╣\n",
      "      ╙╬╬╬╬╬╬╬╣╣╣╣╣╣╝╜\n",
      "          ╙╬╬╬╣╣╣╜\n",
      "             ╙\n",
      "        \n",
      " Version information:\n",
      "  ml-agents: 1.1.0,\n",
      "  ml-agents-envs: 1.1.0,\n",
      "  Communicator API: 1.5.0,\n",
      "  PyTorch: 2.8.0\n",
      "[INFO] Listening on port 5004. Start training by pressing the Play button in the Unity Editor.\n",
      "[INFO] Connected to Unity environment with package version 4.0.0 and communication version 1.5.0\n",
      "[INFO] Connected new brain: ArcadeDriver?team=0\n",
      "[INFO] Hyperparameters for behavior name ArcadeDriver: \n",
      "\ttrainer_type:\tppo\n",
      "\thyperparameters:\t\n",
      "\t  batch_size:\t128\n",
      "\t  buffer_size:\t1024\n",
      "\t  learning_rate:\t0.0003\n",
      "\t  beta:\t0.01\n",
      "\t  epsilon:\t0.2\n",
      "\t  lambd:\t0.95\n",
      "\t  num_epoch:\t3\n",
      "\t  shared_critic:\tFalse\n",
      "\t  learning_rate_schedule:\tlinear\n",
      "\t  beta_schedule:\tlinear\n",
      "\t  epsilon_schedule:\tlinear\n",
      "\tcheckpoint_interval:\t500000\n",
      "\tnetwork_settings:\t\n",
      "\t  normalize:\tTrue\n",
      "\t  hidden_units:\t256\n",
      "\t  num_layers:\t5\n",
      "\t  vis_encode_type:\tsimple\n",
      "\t  memory:\tNone\n",
      "\t  goal_conditioning_type:\thyper\n",
      "\t  deterministic:\tFalse\n",
      "\treward_signals:\t\n",
      "\t  extrinsic:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t1.0\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t128\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\t  gail:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t0.05\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t64\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\t    learning_rate:\t0.0003\n",
      "\t    encoding_size:\tNone\n",
      "\t    use_actions:\tTrue\n",
      "\t    use_vail:\tFalse\n",
      "\t    demo_path:\tdemo/KartAgent.demo\n",
      "\tinit_path:\tNone\n",
      "\tkeep_checkpoints:\t5\n",
      "\teven_checkpoints:\tFalse\n",
      "\tmax_steps:\t500000\n",
      "\ttime_horizon:\t64\n",
      "\tsummary_freq:\t15000\n",
      "\tthreaded:\tFalse\n",
      "\tself_play:\tNone\n",
      "\tbehavioral_cloning:\t\n",
      "\t  demo_path:\tdemo/KartAgent.demo\n",
      "\t  steps:\t0\n",
      "\t  strength:\t0.05\n",
      "\t  samples_per_update:\t512\n",
      "\t  num_epoch:\tNone\n",
      "\t  batch_size:\tNone\n",
      "[INFO] Resuming from /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_Country_PPO_BC_GAIL/ArcadeDriver.\n",
      "[INFO] Resuming training from step 500009.\n",
      "[INFO] ArcadeDriver. Step: 510000. Time Elapsed: 38.062 s. Mean Reward: 1.328. Std of Reward: 6.812. Not Training.\n",
      "[INFO] ArcadeDriver. Step: 525000. Time Elapsed: 82.927 s. Mean Reward: 1.022. Std of Reward: 6.264. Not Training.\n",
      "[INFO] ArcadeDriver. Step: 540000. Time Elapsed: 128.358 s. Mean Reward: 1.413. Std of Reward: 6.169. Not Training.\n",
      "[INFO] ArcadeDriver. Step: 555000. Time Elapsed: 175.038 s. Mean Reward: 0.793. Std of Reward: 5.992. Not Training.\n",
      "[INFO] ArcadeDriver. Step: 570000. Time Elapsed: 221.621 s. Mean Reward: 0.514. Std of Reward: 5.899. Not Training.\n",
      "[INFO] ArcadeDriver. Step: 585000. Time Elapsed: 267.454 s. Mean Reward: 0.778. Std of Reward: 5.938. Not Training.\n",
      "[INFO] ArcadeDriver. Step: 600000. Time Elapsed: 313.651 s. Mean Reward: 1.447. Std of Reward: 6.199. Not Training.\n",
      "[WARNING] Restarting worker[0] after 'Communicator has exited.'\n",
      "[INFO] Listening on port 5004. Start training by pressing the Play button in the Unity Editor.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/bin/mlagents-learn\", line 7, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/site-packages/mlagents/trainers/learn.py\", line 270, in main\n",
      "    run_cli(parse_command_line())\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/site-packages/mlagents/trainers/learn.py\", line 266, in run_cli\n",
      "    run_training(run_seed, options, num_areas)\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/site-packages/mlagents/trainers/learn.py\", line 138, in run_training\n",
      "    tc.start_learning(env_manager)\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/site-packages/mlagents_envs/timers.py\", line 305, in wrapped\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/site-packages/mlagents/trainers/trainer_controller.py\", line 175, in start_learning\n",
      "    n_steps = self.advance(env_manager)\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/site-packages/mlagents_envs/timers.py\", line 305, in wrapped\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/site-packages/mlagents/trainers/trainer_controller.py\", line 233, in advance\n",
      "    new_step_infos = env_manager.get_steps()\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/site-packages/mlagents/trainers/env_manager.py\", line 124, in get_steps\n",
      "    new_step_infos = self._step()\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/site-packages/mlagents/trainers/subprocess_env_manager.py\", line 420, in _step\n",
      "    self._restart_failed_workers(step)\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/site-packages/mlagents/trainers/subprocess_env_manager.py\", line 328, in _restart_failed_workers\n",
      "    self.reset(self.env_parameters)\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/site-packages/mlagents/trainers/env_manager.py\", line 68, in reset\n",
      "    self.first_step_infos = self._reset_env(config)\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/site-packages/mlagents/trainers/subprocess_env_manager.py\", line 446, in _reset_env\n",
      "    ew.previous_step = EnvironmentStep(ew.recv().payload, ew.worker_id, {}, {})\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/site-packages/mlagents/trainers/subprocess_env_manager.py\", line 101, in recv\n",
      "    raise env_exception\n",
      "mlagents_envs.exception.UnityTimeOutException: The Unity environment took too long to respond. Make sure that :\n",
      "\t The environment does not need user interaction to launch\n",
      "\t The Agents' Behavior Parameters > Behavior Type is set to \"Default\"\n",
      "\t The environment and the Python interface have compatible versions.\n",
      "\t If you're running on a headless server without graphics support, turn off display by either passing --no-graphics option or build your Unity executable as server build.\n"
     ]
    }
   ],
   "source": [
    "!mlagents-learn $config_ppo_fp \\\n",
    "               --results-dir=$output_dir \\\n",
    "               --run-id=$run_ppo_id --base-port=$baseport --resume --inference --time-scale=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a47b032",
   "metadata": {},
   "source": [
    "## Training SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6886efe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/config/Kart_sac_BC_GAIL.yaml\n",
      "Kart_Country_SAC_BC_GAIL\n",
      "\n",
      "            ┐  ╖\n",
      "        ╓╖╬│╡  ││╬╖╖\n",
      "    ╓╖╬│││││┘  ╬│││││╬╖\n",
      " ╖╬│││││╬╜        ╙╬│││││╖╖                               ╗╗╗\n",
      " ╬╬╬╬╖││╦╖        ╖╬││╗╣╣╣╬      ╟╣╣╬    ╟╣╣╣             ╜╜╜  ╟╣╣\n",
      " ╬╬╬╬╬╬╬╬╖│╬╖╖╓╬╪│╓╣╣╣╣╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╒╣╣╖╗╣╣╣╗   ╣╣╣ ╣╣╣╣╣╣ ╟╣╣╖   ╣╣╣\n",
      " ╬╬╬╬┐  ╙╬╬╬╬│╓╣╣╣╝╜  ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╣╙ ╙╣╣╣  ╣╣╣ ╙╟╣╣╜╙  ╫╣╣  ╟╣╣\n",
      " ╬╬╬╬┐     ╙╬╬╣╣      ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣     ╣╣╣┌╣╣╜\n",
      " ╬╬╬╜       ╬╬╣╣      ╙╝╣╣╬      ╙╣╣╣╗╖╓╗╣╣╣╜ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣╦╓    ╣╣╣╣╣\n",
      " ╙   ╓╦╖    ╬╬╣╣   ╓╗╗╖            ╙╝╣╣╣╣╝╜   ╘╝╝╜   ╝╝╝  ╝╝╝   ╙╣╣╣    ╟╣╣╣\n",
      "   ╩╬╬╬╬╬╬╦╦╬╬╣╣╗╣╣╣╣╣╣╣╝                                             ╫╣╣╣╣\n",
      "      ╙╬╬╬╬╬╬╬╣╣╣╣╣╣╝╜\n",
      "          ╙╬╬╬╣╣╣╜\n",
      "             ╙\n",
      "        \n",
      " Version information:\n",
      "  ml-agents: 1.1.0,\n",
      "  ml-agents-envs: 1.1.0,\n",
      "  Communicator API: 1.5.0,\n",
      "  PyTorch: 2.8.0\n",
      "[INFO] Listening on port 5004. Start training by pressing the Play button in the Unity Editor.\n",
      "[INFO] Connected to Unity environment with package version 4.0.0 and communication version 1.5.0\n",
      "[INFO] Connected new brain: ArcadeDriver?team=0\n",
      "[INFO] Hyperparameters for behavior name ArcadeDriver: \n",
      "\ttrainer_type:\tsac\n",
      "\thyperparameters:\t\n",
      "\t  learning_rate:\t0.0003\n",
      "\t  learning_rate_schedule:\tlinear\n",
      "\t  batch_size:\t128\n",
      "\t  buffer_size:\t1024\n",
      "\t  buffer_init_steps:\t0\n",
      "\t  tau:\t0.005\n",
      "\t  steps_per_update:\t10.0\n",
      "\t  save_replay_buffer:\tFalse\n",
      "\t  init_entcoef:\t0.5\n",
      "\t  reward_signal_steps_per_update:\t10.0\n",
      "\tcheckpoint_interval:\t500000\n",
      "\tnetwork_settings:\t\n",
      "\t  normalize:\tTrue\n",
      "\t  hidden_units:\t256\n",
      "\t  num_layers:\t5\n",
      "\t  vis_encode_type:\tsimple\n",
      "\t  memory:\tNone\n",
      "\t  goal_conditioning_type:\thyper\n",
      "\t  deterministic:\tFalse\n",
      "\treward_signals:\t\n",
      "\t  extrinsic:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t1.0\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t128\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\t  gail:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t0.05\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t64\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\t    learning_rate:\t0.0003\n",
      "\t    encoding_size:\tNone\n",
      "\t    use_actions:\tTrue\n",
      "\t    use_vail:\tFalse\n",
      "\t    demo_path:\tdemo/KartAgent.demo\n",
      "\tinit_path:\tNone\n",
      "\tkeep_checkpoints:\t5\n",
      "\teven_checkpoints:\tFalse\n",
      "\tmax_steps:\t500000\n",
      "\ttime_horizon:\t64\n",
      "\tsummary_freq:\t15000\n",
      "\tthreaded:\tFalse\n",
      "\tself_play:\tNone\n",
      "\tbehavioral_cloning:\t\n",
      "\t  demo_path:\tdemo/KartAgent.demo\n",
      "\t  steps:\t0\n",
      "\t  strength:\t0.05\n",
      "\t  samples_per_update:\t512\n",
      "\t  num_epoch:\tNone\n",
      "\t  batch_size:\tNone\n",
      "[INFO] ArcadeDriver. Step: 15000. Time Elapsed: 168.507 s. Mean Reward: -0.676. Std of Reward: 3.482. Training.\n",
      "[INFO] ArcadeDriver. Step: 30000. Time Elapsed: 329.077 s. Mean Reward: -0.708. Std of Reward: 3.862. Training.\n",
      "[INFO] ArcadeDriver. Step: 45000. Time Elapsed: 490.932 s. Mean Reward: -0.029. Std of Reward: 5.280. Training.\n",
      "[INFO] ArcadeDriver. Step: 60000. Time Elapsed: 652.386 s. Mean Reward: 1.959. Std of Reward: 8.314. Training.\n",
      "[INFO] ArcadeDriver. Step: 75000. Time Elapsed: 813.858 s. Mean Reward: 2.638. Std of Reward: 10.132. Training.\n",
      "[INFO] ArcadeDriver. Step: 90000. Time Elapsed: 975.653 s. Mean Reward: 4.470. Std of Reward: 12.431. Training.\n",
      "[INFO] ArcadeDriver. Step: 105000. Time Elapsed: 1137.675 s. Mean Reward: 3.115. Std of Reward: 10.264. Training.\n",
      "[INFO] ArcadeDriver. Step: 120000. Time Elapsed: 1299.088 s. Mean Reward: 6.136. Std of Reward: 14.074. Training.\n",
      "[INFO] ArcadeDriver. Step: 135000. Time Elapsed: 1460.757 s. Mean Reward: 5.849. Std of Reward: 14.638. Training.\n",
      "[INFO] ArcadeDriver. Step: 150000. Time Elapsed: 1622.224 s. Mean Reward: 6.328. Std of Reward: 15.780. Training.\n",
      "[INFO] ArcadeDriver. Step: 165000. Time Elapsed: 1783.766 s. Mean Reward: 5.172. Std of Reward: 14.095. Training.\n",
      "[INFO] ArcadeDriver. Step: 180000. Time Elapsed: 1945.286 s. Mean Reward: 6.022. Std of Reward: 14.800. Training.\n",
      "[INFO] ArcadeDriver. Step: 195000. Time Elapsed: 2106.540 s. Mean Reward: 5.796. Std of Reward: 13.045. Training.\n",
      "[INFO] ArcadeDriver. Step: 210000. Time Elapsed: 2267.915 s. Mean Reward: 4.990. Std of Reward: 13.610. Training.\n",
      "[INFO] ArcadeDriver. Step: 225000. Time Elapsed: 2429.202 s. Mean Reward: 7.077. Std of Reward: 15.654. Training.\n",
      "[INFO] ArcadeDriver. Step: 240000. Time Elapsed: 2590.537 s. Mean Reward: 9.366. Std of Reward: 18.844. Training.\n",
      "[INFO] ArcadeDriver. Step: 255000. Time Elapsed: 2752.202 s. Mean Reward: 4.205. Std of Reward: 12.351. Training.\n",
      "[INFO] ArcadeDriver. Step: 270000. Time Elapsed: 2913.621 s. Mean Reward: 5.383. Std of Reward: 14.249. Training.\n",
      "[INFO] ArcadeDriver. Step: 285000. Time Elapsed: 3075.183 s. Mean Reward: 12.995. Std of Reward: 20.818. Training.\n",
      "[INFO] ArcadeDriver. Step: 300000. Time Elapsed: 3236.168 s. Mean Reward: 9.103. Std of Reward: 18.080. Training.\n",
      "[INFO] ArcadeDriver. Step: 315000. Time Elapsed: 3397.373 s. Mean Reward: 6.750. Std of Reward: 15.643. Training.\n",
      "[INFO] ArcadeDriver. Step: 330000. Time Elapsed: 3559.374 s. Mean Reward: 6.985. Std of Reward: 14.185. Training.\n",
      "[INFO] ArcadeDriver. Step: 345000. Time Elapsed: 3716.649 s. Mean Reward: 8.920. Std of Reward: 16.078. Training.\n",
      "[INFO] ArcadeDriver. Step: 360000. Time Elapsed: 3869.787 s. Mean Reward: 9.455. Std of Reward: 17.810. Training.\n",
      "[INFO] ArcadeDriver. Step: 375000. Time Elapsed: 4023.051 s. Mean Reward: 4.451. Std of Reward: 12.203. Training.\n",
      "[INFO] ArcadeDriver. Step: 390000. Time Elapsed: 4175.760 s. Mean Reward: 3.727. Std of Reward: 12.939. Training.\n",
      "[INFO] ArcadeDriver. Step: 405000. Time Elapsed: 4328.127 s. Mean Reward: 6.779. Std of Reward: 15.632. Training.\n",
      "[INFO] ArcadeDriver. Step: 420000. Time Elapsed: 4481.561 s. Mean Reward: 6.090. Std of Reward: 13.863. Training.\n",
      "[INFO] ArcadeDriver. Step: 435000. Time Elapsed: 4633.979 s. Mean Reward: 5.502. Std of Reward: 16.135. Training.\n",
      "[INFO] ArcadeDriver. Step: 450000. Time Elapsed: 4786.658 s. Mean Reward: 7.705. Std of Reward: 16.180. Training.\n",
      "[INFO] ArcadeDriver. Step: 465000. Time Elapsed: 4939.399 s. Mean Reward: 7.988. Std of Reward: 16.071. Training.\n",
      "[INFO] ArcadeDriver. Step: 480000. Time Elapsed: 5091.954 s. Mean Reward: 6.544. Std of Reward: 14.374. Training.\n",
      "[INFO] ArcadeDriver. Step: 495000. Time Elapsed: 5244.823 s. Mean Reward: 8.763. Std of Reward: 16.467. Training.\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_Country_SAC_BC_GAIL/ArcadeDriver/ArcadeDriver-499946.onnx\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_Country_SAC_BC_GAIL/ArcadeDriver/ArcadeDriver-500010.onnx\n",
      "[INFO] Copied /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_Country_SAC_BC_GAIL/ArcadeDriver/ArcadeDriver-500010.onnx to /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_Country_SAC_BC_GAIL/ArcadeDriver.onnx.\n"
     ]
    }
   ],
   "source": [
    "config_sac_fp = os.path.join(cur_dir, \"config\", \"Kart_sac_BC_GAIL.yaml\")\n",
    "run_sac_id = \"Kart_Country_SAC_BC_GAIL\"\n",
    "print(config_sac_fp)\n",
    "print(run_sac_id)\n",
    "\n",
    "# !mlagents-learn $config_sac_fp \\\n",
    "#                --env=$env_fp \\\n",
    "#                --results-dir=$output_dir \\\n",
    "#                --run-id=$run_sac_id --base-port=$baseport\n",
    "\n",
    "!mlagents-learn $config_sac_fp \\\n",
    "               --results-dir=$output_dir \\\n",
    "               --run-id=$run_sac_id --base-port=$baseport --time-scale=1 --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f38a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mlagents-learn $config_ppo_fp \\\n",
    "#                --results-dir=$output_dir \\\n",
    "#                --run-id=$run_sac_id --base-port=$baseport --resume --inference --time-scale=1\n",
    "\n",
    "\n",
    "!mlagents-learn $config_sac_fp \\\n",
    "               --results-dir=$output_dir \\\n",
    "               --run-id=$run_sac_id --base-port=$baseport --resume --inference --time-scale=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40c13bd",
   "metadata": {},
   "source": [
    "## Training POCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "694c6520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/config/Kart_poca_BC_GAIL.yaml\n",
      "Kart_Country_POCA_BC_GAIL\n",
      "\n",
      "            ┐  ╖\n",
      "        ╓╖╬│╡  ││╬╖╖\n",
      "    ╓╖╬│││││┘  ╬│││││╬╖\n",
      " ╖╬│││││╬╜        ╙╬│││││╖╖                               ╗╗╗\n",
      " ╬╬╬╬╖││╦╖        ╖╬││╗╣╣╣╬      ╟╣╣╬    ╟╣╣╣             ╜╜╜  ╟╣╣\n",
      " ╬╬╬╬╬╬╬╬╖│╬╖╖╓╬╪│╓╣╣╣╣╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╒╣╣╖╗╣╣╣╗   ╣╣╣ ╣╣╣╣╣╣ ╟╣╣╖   ╣╣╣\n",
      " ╬╬╬╬┐  ╙╬╬╬╬│╓╣╣╣╝╜  ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╣╙ ╙╣╣╣  ╣╣╣ ╙╟╣╣╜╙  ╫╣╣  ╟╣╣\n",
      " ╬╬╬╬┐     ╙╬╬╣╣      ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣     ╣╣╣┌╣╣╜\n",
      " ╬╬╬╜       ╬╬╣╣      ╙╝╣╣╬      ╙╣╣╣╗╖╓╗╣╣╣╜ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣╦╓    ╣╣╣╣╣\n",
      " ╙   ╓╦╖    ╬╬╣╣   ╓╗╗╖            ╙╝╣╣╣╣╝╜   ╘╝╝╜   ╝╝╝  ╝╝╝   ╙╣╣╣    ╟╣╣╣\n",
      "   ╩╬╬╬╬╬╬╦╦╬╬╣╣╗╣╣╣╣╣╣╣╝                                             ╫╣╣╣╣\n",
      "      ╙╬╬╬╬╬╬╬╣╣╣╣╣╣╝╜\n",
      "          ╙╬╬╬╣╣╣╜\n",
      "             ╙\n",
      "        \n",
      " Version information:\n",
      "  ml-agents: 1.1.0,\n",
      "  ml-agents-envs: 1.1.0,\n",
      "  Communicator API: 1.5.0,\n",
      "  PyTorch: 2.8.0\n",
      "[INFO] Listening on port 5004. Start training by pressing the Play button in the Unity Editor.\n",
      "[INFO] Connected to Unity environment with package version 4.0.0 and communication version 1.5.0\n",
      "[INFO] Connected new brain: ArcadeDriver?team=0\n",
      "[INFO] Hyperparameters for behavior name ArcadeDriver: \n",
      "\ttrainer_type:\tpoca\n",
      "\thyperparameters:\t\n",
      "\t  batch_size:\t128\n",
      "\t  buffer_size:\t1024\n",
      "\t  learning_rate:\t0.0003\n",
      "\t  beta:\t0.01\n",
      "\t  epsilon:\t0.2\n",
      "\t  lambd:\t0.95\n",
      "\t  num_epoch:\t3\n",
      "\t  learning_rate_schedule:\tlinear\n",
      "\t  beta_schedule:\tlinear\n",
      "\t  epsilon_schedule:\tlinear\n",
      "\tcheckpoint_interval:\t500000\n",
      "\tnetwork_settings:\t\n",
      "\t  normalize:\tTrue\n",
      "\t  hidden_units:\t256\n",
      "\t  num_layers:\t5\n",
      "\t  vis_encode_type:\tsimple\n",
      "\t  memory:\tNone\n",
      "\t  goal_conditioning_type:\thyper\n",
      "\t  deterministic:\tFalse\n",
      "\treward_signals:\t\n",
      "\t  extrinsic:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t1.0\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t128\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\t  gail:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t0.05\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t64\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\t    learning_rate:\t0.0003\n",
      "\t    encoding_size:\tNone\n",
      "\t    use_actions:\tTrue\n",
      "\t    use_vail:\tFalse\n",
      "\t    demo_path:\tdemo/KartAgent.demo\n",
      "\tinit_path:\tNone\n",
      "\tkeep_checkpoints:\t5\n",
      "\teven_checkpoints:\tFalse\n",
      "\tmax_steps:\t500000\n",
      "\ttime_horizon:\t64\n",
      "\tsummary_freq:\t15000\n",
      "\tthreaded:\tFalse\n",
      "\tself_play:\tNone\n",
      "\tbehavioral_cloning:\t\n",
      "\t  demo_path:\tdemo/KartAgent.demo\n",
      "\t  steps:\t0\n",
      "\t  strength:\t0.05\n",
      "\t  samples_per_update:\t512\n",
      "\t  num_epoch:\tNone\n",
      "\t  batch_size:\tNone\n",
      "[WARNING] Reward signal Gail is not supported with the POCA trainer; results may be unexpected.\n",
      "[INFO] ArcadeDriver. Step: 15000. Time Elapsed: 59.461 s. Mean Reward: 1.812. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 30000. Time Elapsed: 111.529 s. Mean Reward: 5.895. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 45000. Time Elapsed: 163.697 s. Mean Reward: 4.978. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 60000. Time Elapsed: 217.146 s. Mean Reward: 5.579. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 75000. Time Elapsed: 270.801 s. Mean Reward: 4.240. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 90000. Time Elapsed: 323.848 s. Mean Reward: 7.706. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 105000. Time Elapsed: 376.982 s. Mean Reward: 10.487. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 120000. Time Elapsed: 430.332 s. Mean Reward: 4.823. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 135000. Time Elapsed: 483.689 s. Mean Reward: 9.693. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 150000. Time Elapsed: 536.896 s. Mean Reward: 7.196. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 165000. Time Elapsed: 590.415 s. Mean Reward: 18.276. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 180000. Time Elapsed: 644.568 s. Mean Reward: 16.906. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 195000. Time Elapsed: 698.319 s. Mean Reward: 17.634. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 210000. Time Elapsed: 752.280 s. Mean Reward: 21.538. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 225000. Time Elapsed: 806.791 s. Mean Reward: 23.175. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 240000. Time Elapsed: 861.006 s. Mean Reward: 12.176. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 255000. Time Elapsed: 916.037 s. Mean Reward: 17.906. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 270000. Time Elapsed: 970.417 s. Mean Reward: 8.105. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 285000. Time Elapsed: 1025.293 s. Mean Reward: 4.148. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 300000. Time Elapsed: 1079.678 s. Mean Reward: 19.460. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 315000. Time Elapsed: 1134.241 s. Mean Reward: 25.746. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 330000. Time Elapsed: 1189.190 s. Mean Reward: 12.183. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 345000. Time Elapsed: 1243.878 s. Mean Reward: 19.288. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 360000. Time Elapsed: 1298.388 s. Mean Reward: 19.500. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 375000. Time Elapsed: 1352.925 s. Mean Reward: 14.872. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 390000. Time Elapsed: 1407.989 s. Mean Reward: 31.467. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 405000. Time Elapsed: 1462.621 s. Mean Reward: 31.123. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 420000. Time Elapsed: 1517.209 s. Mean Reward: 3.760. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 435000. Time Elapsed: 1572.362 s. Mean Reward: 2.790. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 450000. Time Elapsed: 1627.195 s. Mean Reward: 2.608. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 465000. Time Elapsed: 1681.996 s. Mean Reward: 2.664. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 480000. Time Elapsed: 1736.769 s. Mean Reward: 3.979. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 495000. Time Elapsed: 1791.488 s. Mean Reward: 3.452. Mean Group Reward: 0.000. Training.\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_Country_POCA_BC_GAIL/ArcadeDriver/ArcadeDriver-499990.onnx\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_Country_POCA_BC_GAIL/ArcadeDriver/ArcadeDriver-500054.onnx\n",
      "[INFO] Copied /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_Country_POCA_BC_GAIL/ArcadeDriver/ArcadeDriver-500054.onnx to /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_Country_POCA_BC_GAIL/ArcadeDriver.onnx.\n"
     ]
    }
   ],
   "source": [
    "config_poca_fp = os.path.join(cur_dir, \"config\", \"Kart_poca_BC_GAIL.yaml\")\n",
    "run_poca_id = \"Kart_Country_POCA_BC_GAIL\"\n",
    "print(config_poca_fp)\n",
    "print(run_poca_id)\n",
    "\n",
    "# !mlagents-learn $config_poca_fp \\\n",
    "#                --env=$env_fp \\\n",
    "#                --results-dir=$output_dir \\\n",
    "#                --run-id=$run_poca_id --base-port=$baseport\n",
    "\n",
    "!mlagents-learn $config_poca_fp \\\n",
    "               --results-dir=$output_dir \\\n",
    "               --run-id=$run_poca_id --base-port=$baseport  --time-scale=1 --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e109ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlagents-learn $config_poca_fp \\\n",
    "               --results-dir=$output_dir \\\n",
    "               --run-id=$run_poca_id --base-port=$baseport --resume --inference --time-scale=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad33878",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5c2e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mlagents-learn $config_fp \\\n",
    "#                --env=$env_fp \\\n",
    "#                --results-dir=$test_dir \\\n",
    "#                --run-id=$run_id \\\n",
    "#                --resume --inference\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
