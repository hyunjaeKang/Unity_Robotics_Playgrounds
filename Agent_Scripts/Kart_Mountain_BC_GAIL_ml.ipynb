{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70a95c2c",
   "metadata": {},
   "source": [
    "# Kart_BC_GAIL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8c6c95",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8ba6fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "\n",
    "# Global Setting\n",
    "cur_dir = os.getcwd()\n",
    "env_dir = os.path.abspath(os.path.join(cur_dir, \"..\", \"Unity6000_Envs\"))\n",
    "output_dir = os.path.abspath(os.path.join(cur_dir, \"temp\", \"mlagents_learn_output\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a62e191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Unity6000_Envs/Kart_Darwin.app\n"
     ]
    }
   ],
   "source": [
    "# Unity Enviroment\n",
    "game = \"Kart\"\n",
    "os_name = platform.system()\n",
    "\n",
    "if os_name == 'Linux':\n",
    "    env_name = os.path.join(env_dir, f\"{game}_{os_name}.x86_64\")\n",
    "elif os_name == 'Darwin':\n",
    "    env_name = os.path.join(env_dir, f\"{game}_{os_name}.app\")\n",
    "env_fp = os.path.join(env_dir, env_name)\n",
    "print(env_fp)\n",
    "baseport = 1091"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9e227b",
   "metadata": {},
   "source": [
    "## Training PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fb8ef05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/config/Kart_ppo_BC_GAIL.yaml\n",
      "Kart_Mountain_PPO_BC_GAIL\n",
      "\n",
      "            ┐  ╖\n",
      "        ╓╖╬│╡  ││╬╖╖\n",
      "    ╓╖╬│││││┘  ╬│││││╬╖\n",
      " ╖╬│││││╬╜        ╙╬│││││╖╖                               ╗╗╗\n",
      " ╬╬╬╬╖││╦╖        ╖╬││╗╣╣╣╬      ╟╣╣╬    ╟╣╣╣             ╜╜╜  ╟╣╣\n",
      " ╬╬╬╬╬╬╬╬╖│╬╖╖╓╬╪│╓╣╣╣╣╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╒╣╣╖╗╣╣╣╗   ╣╣╣ ╣╣╣╣╣╣ ╟╣╣╖   ╣╣╣\n",
      " ╬╬╬╬┐  ╙╬╬╬╬│╓╣╣╣╝╜  ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╣╙ ╙╣╣╣  ╣╣╣ ╙╟╣╣╜╙  ╫╣╣  ╟╣╣\n",
      " ╬╬╬╬┐     ╙╬╬╣╣      ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣     ╣╣╣┌╣╣╜\n",
      " ╬╬╬╜       ╬╬╣╣      ╙╝╣╣╬      ╙╣╣╣╗╖╓╗╣╣╣╜ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣╦╓    ╣╣╣╣╣\n",
      " ╙   ╓╦╖    ╬╬╣╣   ╓╗╗╖            ╙╝╣╣╣╣╝╜   ╘╝╝╜   ╝╝╝  ╝╝╝   ╙╣╣╣    ╟╣╣╣\n",
      "   ╩╬╬╬╬╬╬╦╦╬╬╣╣╗╣╣╣╣╣╣╣╝                                             ╫╣╣╣╣\n",
      "      ╙╬╬╬╬╬╬╬╣╣╣╣╣╣╝╜\n",
      "          ╙╬╬╬╣╣╣╜\n",
      "             ╙\n",
      "        \n",
      " Version information:\n",
      "  ml-agents: 1.1.0,\n",
      "  ml-agents-envs: 1.1.0,\n",
      "  Communicator API: 1.5.0,\n",
      "  PyTorch: 2.8.0\n",
      "[INFO] Listening on port 5004. Start training by pressing the Play button in the Unity Editor.\n",
      "[INFO] Connected to Unity environment with package version 4.0.0 and communication version 1.5.0\n",
      "[INFO] Connected new brain: ArcadeDriver?team=0\n",
      "[INFO] Hyperparameters for behavior name ArcadeDriver: \n",
      "\ttrainer_type:\tppo\n",
      "\thyperparameters:\t\n",
      "\t  batch_size:\t128\n",
      "\t  buffer_size:\t1024\n",
      "\t  learning_rate:\t0.0003\n",
      "\t  beta:\t0.01\n",
      "\t  epsilon:\t0.2\n",
      "\t  lambd:\t0.95\n",
      "\t  num_epoch:\t3\n",
      "\t  shared_critic:\tFalse\n",
      "\t  learning_rate_schedule:\tlinear\n",
      "\t  beta_schedule:\tlinear\n",
      "\t  epsilon_schedule:\tlinear\n",
      "\tcheckpoint_interval:\t500000\n",
      "\tnetwork_settings:\t\n",
      "\t  normalize:\tTrue\n",
      "\t  hidden_units:\t256\n",
      "\t  num_layers:\t5\n",
      "\t  vis_encode_type:\tsimple\n",
      "\t  memory:\tNone\n",
      "\t  goal_conditioning_type:\thyper\n",
      "\t  deterministic:\tFalse\n",
      "\treward_signals:\t\n",
      "\t  extrinsic:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t1.0\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t128\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\t  gail:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t0.05\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t64\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\t    learning_rate:\t0.0003\n",
      "\t    encoding_size:\tNone\n",
      "\t    use_actions:\tTrue\n",
      "\t    use_vail:\tFalse\n",
      "\t    demo_path:\tdemo/KartAgent.demo\n",
      "\tinit_path:\tNone\n",
      "\tkeep_checkpoints:\t5\n",
      "\teven_checkpoints:\tFalse\n",
      "\tmax_steps:\t500000\n",
      "\ttime_horizon:\t64\n",
      "\tsummary_freq:\t15000\n",
      "\tthreaded:\tFalse\n",
      "\tself_play:\tNone\n",
      "\tbehavioral_cloning:\t\n",
      "\t  demo_path:\tdemo/KartAgent.demo\n",
      "\t  steps:\t0\n",
      "\t  strength:\t0.05\n",
      "\t  samples_per_update:\t512\n",
      "\t  num_epoch:\tNone\n",
      "\t  batch_size:\tNone\n",
      "[INFO] ArcadeDriver. Step: 15000. Time Elapsed: 56.169 s. Mean Reward: 1.047. Std of Reward: 5.044. Training.\n",
      "[INFO] ArcadeDriver. Step: 30000. Time Elapsed: 105.421 s. Mean Reward: 2.934. Std of Reward: 6.321. Training.\n",
      "[INFO] ArcadeDriver. Step: 45000. Time Elapsed: 155.396 s. Mean Reward: 3.863. Std of Reward: 9.059. Training.\n",
      "[INFO] ArcadeDriver. Step: 60000. Time Elapsed: 205.959 s. Mean Reward: 4.809. Std of Reward: 9.161. Training.\n",
      "[INFO] ArcadeDriver. Step: 75000. Time Elapsed: 256.580 s. Mean Reward: 5.564. Std of Reward: 10.341. Training.\n",
      "[INFO] ArcadeDriver. Step: 90000. Time Elapsed: 306.120 s. Mean Reward: 5.330. Std of Reward: 10.374. Training.\n",
      "[INFO] ArcadeDriver. Step: 105000. Time Elapsed: 356.875 s. Mean Reward: 6.739. Std of Reward: 11.492. Training.\n",
      "[INFO] ArcadeDriver. Step: 120000. Time Elapsed: 408.154 s. Mean Reward: 4.500. Std of Reward: 10.060. Training.\n",
      "[INFO] ArcadeDriver. Step: 135000. Time Elapsed: 459.743 s. Mean Reward: 1.985. Std of Reward: 6.242. Training.\n",
      "[INFO] ArcadeDriver. Step: 150000. Time Elapsed: 511.434 s. Mean Reward: 4.209. Std of Reward: 9.662. Training.\n",
      "[INFO] ArcadeDriver. Step: 165000. Time Elapsed: 563.076 s. Mean Reward: 4.638. Std of Reward: 10.066. Training.\n",
      "[INFO] ArcadeDriver. Step: 180000. Time Elapsed: 614.459 s. Mean Reward: 5.407. Std of Reward: 10.876. Training.\n",
      "[INFO] ArcadeDriver. Step: 195000. Time Elapsed: 666.369 s. Mean Reward: 5.601. Std of Reward: 10.776. Training.\n",
      "[INFO] ArcadeDriver. Step: 210000. Time Elapsed: 717.972 s. Mean Reward: 5.870. Std of Reward: 10.994. Training.\n",
      "[INFO] ArcadeDriver. Step: 225000. Time Elapsed: 769.384 s. Mean Reward: 7.232. Std of Reward: 11.713. Training.\n",
      "[INFO] ArcadeDriver. Step: 240000. Time Elapsed: 820.920 s. Mean Reward: 5.936. Std of Reward: 10.999. Training.\n",
      "[INFO] ArcadeDriver. Step: 255000. Time Elapsed: 872.755 s. Mean Reward: 5.816. Std of Reward: 10.937. Training.\n",
      "[INFO] ArcadeDriver. Step: 270000. Time Elapsed: 924.589 s. Mean Reward: 8.316. Std of Reward: 12.178. Training.\n",
      "[INFO] ArcadeDriver. Step: 285000. Time Elapsed: 976.080 s. Mean Reward: 7.342. Std of Reward: 11.709. Training.\n",
      "[INFO] ArcadeDriver. Step: 300000. Time Elapsed: 1028.003 s. Mean Reward: 6.488. Std of Reward: 11.333. Training.\n",
      "[INFO] ArcadeDriver. Step: 315000. Time Elapsed: 1079.613 s. Mean Reward: 9.411. Std of Reward: 12.368. Training.\n",
      "[INFO] ArcadeDriver. Step: 330000. Time Elapsed: 1131.518 s. Mean Reward: 4.589. Std of Reward: 9.909. Training.\n",
      "[INFO] ArcadeDriver. Step: 345000. Time Elapsed: 1182.995 s. Mean Reward: 9.347. Std of Reward: 12.354. Training.\n",
      "[INFO] ArcadeDriver. Step: 360000. Time Elapsed: 1234.939 s. Mean Reward: 2.948. Std of Reward: 8.442. Training.\n",
      "[INFO] ArcadeDriver. Step: 375000. Time Elapsed: 1286.479 s. Mean Reward: 2.285. Std of Reward: 7.589. Training.\n",
      "[INFO] ArcadeDriver. Step: 390000. Time Elapsed: 1337.743 s. Mean Reward: -1.321. Std of Reward: 22.972. Training.\n",
      "[INFO] ArcadeDriver. Step: 405000. Time Elapsed: 1389.317 s. Mean Reward: 4.308. Std of Reward: 9.851. Training.\n",
      "[INFO] ArcadeDriver. Step: 420000. Time Elapsed: 1441.307 s. Mean Reward: 3.632. Std of Reward: 9.799. Training.\n",
      "[INFO] ArcadeDriver. Step: 435000. Time Elapsed: 1492.926 s. Mean Reward: 4.814. Std of Reward: 10.688. Training.\n",
      "[INFO] ArcadeDriver. Step: 450000. Time Elapsed: 1544.380 s. Mean Reward: 4.718. Std of Reward: 10.411. Training.\n",
      "[INFO] ArcadeDriver. Step: 465000. Time Elapsed: 1596.249 s. Mean Reward: 5.204. Std of Reward: 11.275. Training.\n",
      "[INFO] ArcadeDriver. Step: 480000. Time Elapsed: 1647.701 s. Mean Reward: 4.575. Std of Reward: 10.927. Training.\n",
      "[INFO] ArcadeDriver. Step: 495000. Time Elapsed: 1699.832 s. Mean Reward: 4.488. Std of Reward: 10.661. Training.\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_Mountain_PPO_BC_GAIL/ArcadeDriver/ArcadeDriver-499987.onnx\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_Mountain_PPO_BC_GAIL/ArcadeDriver/ArcadeDriver-500051.onnx\n",
      "[INFO] Copied /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_Mountain_PPO_BC_GAIL/ArcadeDriver/ArcadeDriver-500051.onnx to /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_Mountain_PPO_BC_GAIL/ArcadeDriver.onnx.\n"
     ]
    }
   ],
   "source": [
    "config_ppo_fp = os.path.join(cur_dir, \"config\", \"Kart_ppo_BC_GAIL.yaml\")\n",
    "run_ppo_id = \"Kart_Mountain_PPO_BC_GAIL\"\n",
    "print(config_ppo_fp)\n",
    "print(run_ppo_id)\n",
    "\n",
    "# !mlagents-learn $config_ppo_fp \\\n",
    "#                --env=$env_fp \\\n",
    "#                --results-dir=$output_dir \\\n",
    "#                --run-id=$run_ppo_id --base-port=$baseport\n",
    "\n",
    "!mlagents-learn $config_ppo_fp \\\n",
    "               --results-dir=$output_dir \\\n",
    "               --run-id=$run_ppo_id --base-port=$baseport --time-scale=1 --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a4a2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlagents-learn $config_ppo_fp \\\n",
    "               --results-dir=$output_dir \\\n",
    "               --run-id=$run_ppo_id --base-port=$baseport --resume --inference --time-scale=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a47b032",
   "metadata": {},
   "source": [
    "## Training SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6886efe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/config/Kart_sac_BC_GAIL.yaml\n",
      "Kart_Mountain_SAC_BC_GAIL\n",
      "\n",
      "            ┐  ╖\n",
      "        ╓╖╬│╡  ││╬╖╖\n",
      "    ╓╖╬│││││┘  ╬│││││╬╖\n",
      " ╖╬│││││╬╜        ╙╬│││││╖╖                               ╗╗╗\n",
      " ╬╬╬╬╖││╦╖        ╖╬││╗╣╣╣╬      ╟╣╣╬    ╟╣╣╣             ╜╜╜  ╟╣╣\n",
      " ╬╬╬╬╬╬╬╬╖│╬╖╖╓╬╪│╓╣╣╣╣╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╒╣╣╖╗╣╣╣╗   ╣╣╣ ╣╣╣╣╣╣ ╟╣╣╖   ╣╣╣\n",
      " ╬╬╬╬┐  ╙╬╬╬╬│╓╣╣╣╝╜  ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╣╙ ╙╣╣╣  ╣╣╣ ╙╟╣╣╜╙  ╫╣╣  ╟╣╣\n",
      " ╬╬╬╬┐     ╙╬╬╣╣      ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣     ╣╣╣┌╣╣╜\n",
      " ╬╬╬╜       ╬╬╣╣      ╙╝╣╣╬      ╙╣╣╣╗╖╓╗╣╣╣╜ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣╦╓    ╣╣╣╣╣\n",
      " ╙   ╓╦╖    ╬╬╣╣   ╓╗╗╖            ╙╝╣╣╣╣╝╜   ╘╝╝╜   ╝╝╝  ╝╝╝   ╙╣╣╣    ╟╣╣╣\n",
      "   ╩╬╬╬╬╬╬╦╦╬╬╣╣╗╣╣╣╣╣╣╣╝                                             ╫╣╣╣╣\n",
      "      ╙╬╬╬╬╬╬╬╣╣╣╣╣╣╝╜\n",
      "          ╙╬╬╬╣╣╣╜\n",
      "             ╙\n",
      "        \n",
      " Version information:\n",
      "  ml-agents: 1.1.0,\n",
      "  ml-agents-envs: 1.1.0,\n",
      "  Communicator API: 1.5.0,\n",
      "  PyTorch: 2.8.0\n",
      "[INFO] Listening on port 5004. Start training by pressing the Play button in the Unity Editor.\n",
      "[INFO] Connected to Unity environment with package version 4.0.0 and communication version 1.5.0\n",
      "[INFO] Connected new brain: ArcadeDriver?team=0\n",
      "[INFO] Hyperparameters for behavior name ArcadeDriver: \n",
      "\ttrainer_type:\tsac\n",
      "\thyperparameters:\t\n",
      "\t  learning_rate:\t0.0003\n",
      "\t  learning_rate_schedule:\tlinear\n",
      "\t  batch_size:\t128\n",
      "\t  buffer_size:\t1024\n",
      "\t  buffer_init_steps:\t0\n",
      "\t  tau:\t0.005\n",
      "\t  steps_per_update:\t10.0\n",
      "\t  save_replay_buffer:\tFalse\n",
      "\t  init_entcoef:\t0.5\n",
      "\t  reward_signal_steps_per_update:\t10.0\n",
      "\tcheckpoint_interval:\t500000\n",
      "\tnetwork_settings:\t\n",
      "\t  normalize:\tTrue\n",
      "\t  hidden_units:\t256\n",
      "\t  num_layers:\t5\n",
      "\t  vis_encode_type:\tsimple\n",
      "\t  memory:\tNone\n",
      "\t  goal_conditioning_type:\thyper\n",
      "\t  deterministic:\tFalse\n",
      "\treward_signals:\t\n",
      "\t  extrinsic:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t1.0\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t128\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\t  gail:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t0.05\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t64\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\t    learning_rate:\t0.0003\n",
      "\t    encoding_size:\tNone\n",
      "\t    use_actions:\tTrue\n",
      "\t    use_vail:\tFalse\n",
      "\t    demo_path:\tdemo/KartAgent.demo\n",
      "\tinit_path:\tNone\n",
      "\tkeep_checkpoints:\t5\n",
      "\teven_checkpoints:\tFalse\n",
      "\tmax_steps:\t500000\n",
      "\ttime_horizon:\t64\n",
      "\tsummary_freq:\t15000\n",
      "\tthreaded:\tFalse\n",
      "\tself_play:\tNone\n",
      "\tbehavioral_cloning:\t\n",
      "\t  demo_path:\tdemo/KartAgent.demo\n",
      "\t  steps:\t0\n",
      "\t  strength:\t0.05\n",
      "\t  samples_per_update:\t512\n",
      "\t  num_epoch:\tNone\n",
      "\t  batch_size:\tNone\n",
      "[INFO] ArcadeDriver. Step: 15000. Time Elapsed: 169.155 s. Mean Reward: -0.783. Std of Reward: 3.397. Training.\n",
      "[INFO] ArcadeDriver. Step: 30000. Time Elapsed: 330.927 s. Mean Reward: -0.938. Std of Reward: 3.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 45000. Time Elapsed: 493.559 s. Mean Reward: -0.059. Std of Reward: 3.680. Training.\n",
      "[INFO] ArcadeDriver. Step: 60000. Time Elapsed: 654.904 s. Mean Reward: 0.862. Std of Reward: 5.258. Training.\n",
      "[INFO] ArcadeDriver. Step: 75000. Time Elapsed: 816.939 s. Mean Reward: 1.907. Std of Reward: 5.933. Training.\n",
      "[INFO] ArcadeDriver. Step: 90000. Time Elapsed: 979.029 s. Mean Reward: 2.318. Std of Reward: 5.965. Training.\n",
      "[INFO] ArcadeDriver. Step: 105000. Time Elapsed: 1141.037 s. Mean Reward: 2.210. Std of Reward: 6.233. Training.\n",
      "[INFO] ArcadeDriver. Step: 120000. Time Elapsed: 1302.391 s. Mean Reward: 2.174. Std of Reward: 7.029. Training.\n",
      "[INFO] ArcadeDriver. Step: 135000. Time Elapsed: 1464.988 s. Mean Reward: 2.046. Std of Reward: 5.785. Training.\n",
      "[INFO] ArcadeDriver. Step: 150000. Time Elapsed: 1627.016 s. Mean Reward: 2.043. Std of Reward: 5.803. Training.\n",
      "[INFO] ArcadeDriver. Step: 165000. Time Elapsed: 1789.610 s. Mean Reward: 2.169. Std of Reward: 5.789. Training.\n",
      "[INFO] ArcadeDriver. Step: 180000. Time Elapsed: 1951.627 s. Mean Reward: 2.191. Std of Reward: 5.900. Training.\n",
      "[INFO] ArcadeDriver. Step: 195000. Time Elapsed: 2114.048 s. Mean Reward: 2.837. Std of Reward: 6.011. Training.\n",
      "[INFO] ArcadeDriver. Step: 210000. Time Elapsed: 2275.810 s. Mean Reward: 2.109. Std of Reward: 5.790. Training.\n",
      "[INFO] ArcadeDriver. Step: 225000. Time Elapsed: 2438.074 s. Mean Reward: 2.126. Std of Reward: 5.791. Training.\n",
      "[INFO] ArcadeDriver. Step: 240000. Time Elapsed: 2600.874 s. Mean Reward: 1.941. Std of Reward: 5.843. Training.\n",
      "[INFO] ArcadeDriver. Step: 255000. Time Elapsed: 2762.907 s. Mean Reward: 1.956. Std of Reward: 5.964. Training.\n",
      "[INFO] ArcadeDriver. Step: 270000. Time Elapsed: 2924.936 s. Mean Reward: 1.928. Std of Reward: 5.880. Training.\n",
      "[INFO] ArcadeDriver. Step: 285000. Time Elapsed: 3087.709 s. Mean Reward: 1.623. Std of Reward: 6.002. Training.\n",
      "[INFO] ArcadeDriver. Step: 300000. Time Elapsed: 3250.286 s. Mean Reward: 1.712. Std of Reward: 5.708. Training.\n",
      "[INFO] ArcadeDriver. Step: 315000. Time Elapsed: 3412.949 s. Mean Reward: 2.130. Std of Reward: 6.097. Training.\n",
      "[INFO] ArcadeDriver. Step: 330000. Time Elapsed: 3575.217 s. Mean Reward: 1.487. Std of Reward: 6.038. Training.\n",
      "[INFO] ArcadeDriver. Step: 345000. Time Elapsed: 3732.216 s. Mean Reward: 2.073. Std of Reward: 6.487. Training.\n",
      "[INFO] ArcadeDriver. Step: 360000. Time Elapsed: 3885.413 s. Mean Reward: 1.846. Std of Reward: 6.304. Training.\n",
      "[INFO] ArcadeDriver. Step: 375000. Time Elapsed: 4038.573 s. Mean Reward: 0.783. Std of Reward: 6.179. Training.\n",
      "[INFO] ArcadeDriver. Step: 390000. Time Elapsed: 4191.845 s. Mean Reward: 0.956. Std of Reward: 6.068. Training.\n",
      "[INFO] ArcadeDriver. Step: 405000. Time Elapsed: 4344.693 s. Mean Reward: 1.144. Std of Reward: 6.410. Training.\n",
      "[INFO] ArcadeDriver. Step: 420000. Time Elapsed: 4498.241 s. Mean Reward: 1.044. Std of Reward: 6.321. Training.\n",
      "[INFO] ArcadeDriver. Step: 435000. Time Elapsed: 4651.294 s. Mean Reward: 0.547. Std of Reward: 6.004. Training.\n",
      "[INFO] ArcadeDriver. Step: 450000. Time Elapsed: 4803.746 s. Mean Reward: 1.482. Std of Reward: 6.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 465000. Time Elapsed: 4956.868 s. Mean Reward: 1.485. Std of Reward: 6.400. Training.\n",
      "[INFO] ArcadeDriver. Step: 480000. Time Elapsed: 5109.918 s. Mean Reward: 1.905. Std of Reward: 6.498. Training.\n",
      "[INFO] ArcadeDriver. Step: 495000. Time Elapsed: 5262.703 s. Mean Reward: 1.390. Std of Reward: 6.393. Training.\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_Mountain_SAC_BC_GAIL/ArcadeDriver/ArcadeDriver-499986.onnx\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_Mountain_SAC_BC_GAIL/ArcadeDriver/ArcadeDriver-500050.onnx\n",
      "[INFO] Copied /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_Mountain_SAC_BC_GAIL/ArcadeDriver/ArcadeDriver-500050.onnx to /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_Mountain_SAC_BC_GAIL/ArcadeDriver.onnx.\n"
     ]
    }
   ],
   "source": [
    "config_sac_fp = os.path.join(cur_dir, \"config\", \"Kart_sac_BC_GAIL.yaml\")\n",
    "run_sac_id = \"Kart_Mountain_SAC_BC_GAIL\"\n",
    "print(config_sac_fp)\n",
    "print(run_sac_id)\n",
    "\n",
    "# !mlagents-learn $config_sac_fp \\\n",
    "#                --env=$env_fp \\\n",
    "#                --results-dir=$output_dir \\\n",
    "#                --run-id=$run_sac_id --base-port=$baseport\n",
    "\n",
    "!mlagents-learn $config_sac_fp \\\n",
    "               --results-dir=$output_dir \\\n",
    "               --run-id=$run_sac_id --base-port=$baseport --time-scale=1 --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f38a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mlagents-learn $config_ppo_fp \\\n",
    "#                --results-dir=$output_dir \\\n",
    "#                --run-id=$run_sac_id --base-port=$baseport --resume --inference --time-scale=1\n",
    "\n",
    "\n",
    "!mlagents-learn $config_sac_fp \\\n",
    "               --results-dir=$output_dir \\\n",
    "               --run-id=$run_sac_id --base-port=$baseport --resume --inference --time-scale=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40c13bd",
   "metadata": {},
   "source": [
    "## Training POCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "694c6520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/config/Kart_poca_BC_GAIL.yaml\n",
      "Kart_Mountain_POCA_BC_GAIL\n",
      "\n",
      "            ┐  ╖\n",
      "        ╓╖╬│╡  ││╬╖╖\n",
      "    ╓╖╬│││││┘  ╬│││││╬╖\n",
      " ╖╬│││││╬╜        ╙╬│││││╖╖                               ╗╗╗\n",
      " ╬╬╬╬╖││╦╖        ╖╬││╗╣╣╣╬      ╟╣╣╬    ╟╣╣╣             ╜╜╜  ╟╣╣\n",
      " ╬╬╬╬╬╬╬╬╖│╬╖╖╓╬╪│╓╣╣╣╣╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╒╣╣╖╗╣╣╣╗   ╣╣╣ ╣╣╣╣╣╣ ╟╣╣╖   ╣╣╣\n",
      " ╬╬╬╬┐  ╙╬╬╬╬│╓╣╣╣╝╜  ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╣╙ ╙╣╣╣  ╣╣╣ ╙╟╣╣╜╙  ╫╣╣  ╟╣╣\n",
      " ╬╬╬╬┐     ╙╬╬╣╣      ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣     ╣╣╣┌╣╣╜\n",
      " ╬╬╬╜       ╬╬╣╣      ╙╝╣╣╬      ╙╣╣╣╗╖╓╗╣╣╣╜ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣╦╓    ╣╣╣╣╣\n",
      " ╙   ╓╦╖    ╬╬╣╣   ╓╗╗╖            ╙╝╣╣╣╣╝╜   ╘╝╝╜   ╝╝╝  ╝╝╝   ╙╣╣╣    ╟╣╣╣\n",
      "   ╩╬╬╬╬╬╬╦╦╬╬╣╣╗╣╣╣╣╣╣╣╝                                             ╫╣╣╣╣\n",
      "      ╙╬╬╬╬╬╬╬╣╣╣╣╣╣╝╜\n",
      "          ╙╬╬╬╣╣╣╜\n",
      "             ╙\n",
      "        \n",
      " Version information:\n",
      "  ml-agents: 1.1.0,\n",
      "  ml-agents-envs: 1.1.0,\n",
      "  Communicator API: 1.5.0,\n",
      "  PyTorch: 2.8.0\n",
      "[INFO] Listening on port 5004. Start training by pressing the Play button in the Unity Editor.\n",
      "[INFO] Connected to Unity environment with package version 4.0.0 and communication version 1.5.0\n",
      "[INFO] Connected new brain: ArcadeDriver?team=0\n",
      "[INFO] Hyperparameters for behavior name ArcadeDriver: \n",
      "\ttrainer_type:\tpoca\n",
      "\thyperparameters:\t\n",
      "\t  batch_size:\t128\n",
      "\t  buffer_size:\t1024\n",
      "\t  learning_rate:\t0.0003\n",
      "\t  beta:\t0.01\n",
      "\t  epsilon:\t0.2\n",
      "\t  lambd:\t0.95\n",
      "\t  num_epoch:\t3\n",
      "\t  learning_rate_schedule:\tlinear\n",
      "\t  beta_schedule:\tlinear\n",
      "\t  epsilon_schedule:\tlinear\n",
      "\tcheckpoint_interval:\t500000\n",
      "\tnetwork_settings:\t\n",
      "\t  normalize:\tTrue\n",
      "\t  hidden_units:\t256\n",
      "\t  num_layers:\t5\n",
      "\t  vis_encode_type:\tsimple\n",
      "\t  memory:\tNone\n",
      "\t  goal_conditioning_type:\thyper\n",
      "\t  deterministic:\tFalse\n",
      "\treward_signals:\t\n",
      "\t  extrinsic:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t1.0\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t128\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\t  gail:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t0.05\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t64\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\t    learning_rate:\t0.0003\n",
      "\t    encoding_size:\tNone\n",
      "\t    use_actions:\tTrue\n",
      "\t    use_vail:\tFalse\n",
      "\t    demo_path:\tdemo/KartAgent.demo\n",
      "\tinit_path:\tNone\n",
      "\tkeep_checkpoints:\t5\n",
      "\teven_checkpoints:\tFalse\n",
      "\tmax_steps:\t500000\n",
      "\ttime_horizon:\t64\n",
      "\tsummary_freq:\t15000\n",
      "\tthreaded:\tFalse\n",
      "\tself_play:\tNone\n",
      "\tbehavioral_cloning:\t\n",
      "\t  demo_path:\tdemo/KartAgent.demo\n",
      "\t  steps:\t0\n",
      "\t  strength:\t0.05\n",
      "\t  samples_per_update:\t512\n",
      "\t  num_epoch:\tNone\n",
      "\t  batch_size:\tNone\n",
      "[WARNING] Reward signal Gail is not supported with the POCA trainer; results may be unexpected.\n",
      "[INFO] ArcadeDriver. Step: 15000. Time Elapsed: 59.201 s. Mean Reward: 1.129. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 30000. Time Elapsed: 111.658 s. Mean Reward: 2.266. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 45000. Time Elapsed: 165.161 s. Mean Reward: 2.791. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 60000. Time Elapsed: 217.718 s. Mean Reward: 5.187. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 75000. Time Elapsed: 270.160 s. Mean Reward: 4.270. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 90000. Time Elapsed: 323.536 s. Mean Reward: 7.055. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 105000. Time Elapsed: 377.441 s. Mean Reward: 6.243. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 120000. Time Elapsed: 431.636 s. Mean Reward: 7.930. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 135000. Time Elapsed: 486.784 s. Mean Reward: 7.585. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 150000. Time Elapsed: 542.045 s. Mean Reward: 6.425. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 165000. Time Elapsed: 596.719 s. Mean Reward: 6.296. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 180000. Time Elapsed: 650.458 s. Mean Reward: 6.173. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 195000. Time Elapsed: 704.161 s. Mean Reward: 4.196. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 210000. Time Elapsed: 758.185 s. Mean Reward: 4.900. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 225000. Time Elapsed: 812.498 s. Mean Reward: 6.155. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 240000. Time Elapsed: 866.460 s. Mean Reward: 7.839. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 255000. Time Elapsed: 921.463 s. Mean Reward: 3.459. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 270000. Time Elapsed: 976.194 s. Mean Reward: 6.274. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 285000. Time Elapsed: 1031.130 s. Mean Reward: 5.501. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 300000. Time Elapsed: 1086.425 s. Mean Reward: 6.156. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 315000. Time Elapsed: 1141.174 s. Mean Reward: 7.537. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 330000. Time Elapsed: 1200.910 s. Mean Reward: 7.452. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 345000. Time Elapsed: 1255.813 s. Mean Reward: 8.351. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 360000. Time Elapsed: 1313.673 s. Mean Reward: 5.165. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 375000. Time Elapsed: 1369.270 s. Mean Reward: 5.139. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 390000. Time Elapsed: 1424.724 s. Mean Reward: 9.800. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 405000. Time Elapsed: 1479.275 s. Mean Reward: 10.415. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 420000. Time Elapsed: 1534.067 s. Mean Reward: 8.880. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 435000. Time Elapsed: 1589.189 s. Mean Reward: 8.798. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 450000. Time Elapsed: 1646.043 s. Mean Reward: 6.502. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 465000. Time Elapsed: 1702.864 s. Mean Reward: 6.856. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 480000. Time Elapsed: 1770.038 s. Mean Reward: 5.331. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 495000. Time Elapsed: 1826.650 s. Mean Reward: 5.914. Mean Group Reward: 0.000. Training.\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_Mountain_POCA_BC_GAIL/ArcadeDriver/ArcadeDriver-499952.onnx\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_Mountain_POCA_BC_GAIL/ArcadeDriver/ArcadeDriver-500016.onnx\n",
      "[INFO] Copied /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_Mountain_POCA_BC_GAIL/ArcadeDriver/ArcadeDriver-500016.onnx to /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_Mountain_POCA_BC_GAIL/ArcadeDriver.onnx.\n"
     ]
    }
   ],
   "source": [
    "config_poca_fp = os.path.join(cur_dir, \"config\", \"Kart_poca_BC_GAIL.yaml\")\n",
    "run_poca_id = \"Kart_Mountain_POCA_BC_GAIL\"\n",
    "print(config_poca_fp)\n",
    "print(run_poca_id)\n",
    "\n",
    "# !mlagents-learn $config_poca_fp \\\n",
    "#                --env=$env_fp \\\n",
    "#                --results-dir=$output_dir \\\n",
    "#                --run-id=$run_poca_id --base-port=$baseport\n",
    "\n",
    "!mlagents-learn $config_poca_fp \\\n",
    "               --results-dir=$output_dir \\\n",
    "               --run-id=$run_poca_id --base-port=$baseport  --time-scale=1 --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35e109ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            ┐  ╖\n",
      "        ╓╖╬│╡  ││╬╖╖\n",
      "    ╓╖╬│││││┘  ╬│││││╬╖\n",
      " ╖╬│││││╬╜        ╙╬│││││╖╖                               ╗╗╗\n",
      " ╬╬╬╬╖││╦╖        ╖╬││╗╣╣╣╬      ╟╣╣╬    ╟╣╣╣             ╜╜╜  ╟╣╣\n",
      " ╬╬╬╬╬╬╬╬╖│╬╖╖╓╬╪│╓╣╣╣╣╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╒╣╣╖╗╣╣╣╗   ╣╣╣ ╣╣╣╣╣╣ ╟╣╣╖   ╣╣╣\n",
      " ╬╬╬╬┐  ╙╬╬╬╬│╓╣╣╣╝╜  ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╣╙ ╙╣╣╣  ╣╣╣ ╙╟╣╣╜╙  ╫╣╣  ╟╣╣\n",
      " ╬╬╬╬┐     ╙╬╬╣╣      ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣     ╣╣╣┌╣╣╜\n",
      " ╬╬╬╜       ╬╬╣╣      ╙╝╣╣╬      ╙╣╣╣╗╖╓╗╣╣╣╜ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣╦╓    ╣╣╣╣╣\n",
      " ╙   ╓╦╖    ╬╬╣╣   ╓╗╗╖            ╙╝╣╣╣╣╝╜   ╘╝╝╜   ╝╝╝  ╝╝╝   ╙╣╣╣    ╟╣╣╣\n",
      "   ╩╬╬╬╬╬╬╦╦╬╬╣╣╗╣╣╣╣╣╣╣╝                                             ╫╣╣╣╣\n",
      "      ╙╬╬╬╬╬╬╬╣╣╣╣╣╣╝╜\n",
      "          ╙╬╬╬╣╣╣╜\n",
      "             ╙\n",
      "        \n",
      " Version information:\n",
      "  ml-agents: 1.1.0,\n",
      "  ml-agents-envs: 1.1.0,\n",
      "  Communicator API: 1.5.0,\n",
      "  PyTorch: 2.8.0\n",
      "[INFO] Listening on port 5004. Start training by pressing the Play button in the Unity Editor.\n",
      "[INFO] Connected to Unity environment with package version 4.0.0 and communication version 1.5.0\n",
      "[INFO] Connected new brain: ArcadeDriver?team=0\n",
      "[INFO] Hyperparameters for behavior name ArcadeDriver: \n",
      "\ttrainer_type:\tpoca\n",
      "\thyperparameters:\t\n",
      "\t  batch_size:\t128\n",
      "\t  buffer_size:\t1024\n",
      "\t  learning_rate:\t0.0003\n",
      "\t  beta:\t0.01\n",
      "\t  epsilon:\t0.2\n",
      "\t  lambd:\t0.95\n",
      "\t  num_epoch:\t3\n",
      "\t  learning_rate_schedule:\tlinear\n",
      "\t  beta_schedule:\tlinear\n",
      "\t  epsilon_schedule:\tlinear\n",
      "\tcheckpoint_interval:\t500000\n",
      "\tnetwork_settings:\t\n",
      "\t  normalize:\tTrue\n",
      "\t  hidden_units:\t256\n",
      "\t  num_layers:\t5\n",
      "\t  vis_encode_type:\tsimple\n",
      "\t  memory:\tNone\n",
      "\t  goal_conditioning_type:\thyper\n",
      "\t  deterministic:\tFalse\n",
      "\treward_signals:\t\n",
      "\t  extrinsic:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t1.0\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t128\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\t  gail:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t0.05\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t64\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\t    learning_rate:\t0.0003\n",
      "\t    encoding_size:\tNone\n",
      "\t    use_actions:\tTrue\n",
      "\t    use_vail:\tFalse\n",
      "\t    demo_path:\tdemo/KartAgent.demo\n",
      "\tinit_path:\tNone\n",
      "\tkeep_checkpoints:\t5\n",
      "\teven_checkpoints:\tFalse\n",
      "\tmax_steps:\t500000\n",
      "\ttime_horizon:\t64\n",
      "\tsummary_freq:\t15000\n",
      "\tthreaded:\tFalse\n",
      "\tself_play:\tNone\n",
      "\tbehavioral_cloning:\t\n",
      "\t  demo_path:\tdemo/KartAgent.demo\n",
      "\t  steps:\t0\n",
      "\t  strength:\t0.05\n",
      "\t  samples_per_update:\t512\n",
      "\t  num_epoch:\tNone\n",
      "\t  batch_size:\tNone\n",
      "[WARNING] Reward signal Gail is not supported with the POCA trainer; results may be unexpected.\n",
      "[INFO] Resuming from /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_Mountain_POCA_BC_GAIL/ArcadeDriver.\n",
      "[INFO] Resuming training from step 500016.\n",
      "[WARNING] Restarting worker[0] after 'Communicator has exited.'\n",
      "[INFO] Listening on port 5004. Start training by pressing the Play button in the Unity Editor.\n",
      "^C\n",
      "Exception ignored in atexit callback: <function _exit_function at 0x12420eb90>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/multiprocessing/util.py\", line 357, in _exit_function\n",
      "    p.join()\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/multiprocessing/popen_fork.py\", line 43, in wait\n",
      "    return self.poll(os.WNOHANG if timeout == 0.0 else 0)\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/multiprocessing/popen_fork.py\", line 27, in poll\n",
      "    pid, sts = os.waitpid(self.pid, flag)\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "!mlagents-learn $config_poca_fp \\\n",
    "               --results-dir=$output_dir \\\n",
    "               --run-id=$run_poca_id --base-port=$baseport --resume --inference --time-scale=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad33878",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5c2e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mlagents-learn $config_fp \\\n",
    "#                --env=$env_fp \\\n",
    "#                --results-dir=$test_dir \\\n",
    "#                --run-id=$run_id \\\n",
    "#                --resume --inference\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
