{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70a95c2c",
   "metadata": {},
   "source": [
    "# Kart_ML-Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8ba6fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "\n",
    "# Global Setting\n",
    "cur_dir = os.getcwd()\n",
    "env_dir = os.path.abspath(os.path.join(cur_dir, \"..\", \"Unity6000_Envs\"))\n",
    "output_dir = os.path.abspath(os.path.join(cur_dir, \"temp\", \"mlagents_learn_output\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a62e191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Unity6000_Envs/Kart_Darwin.app\n"
     ]
    }
   ],
   "source": [
    "# Unity Enviroment\n",
    "game = \"Kart\"\n",
    "os_name = platform.system()\n",
    "\n",
    "if os_name == 'Linux':\n",
    "    env_name = os.path.join(env_dir, f\"{game}_{os_name}.x86_64\")\n",
    "elif os_name == 'Darwin':\n",
    "    env_name = os.path.join(env_dir, f\"{game}_{os_name}.app\")\n",
    "env_fp = os.path.join(env_dir, env_name)\n",
    "print(env_fp)\n",
    "\n",
    "baseport = 1990"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9e227b",
   "metadata": {},
   "source": [
    "## Training PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01b4cca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/config/Kart_ppo.yaml\n",
      "Kart_PPO\n",
      "/Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output\n",
      "\n",
      "            ┐  ╖\n",
      "        ╓╖╬│╡  ││╬╖╖\n",
      "    ╓╖╬│││││┘  ╬│││││╬╖\n",
      " ╖╬│││││╬╜        ╙╬│││││╖╖                               ╗╗╗\n",
      " ╬╬╬╬╖││╦╖        ╖╬││╗╣╣╣╬      ╟╣╣╬    ╟╣╣╣             ╜╜╜  ╟╣╣\n",
      " ╬╬╬╬╬╬╬╬╖│╬╖╖╓╬╪│╓╣╣╣╣╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╒╣╣╖╗╣╣╣╗   ╣╣╣ ╣╣╣╣╣╣ ╟╣╣╖   ╣╣╣\n",
      " ╬╬╬╬┐  ╙╬╬╬╬│╓╣╣╣╝╜  ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╣╙ ╙╣╣╣  ╣╣╣ ╙╟╣╣╜╙  ╫╣╣  ╟╣╣\n",
      " ╬╬╬╬┐     ╙╬╬╣╣      ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣     ╣╣╣┌╣╣╜\n",
      " ╬╬╬╜       ╬╬╣╣      ╙╝╣╣╬      ╙╣╣╣╗╖╓╗╣╣╣╜ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣╦╓    ╣╣╣╣╣\n",
      " ╙   ╓╦╖    ╬╬╣╣   ╓╗╗╖            ╙╝╣╣╣╣╝╜   ╘╝╝╜   ╝╝╝  ╝╝╝   ╙╣╣╣    ╟╣╣╣\n",
      "   ╩╬╬╬╬╬╬╦╦╬╬╣╣╗╣╣╣╣╣╣╣╝                                             ╫╣╣╣╣\n",
      "      ╙╬╬╬╬╬╬╬╣╣╣╣╣╣╝╜\n",
      "          ╙╬╬╬╣╣╣╜\n",
      "             ╙\n",
      "        \n",
      " Version information:\n",
      "  ml-agents: 1.1.0,\n",
      "  ml-agents-envs: 1.1.0,\n",
      "  Communicator API: 1.5.0,\n",
      "  PyTorch: 2.8.0\n",
      "[INFO] Listening on port 5004. Start training by pressing the Play button in the Unity Editor.\n",
      "[INFO] Connected to Unity environment with package version 4.0.0 and communication version 1.5.0\n",
      "[INFO] Connected new brain: ArcadeDriver?team=0\n",
      "[INFO] Hyperparameters for behavior name ArcadeDriver: \n",
      "\ttrainer_type:\tppo\n",
      "\thyperparameters:\t\n",
      "\t  batch_size:\t128\n",
      "\t  buffer_size:\t1024\n",
      "\t  learning_rate:\t0.0003\n",
      "\t  beta:\t0.01\n",
      "\t  epsilon:\t0.2\n",
      "\t  lambd:\t0.95\n",
      "\t  num_epoch:\t3\n",
      "\t  shared_critic:\tFalse\n",
      "\t  learning_rate_schedule:\tlinear\n",
      "\t  beta_schedule:\tlinear\n",
      "\t  epsilon_schedule:\tlinear\n",
      "\tcheckpoint_interval:\t500000\n",
      "\tnetwork_settings:\t\n",
      "\t  normalize:\tTrue\n",
      "\t  hidden_units:\t256\n",
      "\t  num_layers:\t5\n",
      "\t  vis_encode_type:\tsimple\n",
      "\t  memory:\tNone\n",
      "\t  goal_conditioning_type:\thyper\n",
      "\t  deterministic:\tFalse\n",
      "\treward_signals:\t\n",
      "\t  extrinsic:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t1.0\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t128\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\tinit_path:\tNone\n",
      "\tkeep_checkpoints:\t5\n",
      "\teven_checkpoints:\tFalse\n",
      "\tmax_steps:\t500000\n",
      "\ttime_horizon:\t64\n",
      "\tsummary_freq:\t15000\n",
      "\tthreaded:\tFalse\n",
      "\tself_play:\tNone\n",
      "\tbehavioral_cloning:\tNone\n",
      "[INFO] ArcadeDriver. Step: 15000. Time Elapsed: 40.716 s. Mean Reward: -5.883. Std of Reward: 0.505. Training.\n",
      "[INFO] ArcadeDriver. Step: 30000. Time Elapsed: 72.875 s. Mean Reward: -5.974. Std of Reward: 0.165. Training.\n",
      "[INFO] ArcadeDriver. Step: 45000. Time Elapsed: 104.563 s. Mean Reward: -5.964. Std of Reward: 0.186. Training.\n",
      "[INFO] ArcadeDriver. Step: 60000. Time Elapsed: 136.413 s. Mean Reward: -5.989. Std of Reward: 0.049. Training.\n",
      "[INFO] ArcadeDriver. Step: 75000. Time Elapsed: 168.717 s. Mean Reward: -5.989. Std of Reward: 0.026. Training.\n",
      "[INFO] ArcadeDriver. Step: 90000. Time Elapsed: 201.229 s. Mean Reward: -5.989. Std of Reward: 0.021. Training.\n",
      "[INFO] ArcadeDriver. Step: 105000. Time Elapsed: 233.354 s. Mean Reward: -5.992. Std of Reward: 0.011. Training.\n",
      "[INFO] ArcadeDriver. Step: 120000. Time Elapsed: 265.392 s. Mean Reward: -5.993. Std of Reward: 0.013. Training.\n",
      "[INFO] ArcadeDriver. Step: 135000. Time Elapsed: 297.251 s. Mean Reward: -5.993. Std of Reward: 0.010. Training.\n",
      "[INFO] ArcadeDriver. Step: 150000. Time Elapsed: 329.260 s. Mean Reward: -5.993. Std of Reward: 0.008. Training.\n",
      "[INFO] ArcadeDriver. Step: 165000. Time Elapsed: 361.135 s. Mean Reward: -5.993. Std of Reward: 0.008. Training.\n",
      "[INFO] ArcadeDriver. Step: 180000. Time Elapsed: 392.965 s. Mean Reward: -5.993. Std of Reward: 0.010. Training.\n",
      "[INFO] ArcadeDriver. Step: 195000. Time Elapsed: 424.760 s. Mean Reward: -5.992. Std of Reward: 0.009. Training.\n",
      "[INFO] ArcadeDriver. Step: 210000. Time Elapsed: 456.695 s. Mean Reward: -5.993. Std of Reward: 0.008. Training.\n",
      "[INFO] ArcadeDriver. Step: 225000. Time Elapsed: 488.391 s. Mean Reward: -5.993. Std of Reward: 0.008. Training.\n",
      "[INFO] ArcadeDriver. Step: 240000. Time Elapsed: 520.197 s. Mean Reward: -5.993. Std of Reward: 0.015. Training.\n",
      "[INFO] ArcadeDriver. Step: 255000. Time Elapsed: 551.818 s. Mean Reward: -5.993. Std of Reward: 0.009. Training.\n",
      "[INFO] ArcadeDriver. Step: 270000. Time Elapsed: 583.819 s. Mean Reward: -5.993. Std of Reward: 0.007. Training.\n",
      "[INFO] ArcadeDriver. Step: 285000. Time Elapsed: 615.589 s. Mean Reward: -5.993. Std of Reward: 0.007. Training.\n",
      "[INFO] ArcadeDriver. Step: 300000. Time Elapsed: 647.504 s. Mean Reward: -5.993. Std of Reward: 0.010. Training.\n",
      "[INFO] ArcadeDriver. Step: 315000. Time Elapsed: 679.182 s. Mean Reward: -5.992. Std of Reward: 0.008. Training.\n",
      "[INFO] ArcadeDriver. Step: 330000. Time Elapsed: 710.917 s. Mean Reward: -5.993. Std of Reward: 0.007. Training.\n",
      "[INFO] ArcadeDriver. Step: 345000. Time Elapsed: 742.625 s. Mean Reward: -5.993. Std of Reward: 0.007. Training.\n",
      "[INFO] ArcadeDriver. Step: 360000. Time Elapsed: 774.586 s. Mean Reward: -5.993. Std of Reward: 0.008. Training.\n",
      "[INFO] ArcadeDriver. Step: 375000. Time Elapsed: 806.316 s. Mean Reward: -5.992. Std of Reward: 0.007. Training.\n",
      "[INFO] ArcadeDriver. Step: 390000. Time Elapsed: 838.213 s. Mean Reward: -5.992. Std of Reward: 0.007. Training.\n",
      "[INFO] ArcadeDriver. Step: 405000. Time Elapsed: 869.994 s. Mean Reward: -5.992. Std of Reward: 0.007. Training.\n",
      "[INFO] ArcadeDriver. Step: 420000. Time Elapsed: 901.749 s. Mean Reward: -5.993. Std of Reward: 0.008. Training.\n",
      "[INFO] ArcadeDriver. Step: 435000. Time Elapsed: 933.418 s. Mean Reward: -5.992. Std of Reward: 0.007. Training.\n",
      "[INFO] ArcadeDriver. Step: 450000. Time Elapsed: 965.225 s. Mean Reward: -5.992. Std of Reward: 0.007. Training.\n",
      "[INFO] ArcadeDriver. Step: 465000. Time Elapsed: 996.865 s. Mean Reward: -5.992. Std of Reward: 0.007. Training.\n",
      "[INFO] ArcadeDriver. Step: 480000. Time Elapsed: 1028.934 s. Mean Reward: -5.993. Std of Reward: 0.007. Training.\n",
      "[INFO] ArcadeDriver. Step: 495000. Time Elapsed: 1060.774 s. Mean Reward: -5.992. Std of Reward: 0.007. Training.\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_PPO/ArcadeDriver/ArcadeDriver-499984.onnx\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_PPO/ArcadeDriver/ArcadeDriver-500001.onnx\n",
      "[INFO] Copied /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_PPO/ArcadeDriver/ArcadeDriver-500001.onnx to /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_PPO/ArcadeDriver.onnx.\n"
     ]
    }
   ],
   "source": [
    "config_ppo_fp = os.path.join(cur_dir, \"config\", \"Kart_ppo.yaml\")\n",
    "run_ppo_id = \"Kart_PPO\"\n",
    "print(config_ppo_fp)\n",
    "print(run_ppo_id)\n",
    "print(output_dir)\n",
    "\n",
    "# !mlagents-learn $config_ppo_fp \\\n",
    "#             #    --env=$env_fp \\\n",
    "#                --results-dir=$output_dir \\\n",
    "#                --run-id=$run_ppo_id --base-port=$baseport\n",
    "\n",
    "!mlagents-learn $config_ppo_fp \\\n",
    "               --results-dir=$output_dir \\\n",
    "               --run-id=$run_ppo_id --base-port=$baseport\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c63f0a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            ┐  ╖\n",
      "        ╓╖╬│╡  ││╬╖╖\n",
      "    ╓╖╬│││││┘  ╬│││││╬╖\n",
      " ╖╬│││││╬╜        ╙╬│││││╖╖                               ╗╗╗\n",
      " ╬╬╬╬╖││╦╖        ╖╬││╗╣╣╣╬      ╟╣╣╬    ╟╣╣╣             ╜╜╜  ╟╣╣\n",
      " ╬╬╬╬╬╬╬╬╖│╬╖╖╓╬╪│╓╣╣╣╣╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╒╣╣╖╗╣╣╣╗   ╣╣╣ ╣╣╣╣╣╣ ╟╣╣╖   ╣╣╣\n",
      " ╬╬╬╬┐  ╙╬╬╬╬│╓╣╣╣╝╜  ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╣╙ ╙╣╣╣  ╣╣╣ ╙╟╣╣╜╙  ╫╣╣  ╟╣╣\n",
      " ╬╬╬╬┐     ╙╬╬╣╣      ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣     ╣╣╣┌╣╣╜\n",
      " ╬╬╬╜       ╬╬╣╣      ╙╝╣╣╬      ╙╣╣╣╗╖╓╗╣╣╣╜ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣╦╓    ╣╣╣╣╣\n",
      " ╙   ╓╦╖    ╬╬╣╣   ╓╗╗╖            ╙╝╣╣╣╣╝╜   ╘╝╝╜   ╝╝╝  ╝╝╝   ╙╣╣╣    ╟╣╣╣\n",
      "   ╩╬╬╬╬╬╬╦╦╬╬╣╣╗╣╣╣╣╣╣╣╝                                             ╫╣╣╣╣\n",
      "      ╙╬╬╬╬╬╬╬╣╣╣╣╣╣╝╜\n",
      "          ╙╬╬╬╣╣╣╜\n",
      "             ╙\n",
      "        \n",
      " Version information:\n",
      "  ml-agents: 1.1.0,\n",
      "  ml-agents-envs: 1.1.0,\n",
      "  Communicator API: 1.5.0,\n",
      "  PyTorch: 2.8.0\n",
      "[INFO] Listening on port 5004. Start training by pressing the Play button in the Unity Editor.\n",
      "[INFO] Connected to Unity environment with package version 4.0.0 and communication version 1.5.0\n",
      "[INFO] Connected new brain: ArcadeDriver?team=0\n",
      "[INFO] Hyperparameters for behavior name ArcadeDriver: \n",
      "\ttrainer_type:\tppo\n",
      "\thyperparameters:\t\n",
      "\t  batch_size:\t128\n",
      "\t  buffer_size:\t1024\n",
      "\t  learning_rate:\t0.0003\n",
      "\t  beta:\t0.01\n",
      "\t  epsilon:\t0.2\n",
      "\t  lambd:\t0.95\n",
      "\t  num_epoch:\t3\n",
      "\t  shared_critic:\tFalse\n",
      "\t  learning_rate_schedule:\tlinear\n",
      "\t  beta_schedule:\tlinear\n",
      "\t  epsilon_schedule:\tlinear\n",
      "\tcheckpoint_interval:\t500000\n",
      "\tnetwork_settings:\t\n",
      "\t  normalize:\tTrue\n",
      "\t  hidden_units:\t256\n",
      "\t  num_layers:\t5\n",
      "\t  vis_encode_type:\tsimple\n",
      "\t  memory:\tNone\n",
      "\t  goal_conditioning_type:\thyper\n",
      "\t  deterministic:\tFalse\n",
      "\treward_signals:\t\n",
      "\t  extrinsic:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t1.0\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t128\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\tinit_path:\tNone\n",
      "\tkeep_checkpoints:\t5\n",
      "\teven_checkpoints:\tFalse\n",
      "\tmax_steps:\t500000\n",
      "\ttime_horizon:\t64\n",
      "\tsummary_freq:\t15000\n",
      "\tthreaded:\tFalse\n",
      "\tself_play:\tNone\n",
      "\tbehavioral_cloning:\tNone\n",
      "[INFO] Resuming from /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_PPO/ArcadeDriver.\n",
      "[INFO] Resuming training from step 500001.\n",
      "[INFO] ArcadeDriver. Step: 510000. Time Elapsed: 27.710 s. Mean Reward: -5.977. Std of Reward: 0.154. Not Training.\n",
      "[INFO] ArcadeDriver. Step: 525000. Time Elapsed: 56.689 s. Mean Reward: -6.009. Std of Reward: 0.012. Not Training.\n",
      "[INFO] ArcadeDriver. Step: 540000. Time Elapsed: 85.686 s. Mean Reward: -6.009. Std of Reward: 0.012. Not Training.\n",
      "[INFO] ArcadeDriver. Step: 555000. Time Elapsed: 117.316 s. Mean Reward: -6.000. Std of Reward: 0.020. Not Training.\n",
      "[WARNING] Restarting worker[0] after 'Communicator has exited.'\n",
      "[INFO] Listening on port 5004. Start training by pressing the Play button in the Unity Editor.\n",
      "^C\n",
      "Exception ignored in atexit callback: <function _exit_function at 0x146d0eb90>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/multiprocessing/util.py\", line 357, in _exit_function\n",
      "    p.join()\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/multiprocessing/popen_fork.py\", line 43, in wait\n",
      "    return self.poll(os.WNOHANG if timeout == 0.0 else 0)\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/multiprocessing/popen_fork.py\", line 27, in poll\n",
      "    pid, sts = os.waitpid(self.pid, flag)\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "# !mlagents-learn $config_ppo_fp \\\n",
    "#                --env=$env_fp \\\n",
    "#                --results-dir=$output_dir \\\n",
    "#                --run-id=$run_ppo_id --base-port=$baseport --resume --inference\n",
    "\n",
    "!mlagents-learn $config_ppo_fp \\\n",
    "                --results-dir=$output_dir \\\n",
    "                --run-id=$run_ppo_id --base-port=$baseport --resume --inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a47b032",
   "metadata": {},
   "source": [
    "## Training SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6886efe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_sac_fp = os.path.join(cur_dir, \"config\", \"Kart_sac.yaml\")\n",
    "run_sac_id = \"Kart_SAC\"\n",
    "print(config_ppo_fp)\n",
    "print(run_sac_id)\n",
    "\n",
    "!mlagents-learn $config_ppo_fp \\\n",
    "               --env=$env_fp \\\n",
    "               --results-dir=$output_dir \\\n",
    "               --run-id=$run_sac_id --base-port=$baseport"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40c13bd",
   "metadata": {},
   "source": [
    "## Training POCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694c6520",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_poca_fp = os.path.join(cur_dir, \"config\", \"Kart_poca.yaml\")\n",
    "run_poca_id = \"Kart_POCA\"\n",
    "print(config_poca_fp)\n",
    "print(run_poca_id)\n",
    "\n",
    "!mlagents-learn $config_poca_fp \\\n",
    "               --env=$env_fp \\\n",
    "               --results-dir=$output_dir \\\n",
    "               --run-id=$run_poca_id --base-port=$baseport"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad33878",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5c2e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mlagents-learn $config_fp \\\n",
    "#                --env=$env_fp \\\n",
    "#                --results-dir=$test_dir \\\n",
    "#                --run-id=$run_id \\\n",
    "#                --resume --inference\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
