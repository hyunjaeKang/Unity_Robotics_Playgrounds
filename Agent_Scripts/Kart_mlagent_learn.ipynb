{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70a95c2c",
   "metadata": {},
   "source": [
    "# Kart_ML-Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8ba6fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "\n",
    "# Global Setting\n",
    "cur_dir = os.getcwd()\n",
    "env_dir = os.path.abspath(os.path.join(cur_dir, \"..\", \"Unity6000_Envs\"))\n",
    "output_dir = os.path.abspath(os.path.join(cur_dir, \"temp\", \"mlagents_learn_output\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a62e191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Unity6000_Envs/Kart_Darwin.app\n"
     ]
    }
   ],
   "source": [
    "# Unity Enviroment\n",
    "game = \"Kart\"\n",
    "os_name = platform.system()\n",
    "\n",
    "if os_name == 'Linux':\n",
    "    env_name = os.path.join(env_dir, f\"{game}_{os_name}.x86_64\")\n",
    "elif os_name == 'Darwin':\n",
    "    env_name = os.path.join(env_dir, f\"{game}_{os_name}.app\")\n",
    "env_fp = os.path.join(env_dir, env_name)\n",
    "print(env_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9e227b",
   "metadata": {},
   "source": [
    "## Training PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01b4cca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/config/Kart_ppo.yaml\n",
      "Kart_PPO\n",
      "\n",
      "            ┐  ╖\n",
      "        ╓╖╬│╡  ││╬╖╖\n",
      "    ╓╖╬│││││┘  ╬│││││╬╖\n",
      " ╖╬│││││╬╜        ╙╬│││││╖╖                               ╗╗╗\n",
      " ╬╬╬╬╖││╦╖        ╖╬││╗╣╣╣╬      ╟╣╣╬    ╟╣╣╣             ╜╜╜  ╟╣╣\n",
      " ╬╬╬╬╬╬╬╬╖│╬╖╖╓╬╪│╓╣╣╣╣╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╒╣╣╖╗╣╣╣╗   ╣╣╣ ╣╣╣╣╣╣ ╟╣╣╖   ╣╣╣\n",
      " ╬╬╬╬┐  ╙╬╬╬╬│╓╣╣╣╝╜  ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╣╙ ╙╣╣╣  ╣╣╣ ╙╟╣╣╜╙  ╫╣╣  ╟╣╣\n",
      " ╬╬╬╬┐     ╙╬╬╣╣      ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣     ╣╣╣┌╣╣╜\n",
      " ╬╬╬╜       ╬╬╣╣      ╙╝╣╣╬      ╙╣╣╣╗╖╓╗╣╣╣╜ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣╦╓    ╣╣╣╣╣\n",
      " ╙   ╓╦╖    ╬╬╣╣   ╓╗╗╖            ╙╝╣╣╣╣╝╜   ╘╝╝╜   ╝╝╝  ╝╝╝   ╙╣╣╣    ╟╣╣╣\n",
      "   ╩╬╬╬╬╬╬╦╦╬╬╣╣╗╣╣╣╣╣╣╣╝                                             ╫╣╣╣╣\n",
      "      ╙╬╬╬╬╬╬╬╣╣╣╣╣╣╝╜\n",
      "          ╙╬╬╬╣╣╣╜\n",
      "             ╙\n",
      "        \n",
      " Version information:\n",
      "  ml-agents: 1.1.0,\n",
      "  ml-agents-envs: 1.1.0,\n",
      "  Communicator API: 1.5.0,\n",
      "  PyTorch: 2.8.0\n",
      "[INFO] Connected to Unity environment with package version 4.0.0 and communication version 1.5.0\n",
      "[INFO] Connected new brain: ArcadeDriver?team=0\n",
      "[INFO] Hyperparameters for behavior name ArcadeDriver: \n",
      "\ttrainer_type:\tppo\n",
      "\thyperparameters:\t\n",
      "\t  batch_size:\t128\n",
      "\t  buffer_size:\t1024\n",
      "\t  learning_rate:\t0.0003\n",
      "\t  beta:\t0.01\n",
      "\t  epsilon:\t0.2\n",
      "\t  lambd:\t0.95\n",
      "\t  num_epoch:\t3\n",
      "\t  shared_critic:\tFalse\n",
      "\t  learning_rate_schedule:\tlinear\n",
      "\t  beta_schedule:\tlinear\n",
      "\t  epsilon_schedule:\tlinear\n",
      "\tcheckpoint_interval:\t500000\n",
      "\tnetwork_settings:\t\n",
      "\t  normalize:\tTrue\n",
      "\t  hidden_units:\t256\n",
      "\t  num_layers:\t5\n",
      "\t  vis_encode_type:\tsimple\n",
      "\t  memory:\tNone\n",
      "\t  goal_conditioning_type:\thyper\n",
      "\t  deterministic:\tFalse\n",
      "\treward_signals:\t\n",
      "\t  extrinsic:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t1.0\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t128\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\tinit_path:\tNone\n",
      "\tkeep_checkpoints:\t5\n",
      "\teven_checkpoints:\tFalse\n",
      "\tmax_steps:\t500000\n",
      "\ttime_horizon:\t64\n",
      "\tsummary_freq:\t15000\n",
      "\tthreaded:\tFalse\n",
      "\tself_play:\tNone\n",
      "\tbehavioral_cloning:\tNone\n",
      "[INFO] ArcadeDriver. Step: 15000. Time Elapsed: 34.926 s. Mean Reward: -4.983. Std of Reward: 0.192. Training.\n",
      "[INFO] ArcadeDriver. Step: 30000. Time Elapsed: 66.659 s. Mean Reward: -4.986. Std of Reward: 0.031. Training.\n",
      "[INFO] ArcadeDriver. Step: 45000. Time Elapsed: 99.378 s. Mean Reward: -5.466. Std of Reward: 0.503. Training.\n",
      "[INFO] ArcadeDriver. Step: 60000. Time Elapsed: 132.171 s. Mean Reward: -5.981. Std of Reward: 0.030. Training.\n",
      "[INFO] ArcadeDriver. Step: 75000. Time Elapsed: 164.762 s. Mean Reward: -5.986. Std of Reward: 0.020. Training.\n",
      "[INFO] ArcadeDriver. Step: 90000. Time Elapsed: 197.570 s. Mean Reward: -5.987. Std of Reward: 0.016. Training.\n",
      "[INFO] ArcadeDriver. Step: 105000. Time Elapsed: 230.171 s. Mean Reward: -5.989. Std of Reward: 0.010. Training.\n",
      "[INFO] ArcadeDriver. Step: 120000. Time Elapsed: 262.970 s. Mean Reward: -5.988. Std of Reward: 0.014. Training.\n",
      "[INFO] ArcadeDriver. Step: 135000. Time Elapsed: 295.554 s. Mean Reward: -5.989. Std of Reward: 0.009. Training.\n",
      "[INFO] ArcadeDriver. Step: 150000. Time Elapsed: 328.329 s. Mean Reward: -5.988. Std of Reward: 0.014. Training.\n",
      "[INFO] ArcadeDriver. Step: 165000. Time Elapsed: 360.970 s. Mean Reward: -5.990. Std of Reward: 0.010. Training.\n",
      "[INFO] ArcadeDriver. Step: 180000. Time Elapsed: 393.812 s. Mean Reward: -5.989. Std of Reward: 0.014. Training.\n",
      "[INFO] ArcadeDriver. Step: 195000. Time Elapsed: 426.462 s. Mean Reward: -5.991. Std of Reward: 0.011. Training.\n",
      "[INFO] ArcadeDriver. Step: 210000. Time Elapsed: 459.337 s. Mean Reward: -5.991. Std of Reward: 0.013. Training.\n",
      "[INFO] ArcadeDriver. Step: 225000. Time Elapsed: 491.957 s. Mean Reward: -5.992. Std of Reward: 0.013. Training.\n",
      "[INFO] ArcadeDriver. Step: 240000. Time Elapsed: 524.801 s. Mean Reward: -5.993. Std of Reward: 0.012. Training.\n",
      "[INFO] ArcadeDriver. Step: 255000. Time Elapsed: 557.506 s. Mean Reward: -5.989. Std of Reward: 0.069. Training.\n",
      "[INFO] ArcadeDriver. Step: 270000. Time Elapsed: 590.446 s. Mean Reward: -5.995. Std of Reward: 0.012. Training.\n",
      "[INFO] ArcadeDriver. Step: 285000. Time Elapsed: 623.246 s. Mean Reward: -5.997. Std of Reward: 0.012. Training.\n",
      "[INFO] ArcadeDriver. Step: 300000. Time Elapsed: 656.195 s. Mean Reward: -5.998. Std of Reward: 0.012. Training.\n",
      "[INFO] ArcadeDriver. Step: 315000. Time Elapsed: 688.995 s. Mean Reward: -6.001. Std of Reward: 0.013. Training.\n",
      "[INFO] ArcadeDriver. Step: 330000. Time Elapsed: 721.928 s. Mean Reward: -5.977. Std of Reward: 0.161. Training.\n",
      "[INFO] ArcadeDriver. Step: 345000. Time Elapsed: 754.978 s. Mean Reward: -5.988. Std of Reward: 0.084. Training.\n",
      "[INFO] ArcadeDriver. Step: 360000. Time Elapsed: 788.144 s. Mean Reward: -5.995. Std of Reward: 0.067. Training.\n",
      "[INFO] ArcadeDriver. Step: 375000. Time Elapsed: 821.002 s. Mean Reward: -5.924. Std of Reward: 0.271. Training.\n",
      "[INFO] ArcadeDriver. Step: 390000. Time Elapsed: 854.126 s. Mean Reward: -5.795. Std of Reward: 0.411. Training.\n",
      "[INFO] ArcadeDriver. Step: 405000. Time Elapsed: 889.284 s. Mean Reward: -5.747. Std of Reward: 0.442. Training.\n",
      "[INFO] ArcadeDriver. Step: 420000. Time Elapsed: 928.018 s. Mean Reward: -5.992. Std of Reward: 0.012. Training.\n",
      "[INFO] ArcadeDriver. Step: 435000. Time Elapsed: 966.384 s. Mean Reward: -5.993. Std of Reward: 0.013. Training.\n",
      "[INFO] ArcadeDriver. Step: 450000. Time Elapsed: 1004.975 s. Mean Reward: -5.990. Std of Reward: 0.060. Training.\n",
      "[INFO] ArcadeDriver. Step: 465000. Time Elapsed: 1043.075 s. Mean Reward: -5.995. Std of Reward: 0.012. Training.\n",
      "[INFO] ArcadeDriver. Step: 480000. Time Elapsed: 1081.632 s. Mean Reward: -5.997. Std of Reward: 0.012. Training.\n",
      "[INFO] ArcadeDriver. Step: 495000. Time Elapsed: 1120.307 s. Mean Reward: -5.998. Std of Reward: 0.012. Training.\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_PPO/ArcadeDriver/ArcadeDriver-499984.onnx\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_PPO/ArcadeDriver/ArcadeDriver-500001.onnx\n",
      "[INFO] Copied /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_PPO/ArcadeDriver/ArcadeDriver-500001.onnx to /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_PPO/ArcadeDriver.onnx.\n"
     ]
    }
   ],
   "source": [
    "config_ppo_fp = os.path.join(cur_dir, \"config\", \"Kart_ppo.yaml\")\n",
    "run_ppo_id = \"Kart_PPO\"\n",
    "print(config_ppo_fp)\n",
    "print(run_ppo_id)\n",
    "\n",
    "!mlagents-learn $config_ppo_fp \\\n",
    "               --env=$env_fp \\\n",
    "               --results-dir=$output_dir \\\n",
    "               --run-id=$run_ppo_id --base-port=1990\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c63f0a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mlagents-learn $config_ppo_fp \\\n",
    "#                --env=$env_fp \\\n",
    "#                --results-dir=$output_dir \\\n",
    "#                --run-id=$run_ppo_id --base-port=1990 --resume --inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a47b032",
   "metadata": {},
   "source": [
    "## Training SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6886efe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/config/Kart_ppo.yaml\n",
      "Kart_SAC\n",
      "\n",
      "            ┐  ╖\n",
      "        ╓╖╬│╡  ││╬╖╖\n",
      "    ╓╖╬│││││┘  ╬│││││╬╖\n",
      " ╖╬│││││╬╜        ╙╬│││││╖╖                               ╗╗╗\n",
      " ╬╬╬╬╖││╦╖        ╖╬││╗╣╣╣╬      ╟╣╣╬    ╟╣╣╣             ╜╜╜  ╟╣╣\n",
      " ╬╬╬╬╬╬╬╬╖│╬╖╖╓╬╪│╓╣╣╣╣╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╒╣╣╖╗╣╣╣╗   ╣╣╣ ╣╣╣╣╣╣ ╟╣╣╖   ╣╣╣\n",
      " ╬╬╬╬┐  ╙╬╬╬╬│╓╣╣╣╝╜  ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╣╙ ╙╣╣╣  ╣╣╣ ╙╟╣╣╜╙  ╫╣╣  ╟╣╣\n",
      " ╬╬╬╬┐     ╙╬╬╣╣      ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣     ╣╣╣┌╣╣╜\n",
      " ╬╬╬╜       ╬╬╣╣      ╙╝╣╣╬      ╙╣╣╣╗╖╓╗╣╣╣╜ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣╦╓    ╣╣╣╣╣\n",
      " ╙   ╓╦╖    ╬╬╣╣   ╓╗╗╖            ╙╝╣╣╣╣╝╜   ╘╝╝╜   ╝╝╝  ╝╝╝   ╙╣╣╣    ╟╣╣╣\n",
      "   ╩╬╬╬╬╬╬╦╦╬╬╣╣╗╣╣╣╣╣╣╣╝                                             ╫╣╣╣╣\n",
      "      ╙╬╬╬╬╬╬╬╣╣╣╣╣╣╝╜\n",
      "          ╙╬╬╬╣╣╣╜\n",
      "             ╙\n",
      "        \n",
      " Version information:\n",
      "  ml-agents: 1.1.0,\n",
      "  ml-agents-envs: 1.1.0,\n",
      "  Communicator API: 1.5.0,\n",
      "  PyTorch: 2.8.0\n",
      "[INFO] Connected to Unity environment with package version 4.0.0 and communication version 1.5.0\n",
      "[INFO] Connected new brain: ArcadeDriver?team=0\n",
      "[INFO] Hyperparameters for behavior name ArcadeDriver: \n",
      "\ttrainer_type:\tppo\n",
      "\thyperparameters:\t\n",
      "\t  batch_size:\t128\n",
      "\t  buffer_size:\t1024\n",
      "\t  learning_rate:\t0.0003\n",
      "\t  beta:\t0.01\n",
      "\t  epsilon:\t0.2\n",
      "\t  lambd:\t0.95\n",
      "\t  num_epoch:\t3\n",
      "\t  shared_critic:\tFalse\n",
      "\t  learning_rate_schedule:\tlinear\n",
      "\t  beta_schedule:\tlinear\n",
      "\t  epsilon_schedule:\tlinear\n",
      "\tcheckpoint_interval:\t500000\n",
      "\tnetwork_settings:\t\n",
      "\t  normalize:\tTrue\n",
      "\t  hidden_units:\t256\n",
      "\t  num_layers:\t5\n",
      "\t  vis_encode_type:\tsimple\n",
      "\t  memory:\tNone\n",
      "\t  goal_conditioning_type:\thyper\n",
      "\t  deterministic:\tFalse\n",
      "\treward_signals:\t\n",
      "\t  extrinsic:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t1.0\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t128\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\tinit_path:\tNone\n",
      "\tkeep_checkpoints:\t5\n",
      "\teven_checkpoints:\tFalse\n",
      "\tmax_steps:\t500000\n",
      "\ttime_horizon:\t64\n",
      "\tsummary_freq:\t15000\n",
      "\tthreaded:\tFalse\n",
      "\tself_play:\tNone\n",
      "\tbehavioral_cloning:\tNone\n",
      "[INFO] ArcadeDriver. Step: 15000. Time Elapsed: 43.076 s. Mean Reward: -5.985. Std of Reward: 0.306. Training.\n",
      "[INFO] ArcadeDriver. Step: 30000. Time Elapsed: 81.606 s. Mean Reward: -5.970. Std of Reward: 0.136. Training.\n",
      "[INFO] ArcadeDriver. Step: 45000. Time Elapsed: 120.198 s. Mean Reward: -5.986. Std of Reward: 0.030. Training.\n",
      "[INFO] ArcadeDriver. Step: 60000. Time Elapsed: 159.172 s. Mean Reward: -5.987. Std of Reward: 0.025. Training.\n",
      "[INFO] ArcadeDriver. Step: 75000. Time Elapsed: 197.712 s. Mean Reward: -5.993. Std of Reward: 0.015. Training.\n",
      "[INFO] ArcadeDriver. Step: 90000. Time Elapsed: 236.546 s. Mean Reward: -5.995. Std of Reward: 0.012. Training.\n",
      "[INFO] ArcadeDriver. Step: 105000. Time Elapsed: 275.162 s. Mean Reward: -5.994. Std of Reward: 0.100. Training.\n",
      "[INFO] ArcadeDriver. Step: 120000. Time Elapsed: 314.037 s. Mean Reward: -6.001. Std of Reward: 0.013. Training.\n",
      "[INFO] ArcadeDriver. Step: 135000. Time Elapsed: 352.644 s. Mean Reward: -6.004. Std of Reward: 0.037. Training.\n",
      "[INFO] ArcadeDriver. Step: 150000. Time Elapsed: 391.260 s. Mean Reward: -6.008. Std of Reward: 0.019. Training.\n",
      "[INFO] ArcadeDriver. Step: 165000. Time Elapsed: 429.943 s. Mean Reward: -6.008. Std of Reward: 0.020. Training.\n",
      "[INFO] ArcadeDriver. Step: 180000. Time Elapsed: 468.617 s. Mean Reward: -6.008. Std of Reward: 0.021. Training.\n",
      "[INFO] ArcadeDriver. Step: 195000. Time Elapsed: 507.168 s. Mean Reward: -6.007. Std of Reward: 0.020. Training.\n",
      "[INFO] ArcadeDriver. Step: 210000. Time Elapsed: 545.683 s. Mean Reward: -6.006. Std of Reward: 0.015. Training.\n",
      "[INFO] ArcadeDriver. Step: 225000. Time Elapsed: 584.183 s. Mean Reward: -6.006. Std of Reward: 0.014. Training.\n",
      "[INFO] ArcadeDriver. Step: 240000. Time Elapsed: 622.849 s. Mean Reward: -6.005. Std of Reward: 0.015. Training.\n",
      "[INFO] ArcadeDriver. Step: 255000. Time Elapsed: 661.440 s. Mean Reward: -6.007. Std of Reward: 0.017. Training.\n",
      "[INFO] ArcadeDriver. Step: 270000. Time Elapsed: 700.072 s. Mean Reward: -6.007. Std of Reward: 0.017. Training.\n",
      "[INFO] ArcadeDriver. Step: 285000. Time Elapsed: 738.614 s. Mean Reward: -6.006. Std of Reward: 0.017. Training.\n",
      "[INFO] ArcadeDriver. Step: 300000. Time Elapsed: 777.363 s. Mean Reward: -6.008. Std of Reward: 0.018. Training.\n",
      "[INFO] ArcadeDriver. Step: 315000. Time Elapsed: 815.847 s. Mean Reward: -6.009. Std of Reward: 0.019. Training.\n",
      "[INFO] ArcadeDriver. Step: 330000. Time Elapsed: 854.612 s. Mean Reward: -6.008. Std of Reward: 0.018. Training.\n",
      "[INFO] ArcadeDriver. Step: 345000. Time Elapsed: 893.303 s. Mean Reward: -6.010. Std of Reward: 0.021. Training.\n",
      "[INFO] ArcadeDriver. Step: 360000. Time Elapsed: 932.245 s. Mean Reward: -6.009. Std of Reward: 0.018. Training.\n",
      "[INFO] ArcadeDriver. Step: 375000. Time Elapsed: 970.961 s. Mean Reward: -6.010. Std of Reward: 0.017. Training.\n",
      "[INFO] ArcadeDriver. Step: 390000. Time Elapsed: 1009.785 s. Mean Reward: -6.011. Std of Reward: 0.019. Training.\n",
      "[INFO] ArcadeDriver. Step: 405000. Time Elapsed: 1048.277 s. Mean Reward: -6.009. Std of Reward: 0.017. Training.\n",
      "[INFO] ArcadeDriver. Step: 420000. Time Elapsed: 1086.427 s. Mean Reward: -6.011. Std of Reward: 0.019. Training.\n",
      "[INFO] ArcadeDriver. Step: 435000. Time Elapsed: 1124.554 s. Mean Reward: -6.009. Std of Reward: 0.020. Training.\n",
      "[INFO] ArcadeDriver. Step: 450000. Time Elapsed: 1163.228 s. Mean Reward: -6.013. Std of Reward: 0.028. Training.\n",
      "[INFO] ArcadeDriver. Step: 465000. Time Elapsed: 1201.796 s. Mean Reward: -6.014. Std of Reward: 0.030. Training.\n",
      "[INFO] ArcadeDriver. Step: 480000. Time Elapsed: 1240.562 s. Mean Reward: -6.012. Std of Reward: 0.029. Training.\n",
      "[INFO] ArcadeDriver. Step: 495000. Time Elapsed: 1279.079 s. Mean Reward: -6.012. Std of Reward: 0.027. Training.\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_SAC/ArcadeDriver/ArcadeDriver-499984.onnx\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_SAC/ArcadeDriver/ArcadeDriver-500001.onnx\n",
      "[INFO] Copied /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_SAC/ArcadeDriver/ArcadeDriver-500001.onnx to /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_SAC/ArcadeDriver.onnx.\n"
     ]
    }
   ],
   "source": [
    "config_sac_fp = os.path.join(cur_dir, \"config\", \"Kart_sac.yaml\")\n",
    "run_sac_id = \"Kart_SAC\"\n",
    "print(config_ppo_fp)\n",
    "print(run_sac_id)\n",
    "\n",
    "!mlagents-learn $config_ppo_fp \\\n",
    "               --env=$env_fp \\\n",
    "               --results-dir=$output_dir \\\n",
    "               --run-id=$run_sac_id --base-port=1990"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40c13bd",
   "metadata": {},
   "source": [
    "## Training POCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "694c6520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/config/Kart_poca.yaml\n",
      "Kart_POCA\n",
      "\n",
      "            ┐  ╖\n",
      "        ╓╖╬│╡  ││╬╖╖\n",
      "    ╓╖╬│││││┘  ╬│││││╬╖\n",
      " ╖╬│││││╬╜        ╙╬│││││╖╖                               ╗╗╗\n",
      " ╬╬╬╬╖││╦╖        ╖╬││╗╣╣╣╬      ╟╣╣╬    ╟╣╣╣             ╜╜╜  ╟╣╣\n",
      " ╬╬╬╬╬╬╬╬╖│╬╖╖╓╬╪│╓╣╣╣╣╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╒╣╣╖╗╣╣╣╗   ╣╣╣ ╣╣╣╣╣╣ ╟╣╣╖   ╣╣╣\n",
      " ╬╬╬╬┐  ╙╬╬╬╬│╓╣╣╣╝╜  ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╣╙ ╙╣╣╣  ╣╣╣ ╙╟╣╣╜╙  ╫╣╣  ╟╣╣\n",
      " ╬╬╬╬┐     ╙╬╬╣╣      ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣     ╣╣╣┌╣╣╜\n",
      " ╬╬╬╜       ╬╬╣╣      ╙╝╣╣╬      ╙╣╣╣╗╖╓╗╣╣╣╜ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣╦╓    ╣╣╣╣╣\n",
      " ╙   ╓╦╖    ╬╬╣╣   ╓╗╗╖            ╙╝╣╣╣╣╝╜   ╘╝╝╜   ╝╝╝  ╝╝╝   ╙╣╣╣    ╟╣╣╣\n",
      "   ╩╬╬╬╬╬╬╦╦╬╬╣╣╗╣╣╣╣╣╣╣╝                                             ╫╣╣╣╣\n",
      "      ╙╬╬╬╬╬╬╬╣╣╣╣╣╣╝╜\n",
      "          ╙╬╬╬╣╣╣╜\n",
      "             ╙\n",
      "        \n",
      " Version information:\n",
      "  ml-agents: 1.1.0,\n",
      "  ml-agents-envs: 1.1.0,\n",
      "  Communicator API: 1.5.0,\n",
      "  PyTorch: 2.8.0\n",
      "[INFO] Connected to Unity environment with package version 4.0.0 and communication version 1.5.0\n",
      "[INFO] Connected new brain: ArcadeDriver?team=0\n",
      "[INFO] Hyperparameters for behavior name ArcadeDriver: \n",
      "\ttrainer_type:\tpoca\n",
      "\thyperparameters:\t\n",
      "\t  batch_size:\t128\n",
      "\t  buffer_size:\t1024\n",
      "\t  learning_rate:\t0.0003\n",
      "\t  beta:\t0.01\n",
      "\t  epsilon:\t0.2\n",
      "\t  lambd:\t0.95\n",
      "\t  num_epoch:\t3\n",
      "\t  learning_rate_schedule:\tlinear\n",
      "\t  beta_schedule:\tlinear\n",
      "\t  epsilon_schedule:\tlinear\n",
      "\tcheckpoint_interval:\t500000\n",
      "\tnetwork_settings:\t\n",
      "\t  normalize:\tTrue\n",
      "\t  hidden_units:\t256\n",
      "\t  num_layers:\t5\n",
      "\t  vis_encode_type:\tsimple\n",
      "\t  memory:\tNone\n",
      "\t  goal_conditioning_type:\thyper\n",
      "\t  deterministic:\tFalse\n",
      "\treward_signals:\t\n",
      "\t  extrinsic:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t1.0\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t128\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\tinit_path:\tNone\n",
      "\tkeep_checkpoints:\t5\n",
      "\teven_checkpoints:\tFalse\n",
      "\tmax_steps:\t500000\n",
      "\ttime_horizon:\t64\n",
      "\tsummary_freq:\t15000\n",
      "\tthreaded:\tFalse\n",
      "\tself_play:\tNone\n",
      "\tbehavioral_cloning:\tNone\n",
      "[INFO] ArcadeDriver. Step: 15000. Time Elapsed: 47.072 s. Mean Reward: -5.738. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 30000. Time Elapsed: 90.664 s. Mean Reward: -5.957. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 45000. Time Elapsed: 133.797 s. Mean Reward: -5.947. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 60000. Time Elapsed: 177.323 s. Mean Reward: -5.970. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 75000. Time Elapsed: 220.464 s. Mean Reward: -6.039. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 90000. Time Elapsed: 264.081 s. Mean Reward: -6.006. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 105000. Time Elapsed: 307.006 s. Mean Reward: -6.004. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 120000. Time Elapsed: 350.631 s. Mean Reward: -5.883. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 135000. Time Elapsed: 393.764 s. Mean Reward: -6.006. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 150000. Time Elapsed: 437.240 s. Mean Reward: -6.006. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 165000. Time Elapsed: 480.131 s. Mean Reward: -6.007. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 180000. Time Elapsed: 523.748 s. Mean Reward: -6.007. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 195000. Time Elapsed: 566.973 s. Mean Reward: -6.007. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 210000. Time Elapsed: 610.439 s. Mean Reward: -6.005. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 225000. Time Elapsed: 653.531 s. Mean Reward: -6.005. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 240000. Time Elapsed: 697.131 s. Mean Reward: -6.005. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 255000. Time Elapsed: 740.323 s. Mean Reward: -6.004. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 270000. Time Elapsed: 783.907 s. Mean Reward: -6.003. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 285000. Time Elapsed: 827.123 s. Mean Reward: -6.003. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 300000. Time Elapsed: 870.616 s. Mean Reward: -6.003. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 315000. Time Elapsed: 913.773 s. Mean Reward: -6.003. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 330000. Time Elapsed: 957.406 s. Mean Reward: -6.003. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 345000. Time Elapsed: 1000.706 s. Mean Reward: -6.003. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 360000. Time Elapsed: 1044.015 s. Mean Reward: -6.002. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 375000. Time Elapsed: 1087.199 s. Mean Reward: -6.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 390000. Time Elapsed: 1130.824 s. Mean Reward: -6.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 405000. Time Elapsed: 1174.165 s. Mean Reward: -6.001. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 420000. Time Elapsed: 1217.874 s. Mean Reward: -6.001. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 435000. Time Elapsed: 1261.198 s. Mean Reward: -5.999. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 450000. Time Elapsed: 1304.840 s. Mean Reward: -6.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 465000. Time Elapsed: 1348.099 s. Mean Reward: -6.002. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 480000. Time Elapsed: 1391.757 s. Mean Reward: -6.000. Mean Group Reward: 0.000. Training.\n",
      "[INFO] ArcadeDriver. Step: 495000. Time Elapsed: 1435.507 s. Mean Reward: -5.999. Mean Group Reward: 0.000. Training.\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_POCA/ArcadeDriver/ArcadeDriver-499984.onnx\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_POCA/ArcadeDriver/ArcadeDriver-500001.onnx\n",
      "[INFO] Copied /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_POCA/ArcadeDriver/ArcadeDriver-500001.onnx to /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Kart_POCA/ArcadeDriver.onnx.\n"
     ]
    }
   ],
   "source": [
    "config_poca_fp = os.path.join(cur_dir, \"config\", \"Kart_poca.yaml\")\n",
    "run_poca_id = \"Kart_POCA\"\n",
    "print(config_poca_fp)\n",
    "print(run_poca_id)\n",
    "\n",
    "!mlagents-learn $config_poca_fp \\\n",
    "               --env=$env_fp \\\n",
    "               --results-dir=$output_dir \\\n",
    "               --run-id=$run_poca_id --base-port=1990"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad33878",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f5c2e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mlagents-learn $config_fp \\\n",
    "#                --env=$env_fp \\\n",
    "#                --results-dir=$test_dir \\\n",
    "#                --run-id=$run_id \\\n",
    "#                --resume --inference\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
