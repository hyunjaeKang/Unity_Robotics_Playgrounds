{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70a95c2c",
   "metadata": {},
   "source": [
    "# Kart_BC_GAIL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8c6c95",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8ba6fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "\n",
    "# Global Setting\n",
    "cur_dir = os.getcwd()\n",
    "env_dir = os.path.abspath(os.path.join(cur_dir, \"..\", \"Unity6000_Envs\"))\n",
    "output_dir = os.path.abspath(os.path.join(cur_dir, \"temp\", \"mlagents_learn_output\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a62e191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Unity6000_Envs/Pong_Darwin.app\n"
     ]
    }
   ],
   "source": [
    "# Unity Enviroment\n",
    "game = \"Pong\"\n",
    "os_name = platform.system()\n",
    "\n",
    "if os_name == 'Linux':\n",
    "    env_name = os.path.join(env_dir, f\"{game}_{os_name}.x86_64\")\n",
    "elif os_name == 'Darwin':\n",
    "    env_name = os.path.join(env_dir, f\"{game}_{os_name}.app\")\n",
    "env_fp = os.path.join(env_dir, env_name)\n",
    "print(env_fp)\n",
    "baseport = 1992"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9e227b",
   "metadata": {},
   "source": [
    "## Training PPO-PPO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb8ef05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/config/Pong_adversarial_ppo_ppo.yaml\n",
      "Pong_Adv_PPO_PPO\n",
      "\n",
      "            ┐  ╖\n",
      "        ╓╖╬│╡  ││╬╖╖\n",
      "    ╓╖╬│││││┘  ╬│││││╬╖\n",
      " ╖╬│││││╬╜        ╙╬│││││╖╖                               ╗╗╗\n",
      " ╬╬╬╬╖││╦╖        ╖╬││╗╣╣╣╬      ╟╣╣╬    ╟╣╣╣             ╜╜╜  ╟╣╣\n",
      " ╬╬╬╬╬╬╬╬╖│╬╖╖╓╬╪│╓╣╣╣╣╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╒╣╣╖╗╣╣╣╗   ╣╣╣ ╣╣╣╣╣╣ ╟╣╣╖   ╣╣╣\n",
      " ╬╬╬╬┐  ╙╬╬╬╬│╓╣╣╣╝╜  ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╣╙ ╙╣╣╣  ╣╣╣ ╙╟╣╣╜╙  ╫╣╣  ╟╣╣\n",
      " ╬╬╬╬┐     ╙╬╬╣╣      ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣     ╣╣╣┌╣╣╜\n",
      " ╬╬╬╜       ╬╬╣╣      ╙╝╣╣╬      ╙╣╣╣╗╖╓╗╣╣╣╜ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣╦╓    ╣╣╣╣╣\n",
      " ╙   ╓╦╖    ╬╬╣╣   ╓╗╗╖            ╙╝╣╣╣╣╝╜   ╘╝╝╜   ╝╝╝  ╝╝╝   ╙╣╣╣    ╟╣╣╣\n",
      "   ╩╬╬╬╬╬╬╦╦╬╬╣╣╗╣╣╣╣╣╣╣╝                                             ╫╣╣╣╣\n",
      "      ╙╬╬╬╬╬╬╬╣╣╣╣╣╣╝╜\n",
      "          ╙╬╬╬╣╣╣╜\n",
      "             ╙\n",
      "        \n",
      " Version information:\n",
      "  ml-agents: 1.1.0,\n",
      "  ml-agents-envs: 1.1.0,\n",
      "  Communicator API: 1.5.0,\n",
      "  PyTorch: 2.8.0\n",
      "[INFO] Connected to Unity environment with package version 4.0.0 and communication version 1.5.0\n",
      "[INFO] Connected new brain: Behavior_B?team=1\n",
      "[INFO] Connected new brain: Behavior_A?team=0\n",
      "[INFO] Hyperparameters for behavior name Behavior_B: \n",
      "\ttrainer_type:\tppo\n",
      "\thyperparameters:\t\n",
      "\t  batch_size:\t128\n",
      "\t  buffer_size:\t1280\n",
      "\t  learning_rate:\t0.0003\n",
      "\t  beta:\t0.005\n",
      "\t  epsilon:\t0.2\n",
      "\t  lambd:\t0.95\n",
      "\t  num_epoch:\t3\n",
      "\t  shared_critic:\tFalse\n",
      "\t  learning_rate_schedule:\tlinear\n",
      "\t  beta_schedule:\tlinear\n",
      "\t  epsilon_schedule:\tlinear\n",
      "\tcheckpoint_interval:\t500000\n",
      "\tnetwork_settings:\t\n",
      "\t  normalize:\tTrue\n",
      "\t  hidden_units:\t128\n",
      "\t  num_layers:\t2\n",
      "\t  vis_encode_type:\tsimple\n",
      "\t  memory:\tNone\n",
      "\t  goal_conditioning_type:\thyper\n",
      "\t  deterministic:\tFalse\n",
      "\treward_signals:\t\n",
      "\t  extrinsic:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t1.0\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t128\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\tinit_path:\tNone\n",
      "\tkeep_checkpoints:\t5\n",
      "\teven_checkpoints:\tFalse\n",
      "\tmax_steps:\t1000000\n",
      "\ttime_horizon:\t1000\n",
      "\tsummary_freq:\t20000\n",
      "\tthreaded:\tFalse\n",
      "\tself_play:\t\n",
      "\t  save_steps:\t10000\n",
      "\t  team_change:\t40000\n",
      "\t  swap_steps:\t10000\n",
      "\t  window:\t10\n",
      "\t  play_against_latest_model_ratio:\t0.5\n",
      "\t  initial_elo:\t1200.0\n",
      "\tbehavioral_cloning:\tNone\n",
      "[INFO] Hyperparameters for behavior name Behavior_A: \n",
      "\ttrainer_type:\tppo\n",
      "\thyperparameters:\t\n",
      "\t  batch_size:\t128\n",
      "\t  buffer_size:\t1280\n",
      "\t  learning_rate:\t0.0003\n",
      "\t  beta:\t0.005\n",
      "\t  epsilon:\t0.2\n",
      "\t  lambd:\t0.95\n",
      "\t  num_epoch:\t3\n",
      "\t  shared_critic:\tFalse\n",
      "\t  learning_rate_schedule:\tlinear\n",
      "\t  beta_schedule:\tlinear\n",
      "\t  epsilon_schedule:\tlinear\n",
      "\tcheckpoint_interval:\t500000\n",
      "\tnetwork_settings:\t\n",
      "\t  normalize:\tTrue\n",
      "\t  hidden_units:\t128\n",
      "\t  num_layers:\t2\n",
      "\t  vis_encode_type:\tsimple\n",
      "\t  memory:\tNone\n",
      "\t  goal_conditioning_type:\thyper\n",
      "\t  deterministic:\tFalse\n",
      "\treward_signals:\t\n",
      "\t  extrinsic:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t1.0\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t128\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\tinit_path:\tNone\n",
      "\tkeep_checkpoints:\t5\n",
      "\teven_checkpoints:\tFalse\n",
      "\tmax_steps:\t1000000\n",
      "\ttime_horizon:\t1000\n",
      "\tsummary_freq:\t20000\n",
      "\tthreaded:\tFalse\n",
      "\tself_play:\t\n",
      "\t  save_steps:\t10000\n",
      "\t  team_change:\t40000\n",
      "\t  swap_steps:\t10000\n",
      "\t  window:\t10\n",
      "\t  play_against_latest_model_ratio:\t0.5\n",
      "\t  initial_elo:\t1200.0\n",
      "\tbehavioral_cloning:\tNone\n",
      "[INFO] Behavior_B. Step: 20000. Time Elapsed: 59.283 s. Mean Reward: 0.136. Std of Reward: 1.089. Training. ELO: 1202.686.\n",
      "[INFO] Behavior_B. Step: 40000. Time Elapsed: 112.731 s. Mean Reward: 0.387. Std of Reward: 1.072. Training. ELO: 1214.158.\n",
      "[INFO] Behavior_A. Step: 20000. Time Elapsed: 166.191 s. Mean Reward: -0.165. Std of Reward: 1.006. Training. ELO: 1169.048.\n",
      "[INFO] Behavior_A. Step: 40000. Time Elapsed: 219.424 s. Mean Reward: 0.233. Std of Reward: 1.097. Training. ELO: 1175.662.\n",
      "[INFO] Behavior_B. Step: 60000. Time Elapsed: 274.109 s. Mean Reward: 0.693. Std of Reward: 1.074. Training. ELO: 1243.368.\n",
      "[INFO] Behavior_B. Step: 80000. Time Elapsed: 324.918 s. Mean Reward: 0.961. Std of Reward: 0.979. Training. ELO: 1268.413.\n",
      "[INFO] Behavior_A. Step: 60000. Time Elapsed: 380.229 s. Mean Reward: 0.525. Std of Reward: 1.150. Training. ELO: 1170.705.\n",
      "[INFO] Behavior_A. Step: 80000. Time Elapsed: 430.933 s. Mean Reward: 0.849. Std of Reward: 0.971. Training. ELO: 1195.385.\n",
      "[INFO] Behavior_B. Step: 100000. Time Elapsed: 483.986 s. Mean Reward: 1.126. Std of Reward: 1.123. Training. ELO: 1290.968.\n",
      "[INFO] Behavior_B. Step: 120000. Time Elapsed: 536.324 s. Mean Reward: 2.107. Std of Reward: 2.152. Training. ELO: 1295.018.\n",
      "[INFO] Behavior_A. Step: 100000. Time Elapsed: 592.521 s. Mean Reward: 1.324. Std of Reward: 1.932. Training. ELO: 1219.021.\n",
      "[INFO] Behavior_A. Step: 120000. Time Elapsed: 644.444 s. Mean Reward: 2.029. Std of Reward: 3.336. Training. ELO: 1216.080.\n",
      "[INFO] Behavior_B. Step: 140000. Time Elapsed: 695.917 s. Mean Reward: 1.700. Std of Reward: 1.771. Training. ELO: 1302.330.\n",
      "[INFO] Behavior_B. Step: 160000. Time Elapsed: 746.956 s. Mean Reward: 2.643. Std of Reward: 3.317. Training. ELO: 1304.126.\n",
      "[INFO] Behavior_A. Step: 140000. Time Elapsed: 803.132 s. Mean Reward: 1.750. Std of Reward: 2.278. Training. ELO: 1219.629.\n",
      "[INFO] Behavior_A. Step: 160000. Time Elapsed: 857.111 s. Mean Reward: 1.481. Std of Reward: 2.930. Training. ELO: 1218.864.\n",
      "[INFO] Behavior_B. Step: 180000. Time Elapsed: 906.125 s. Mean Reward: 1.979. Std of Reward: 2.224. Training. ELO: 1302.970.\n",
      "[INFO] Behavior_B. Step: 200000. Time Elapsed: 959.331 s. Mean Reward: 2.528. Std of Reward: 4.033. Training. ELO: 1300.039.\n",
      "[INFO] Behavior_A. Step: 180000. Time Elapsed: 1015.482 s. Mean Reward: 2.000. Std of Reward: 2.673. Training. ELO: 1221.504.\n",
      "[INFO] Behavior_A. Step: 200000. Time Elapsed: 1069.098 s. Mean Reward: 2.912. Std of Reward: 3.727. Training. ELO: 1220.919.\n",
      "[INFO] Behavior_B. Step: 220000. Time Elapsed: 1117.291 s. Mean Reward: 2.435. Std of Reward: 2.482. Training. ELO: 1300.949.\n",
      "[INFO] Behavior_B. Step: 240000. Time Elapsed: 1170.696 s. Mean Reward: 2.321. Std of Reward: 1.877. Training. ELO: 1302.753.\n",
      "[INFO] Behavior_A. Step: 220000. Time Elapsed: 1228.387 s. Mean Reward: 3.731. Std of Reward: 4.410. Training. ELO: 1221.251.\n",
      "[INFO] Behavior_A. Step: 240000. Time Elapsed: 1281.603 s. Mean Reward: 2.667. Std of Reward: 3.236. Training. ELO: 1220.954.\n",
      "[INFO] Behavior_B. Step: 260000. Time Elapsed: 1328.148 s. Mean Reward: 4.708. Std of Reward: 3.755. Training. ELO: 1303.272.\n",
      "[INFO] Behavior_B. Step: 280000. Time Elapsed: 1381.289 s. Mean Reward: 2.737. Std of Reward: 3.481. Training. ELO: 1302.949.\n",
      "[INFO] Behavior_A. Step: 260000. Time Elapsed: 1442.337 s. Mean Reward: 5.062. Std of Reward: 3.737. Training. ELO: 1224.195.\n",
      "[INFO] Behavior_A. Step: 280000. Time Elapsed: 1493.124 s. Mean Reward: 3.533. Std of Reward: 4.533. Training. ELO: 1224.347.\n",
      "[INFO] Behavior_B. Step: 300000. Time Elapsed: 1542.258 s. Mean Reward: 4.000. Std of Reward: 4.231. Training. ELO: 1301.045.\n",
      "[INFO] Behavior_B. Step: 320000. Time Elapsed: 1593.494 s. Mean Reward: 7.429. Std of Reward: 4.799. Training. ELO: 1300.294.\n",
      "[INFO] Behavior_A. Step: 300000. Time Elapsed: 1650.975 s. Mean Reward: 4.450. Std of Reward: 4.280. Training. ELO: 1226.083.\n",
      "[INFO] Behavior_A. Step: 320000. Time Elapsed: 1703.556 s. Mean Reward: 3.571. Std of Reward: 3.350. Training. ELO: 1233.315.\n",
      "[INFO] Behavior_B. Step: 340000. Time Elapsed: 1752.214 s. Mean Reward: 2.619. Std of Reward: 3.151. Training. ELO: 1292.130.\n",
      "[INFO] Behavior_B. Step: 360000. Time Elapsed: 1803.701 s. Mean Reward: 2.114. Std of Reward: 3.421. Training. ELO: 1287.218.\n",
      "[INFO] Behavior_A. Step: 340000. Time Elapsed: 1861.690 s. Mean Reward: 2.620. Std of Reward: 2.132. Training. ELO: 1251.163.\n",
      "[INFO] Behavior_A. Step: 360000. Time Elapsed: 1914.362 s. Mean Reward: 2.214. Std of Reward: 1.815. Training. ELO: 1270.837.\n",
      "[INFO] Behavior_B. Step: 380000. Time Elapsed: 1962.674 s. Mean Reward: 0.177. Std of Reward: 1.519. Training. ELO: 1259.691.\n",
      "[INFO] Behavior_B. Step: 400000. Time Elapsed: 2016.918 s. Mean Reward: 0.052. Std of Reward: 1.397. Training. ELO: 1240.854.\n",
      "[INFO] Behavior_A. Step: 380000. Time Elapsed: 2072.831 s. Mean Reward: 1.724. Std of Reward: 0.869. Training. ELO: 1323.091.\n",
      "[INFO] Behavior_A. Step: 400000. Time Elapsed: 2125.767 s. Mean Reward: 1.836. Std of Reward: 0.857. Training. ELO: 1348.685.\n",
      "[INFO] Behavior_B. Step: 420000. Time Elapsed: 2176.177 s. Mean Reward: 1.659. Std of Reward: 3.877. Training. ELO: 1228.402.\n",
      "[INFO] Behavior_B. Step: 440000. Time Elapsed: 2229.287 s. Mean Reward: 2.200. Std of Reward: 3.878. Training. ELO: 1224.807.\n",
      "[INFO] Behavior_A. Step: 420000. Time Elapsed: 2284.565 s. Mean Reward: 1.933. Std of Reward: 2.318. Training. ELO: 1366.865.\n",
      "[INFO] Behavior_A. Step: 440000. Time Elapsed: 2337.557 s. Mean Reward: 5.150. Std of Reward: 3.988. Training. ELO: 1371.377.\n",
      "[INFO] Behavior_B. Step: 460000. Time Elapsed: 2386.688 s. Mean Reward: 4.773. Std of Reward: 4.807. Training. ELO: 1226.612.\n",
      "[INFO] Behavior_B. Step: 480000. Time Elapsed: 2440.084 s. Mean Reward: 2.147. Std of Reward: 3.416. Training. ELO: 1225.405.\n",
      "[INFO] Behavior_A. Step: 460000. Time Elapsed: 2495.567 s. Mean Reward: 1.769. Std of Reward: 1.288. Training. ELO: 1382.604.\n",
      "[INFO] Behavior_A. Step: 480000. Time Elapsed: 2550.281 s. Mean Reward: 2.792. Std of Reward: 3.705. Training. ELO: 1390.329.\n",
      "[INFO] Behavior_B. Step: 500000. Time Elapsed: 2597.024 s. Mean Reward: 6.000. Std of Reward: 4.637. Training. ELO: 1224.564.\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_PPO_PPO/Behavior_B/Behavior_B-499071.onnx\n",
      "[INFO] Behavior_B. Step: 520000. Time Elapsed: 2651.114 s. Mean Reward: 4.450. Std of Reward: 5.150. Training. ELO: 1224.629.\n",
      "[INFO] Behavior_A. Step: 500000. Time Elapsed: 2706.734 s. Mean Reward: 9.750. Std of Reward: 3.065. Training. ELO: 1391.784.\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_PPO_PPO/Behavior_A/Behavior_A-499251.onnx\n",
      "[INFO] Behavior_A. Step: 520000. Time Elapsed: 2760.679 s. Mean Reward: 3.292. Std of Reward: 3.443. Training. ELO: 1388.902.\n",
      "[INFO] Behavior_B. Step: 540000. Time Elapsed: 2810.759 s. Mean Reward: 4.643. Std of Reward: 3.210. Training. ELO: 1230.312.\n",
      "[INFO] Behavior_B. Step: 560000. Time Elapsed: 2863.287 s. Mean Reward: 3.600. Std of Reward: 3.980. Training. ELO: 1236.642.\n",
      "[INFO] Behavior_A. Step: 540000. Time Elapsed: 2918.167 s. Mean Reward: 6.500. Std of Reward: 5.025. Training. ELO: 1385.853.\n",
      "[INFO] Behavior_A. Step: 560000. Time Elapsed: 2971.167 s. Mean Reward: 2.306. Std of Reward: 3.441. Training. ELO: 1380.684.\n",
      "[INFO] Behavior_B. Step: 580000. Time Elapsed: 3022.241 s. Mean Reward: 3.250. Std of Reward: 2.969. Training. ELO: 1242.663.\n",
      "[INFO] Behavior_B. Step: 600000. Time Elapsed: 3074.879 s. Mean Reward: 2.286. Std of Reward: 3.487. Training. ELO: 1245.539.\n",
      "[INFO] Behavior_A. Step: 580000. Time Elapsed: 3131.602 s. Mean Reward: 4.900. Std of Reward: 4.271. Training. ELO: 1378.390.\n",
      "[INFO] Behavior_A. Step: 600000. Time Elapsed: 3184.538 s. Mean Reward: 5.100. Std of Reward: 5.481. Training. ELO: 1377.765.\n",
      "[INFO] Behavior_B. Step: 620000. Time Elapsed: 3234.025 s. Mean Reward: 7.200. Std of Reward: 2.804. Training. ELO: 1248.829.\n",
      "[INFO] Behavior_B. Step: 640000. Time Elapsed: 3287.378 s. Mean Reward: 5.278. Std of Reward: 4.001. Training. ELO: 1250.740.\n",
      "[INFO] Behavior_A. Step: 620000. Time Elapsed: 3343.121 s. Mean Reward: 3.615. Std of Reward: 3.139. Training. ELO: 1376.042.\n",
      "[INFO] Behavior_A. Step: 640000. Time Elapsed: 3398.190 s. Mean Reward: 6.556. Std of Reward: 5.014. Training. ELO: 1373.271.\n",
      "[INFO] Behavior_B. Step: 660000. Time Elapsed: 3445.353 s. Mean Reward: 5.278. Std of Reward: 4.883. Training. ELO: 1251.050.\n",
      "[INFO] Behavior_B. Step: 680000. Time Elapsed: 3499.300 s. Mean Reward: 6.688. Std of Reward: 5.068. Training. ELO: 1249.497.\n",
      "[INFO] Behavior_A. Step: 660000. Time Elapsed: 3554.710 s. Mean Reward: 5.357. Std of Reward: 4.794. Training. ELO: 1372.299.\n",
      "[INFO] Behavior_A. Step: 680000. Time Elapsed: 3607.475 s. Mean Reward: 5.455. Std of Reward: 4.741. Training. ELO: 1374.396.\n",
      "[INFO] Behavior_B. Step: 700000. Time Elapsed: 3655.742 s. Mean Reward: 4.600. Std of Reward: 3.700. Training. ELO: 1250.119.\n",
      "[INFO] Behavior_B. Step: 720000. Time Elapsed: 3708.669 s. Mean Reward: 6.056. Std of Reward: 5.241. Training. ELO: 1253.538.\n",
      "[INFO] Behavior_A. Step: 700000. Time Elapsed: 3765.764 s. Mean Reward: 8.000. Std of Reward: 3.033. Training. ELO: 1371.666.\n",
      "[INFO] Behavior_A. Step: 720000. Time Elapsed: 3818.633 s. Mean Reward: 10.200. Std of Reward: 4.697. Training. ELO: 1372.332.\n",
      "[INFO] Behavior_B. Step: 740000. Time Elapsed: 3865.255 s. Mean Reward: 5.611. Std of Reward: 5.136. Training. ELO: 1255.816.\n",
      "[INFO] Behavior_B. Step: 760000. Time Elapsed: 3917.770 s. Mean Reward: 4.000. Std of Reward: 5.273. Training. ELO: 1257.493.\n",
      "[INFO] Behavior_A. Step: 740000. Time Elapsed: 3976.595 s. Mean Reward: 4.600. Std of Reward: 4.867. Training. ELO: 1369.218.\n",
      "[INFO] Behavior_A. Step: 760000. Time Elapsed: 4029.336 s. Mean Reward: 6.562. Std of Reward: 4.700. Training. ELO: 1367.980.\n",
      "[INFO] Behavior_B. Step: 780000. Time Elapsed: 4076.662 s. Mean Reward: 7.857. Std of Reward: 4.681. Training. ELO: 1257.578.\n",
      "[INFO] Behavior_B. Step: 800000. Time Elapsed: 4129.969 s. Mean Reward: 6.875. Std of Reward: 4.233. Training. ELO: 1260.507.\n",
      "[INFO] Behavior_A. Step: 780000. Time Elapsed: 4188.736 s. Mean Reward: 2.654. Std of Reward: 4.618. Training. ELO: 1364.047.\n",
      "[INFO] Behavior_A. Step: 800000. Time Elapsed: 4240.577 s. Mean Reward: 9.750. Std of Reward: 2.512. Training. ELO: 1361.994.\n",
      "[INFO] Behavior_B. Step: 820000. Time Elapsed: 4286.892 s. Mean Reward: 7.000. Std of Reward: 6.107. Training. ELO: 1262.634.\n",
      "[INFO] Behavior_B. Step: 840000. Time Elapsed: 4338.514 s. Mean Reward: 4.615. Std of Reward: 4.288. Training. ELO: 1265.102.\n",
      "[INFO] Behavior_A. Step: 820000. Time Elapsed: 4400.478 s. Mean Reward: 7.444. Std of Reward: 3.797. Training. ELO: 1361.979.\n",
      "[INFO] Behavior_A. Step: 840000. Time Elapsed: 4452.238 s. Mean Reward: 5.091. Std of Reward: 5.861. Training. ELO: 1362.845.\n",
      "[INFO] Behavior_B. Step: 860000. Time Elapsed: 4499.056 s. Mean Reward: 6.250. Std of Reward: 5.585. Training. ELO: 1268.267.\n",
      "[INFO] Behavior_B. Step: 880000. Time Elapsed: 4551.525 s. Mean Reward: 7.111. Std of Reward: 6.432. Training. ELO: 1270.391.\n",
      "[INFO] Behavior_A. Step: 860000. Time Elapsed: 4612.765 s. Mean Reward: 4.591. Std of Reward: 4.471. Training. ELO: 1361.757.\n",
      "[INFO] Behavior_A. Step: 880000. Time Elapsed: 4663.739 s. Mean Reward: 10.875. Std of Reward: 4.840. Training. ELO: 1361.618.\n",
      "[INFO] Behavior_B. Step: 900000. Time Elapsed: 4709.203 s. Mean Reward: 1.833. Std of Reward: 3.239. Training. ELO: 1271.343.\n",
      "[INFO] Behavior_B. Step: 920000. Time Elapsed: 4761.777 s. Mean Reward: 4.654. Std of Reward: 5.336. Training. ELO: 1272.182.\n",
      "[INFO] Behavior_A. Step: 900000. Time Elapsed: 4822.073 s. Mean Reward: 13.000. Std of Reward: 1.904. Training.\n",
      "[INFO] Behavior_A. Step: 920000. Time Elapsed: 4876.425 s. Mean Reward: 3.875. Std of Reward: 4.891. Training. ELO: 1359.452.\n",
      "[INFO] Behavior_B. Step: 940000. Time Elapsed: 4921.232 s. Mean Reward: 5.556. Std of Reward: 4.997. Training. ELO: 1273.363.\n",
      "[INFO] Behavior_B. Step: 960000. Time Elapsed: 4973.699 s. Mean Reward: 10.100. Std of Reward: 3.499. Training. ELO: 1276.418.\n",
      "[INFO] Behavior_A. Step: 940000. Time Elapsed: 5034.105 s. Mean Reward: 4.071. Std of Reward: 4.547. Training. ELO: 1357.339.\n",
      "[INFO] Behavior_A. Step: 960000. Time Elapsed: 5087.441 s. Mean Reward: 8.833. Std of Reward: 6.018. Training. ELO: 1356.608.\n",
      "[INFO] Behavior_B. Step: 980000. Time Elapsed: 5131.777 s. Mean Reward: 8.500. Std of Reward: 5.736. Training. ELO: 1277.144.\n",
      "[INFO] Behavior_B. Step: 1000000. Time Elapsed: 5185.793 s. Mean Reward: 6.500. Std of Reward: 5.239. Training. ELO: 1278.349.\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_PPO_PPO/Behavior_B/Behavior_B-999868.onnx\n",
      "[INFO] Behavior_A. Step: 980000. Time Elapsed: 5246.932 s. Mean Reward: 7.000. Std of Reward: 5.831. Training. ELO: 1355.294.\n",
      "[INFO] Behavior_A. Step: 1000000. Time Elapsed: 5297.768 s. Mean Reward: 7.500. Std of Reward: 5.510. Training. ELO: 1355.857.\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_PPO_PPO/Behavior_A/Behavior_A-999158.onnx\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_PPO_PPO/Behavior_B/Behavior_B-1012039.onnx\n",
      "[INFO] Copied /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_PPO_PPO/Behavior_B/Behavior_B-1012039.onnx to /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_PPO_PPO/Behavior_B.onnx.\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_PPO_PPO/Behavior_A/Behavior_A-1000158.onnx\n",
      "[INFO] Copied /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_PPO_PPO/Behavior_A/Behavior_A-1000158.onnx to /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_PPO_PPO/Behavior_A.onnx.\n"
     ]
    }
   ],
   "source": [
    "config_ppo_fp = os.path.join(cur_dir, \"config\", \"Pong_adversarial_ppo_ppo.yaml\")\n",
    "run_ppo_id = \"Pong_Adv_PPO_PPO\"\n",
    "print(config_ppo_fp)\n",
    "print(run_ppo_id)\n",
    "\n",
    "!mlagents-learn $config_ppo_fp \\\n",
    "               --env=$env_fp \\\n",
    "               --results-dir=$output_dir \\\n",
    "               --run-id=$run_ppo_id --base-port=$baseport"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17da064",
   "metadata": {},
   "source": [
    "### Testing PP0-PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a4a2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            ┐  ╖\n",
      "        ╓╖╬│╡  ││╬╖╖\n",
      "    ╓╖╬│││││┘  ╬│││││╬╖\n",
      " ╖╬│││││╬╜        ╙╬│││││╖╖                               ╗╗╗\n",
      " ╬╬╬╬╖││╦╖        ╖╬││╗╣╣╣╬      ╟╣╣╬    ╟╣╣╣             ╜╜╜  ╟╣╣\n",
      " ╬╬╬╬╬╬╬╬╖│╬╖╖╓╬╪│╓╣╣╣╣╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╒╣╣╖╗╣╣╣╗   ╣╣╣ ╣╣╣╣╣╣ ╟╣╣╖   ╣╣╣\n",
      " ╬╬╬╬┐  ╙╬╬╬╬│╓╣╣╣╝╜  ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╣╙ ╙╣╣╣  ╣╣╣ ╙╟╣╣╜╙  ╫╣╣  ╟╣╣\n",
      " ╬╬╬╬┐     ╙╬╬╣╣      ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣     ╣╣╣┌╣╣╜\n",
      " ╬╬╬╜       ╬╬╣╣      ╙╝╣╣╬      ╙╣╣╣╗╖╓╗╣╣╣╜ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣╦╓    ╣╣╣╣╣\n",
      " ╙   ╓╦╖    ╬╬╣╣   ╓╗╗╖            ╙╝╣╣╣╣╝╜   ╘╝╝╜   ╝╝╝  ╝╝╝   ╙╣╣╣    ╟╣╣╣\n",
      "   ╩╬╬╬╬╬╬╦╦╬╬╣╣╗╣╣╣╣╣╣╣╝                                             ╫╣╣╣╣\n",
      "      ╙╬╬╬╬╬╬╬╣╣╣╣╣╣╝╜\n",
      "          ╙╬╬╬╣╣╣╜\n",
      "             ╙\n",
      "        \n",
      " Version information:\n",
      "  ml-agents: 1.1.0,\n",
      "  ml-agents-envs: 1.1.0,\n",
      "  Communicator API: 1.5.0,\n",
      "  PyTorch: 2.8.0\n",
      "[INFO] Connected to Unity environment with package version 4.0.0 and communication version 1.5.0\n",
      "[INFO] Connected new brain: Behavior_B?team=1\n",
      "[INFO] Connected new brain: Behavior_A?team=0\n",
      "[INFO] Hyperparameters for behavior name Behavior_B: \n",
      "\ttrainer_type:\tppo\n",
      "\thyperparameters:\t\n",
      "\t  batch_size:\t128\n",
      "\t  buffer_size:\t1280\n",
      "\t  learning_rate:\t0.0003\n",
      "\t  beta:\t0.005\n",
      "\t  epsilon:\t0.2\n",
      "\t  lambd:\t0.95\n",
      "\t  num_epoch:\t3\n",
      "\t  shared_critic:\tFalse\n",
      "\t  learning_rate_schedule:\tlinear\n",
      "\t  beta_schedule:\tlinear\n",
      "\t  epsilon_schedule:\tlinear\n",
      "\tcheckpoint_interval:\t500000\n",
      "\tnetwork_settings:\t\n",
      "\t  normalize:\tTrue\n",
      "\t  hidden_units:\t128\n",
      "\t  num_layers:\t2\n",
      "\t  vis_encode_type:\tsimple\n",
      "\t  memory:\tNone\n",
      "\t  goal_conditioning_type:\thyper\n",
      "\t  deterministic:\tFalse\n",
      "\treward_signals:\t\n",
      "\t  extrinsic:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t1.0\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t128\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\tinit_path:\tNone\n",
      "\tkeep_checkpoints:\t5\n",
      "\teven_checkpoints:\tFalse\n",
      "\tmax_steps:\t1000000\n",
      "\ttime_horizon:\t1000\n",
      "\tsummary_freq:\t20000\n",
      "\tthreaded:\tFalse\n",
      "\tself_play:\t\n",
      "\t  save_steps:\t10000\n",
      "\t  team_change:\t40000\n",
      "\t  swap_steps:\t10000\n",
      "\t  window:\t10\n",
      "\t  play_against_latest_model_ratio:\t0.5\n",
      "\t  initial_elo:\t1200.0\n",
      "\tbehavioral_cloning:\tNone\n",
      "[INFO] Resuming from /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_PPO_PPO/Behavior_B.\n",
      "[INFO] Resuming training from step 1012039.\n",
      "[INFO] Hyperparameters for behavior name Behavior_A: \n",
      "\ttrainer_type:\tppo\n",
      "\thyperparameters:\t\n",
      "\t  batch_size:\t128\n",
      "\t  buffer_size:\t1280\n",
      "\t  learning_rate:\t0.0003\n",
      "\t  beta:\t0.005\n",
      "\t  epsilon:\t0.2\n",
      "\t  lambd:\t0.95\n",
      "\t  num_epoch:\t3\n",
      "\t  shared_critic:\tFalse\n",
      "\t  learning_rate_schedule:\tlinear\n",
      "\t  beta_schedule:\tlinear\n",
      "\t  epsilon_schedule:\tlinear\n",
      "\tcheckpoint_interval:\t500000\n",
      "\tnetwork_settings:\t\n",
      "\t  normalize:\tTrue\n",
      "\t  hidden_units:\t128\n",
      "\t  num_layers:\t2\n",
      "\t  vis_encode_type:\tsimple\n",
      "\t  memory:\tNone\n",
      "\t  goal_conditioning_type:\thyper\n",
      "\t  deterministic:\tFalse\n",
      "\treward_signals:\t\n",
      "\t  extrinsic:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t1.0\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t128\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\tinit_path:\tNone\n",
      "\tkeep_checkpoints:\t5\n",
      "\teven_checkpoints:\tFalse\n",
      "\tmax_steps:\t1000000\n",
      "\ttime_horizon:\t1000\n",
      "\tsummary_freq:\t20000\n",
      "\tthreaded:\tFalse\n",
      "\tself_play:\t\n",
      "\t  save_steps:\t10000\n",
      "\t  team_change:\t40000\n",
      "\t  swap_steps:\t10000\n",
      "\t  window:\t10\n",
      "\t  play_against_latest_model_ratio:\t0.5\n",
      "\t  initial_elo:\t1200.0\n",
      "\tbehavioral_cloning:\tNone\n",
      "[INFO] Resuming from /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_PPO_PPO/Behavior_A.\n",
      "[INFO] Resuming training from step 1000158.\n",
      "[WARNING] Restarting worker[0] after 'Communicator has exited.'\n",
      "[INFO] Connected to Unity environment with package version 4.0.0 and communication version 1.5.0\n",
      "[INFO] Connected new brain: Behavior_B?team=1\n",
      "[INFO] Connected new brain: Behavior_A?team=0\n",
      "[INFO] Behavior_B. Step: 1020000. Time Elapsed: 105.350 s. Mean Reward: 3.500. Std of Reward: 4.555. Not Training. ELO: 1277.393.\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/bin/mlagents-learn\", line 7, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/site-packages/mlagents/trainers/learn.py\", line 270, in main\n",
      "    run_cli(parse_command_line())\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/site-packages/mlagents/trainers/learn.py\", line 266, in run_cli\n",
      "    run_training(run_seed, options, num_areas)\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/site-packages/mlagents/trainers/learn.py\", line 140, in run_training\n",
      "    env_manager.close()\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/site-packages/mlagents/trainers/subprocess_env_manager.py\", line 489, in close\n",
      "    step: EnvironmentResponse = self.step_queue.get_nowait()\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/multiprocessing/queues.py\", line 135, in get_nowait\n",
      "    return self.get(False)\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/multiprocessing/queues.py\", line 108, in get\n",
      "    if not self._rlock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "config_ppo_fp = os.path.join(cur_dir, \"config\", \"Pong_adversarial_ppo_ppo.yaml\")\n",
    "run_ppo_id = \"Pong_Adv_PPO_PPO\"\n",
    "\n",
    "!mlagents-learn $config_ppo_fp --env=$env_fp \\\n",
    "               --results-dir=$output_dir \\\n",
    "               --run-id=$run_ppo_id --base-port=$baseport --resume --inference --time-scale=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a47b032",
   "metadata": {},
   "source": [
    "## Training PPO-SAC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6886efe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/config/Pong_adversarial_ppo_sac.yaml\n",
      "Pong_Adv_PPO_SAC\n",
      "\n",
      "            ┐  ╖\n",
      "        ╓╖╬│╡  ││╬╖╖\n",
      "    ╓╖╬│││││┘  ╬│││││╬╖\n",
      " ╖╬│││││╬╜        ╙╬│││││╖╖                               ╗╗╗\n",
      " ╬╬╬╬╖││╦╖        ╖╬││╗╣╣╣╬      ╟╣╣╬    ╟╣╣╣             ╜╜╜  ╟╣╣\n",
      " ╬╬╬╬╬╬╬╬╖│╬╖╖╓╬╪│╓╣╣╣╣╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╒╣╣╖╗╣╣╣╗   ╣╣╣ ╣╣╣╣╣╣ ╟╣╣╖   ╣╣╣\n",
      " ╬╬╬╬┐  ╙╬╬╬╬│╓╣╣╣╝╜  ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╣╙ ╙╣╣╣  ╣╣╣ ╙╟╣╣╜╙  ╫╣╣  ╟╣╣\n",
      " ╬╬╬╬┐     ╙╬╬╣╣      ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣     ╣╣╣┌╣╣╜\n",
      " ╬╬╬╜       ╬╬╣╣      ╙╝╣╣╬      ╙╣╣╣╗╖╓╗╣╣╣╜ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣╦╓    ╣╣╣╣╣\n",
      " ╙   ╓╦╖    ╬╬╣╣   ╓╗╗╖            ╙╝╣╣╣╣╝╜   ╘╝╝╜   ╝╝╝  ╝╝╝   ╙╣╣╣    ╟╣╣╣\n",
      "   ╩╬╬╬╬╬╬╦╦╬╬╣╣╗╣╣╣╣╣╣╣╝                                             ╫╣╣╣╣\n",
      "      ╙╬╬╬╬╬╬╬╣╣╣╣╣╣╝╜\n",
      "          ╙╬╬╬╣╣╣╜\n",
      "             ╙\n",
      "        \n",
      " Version information:\n",
      "  ml-agents: 1.1.0,\n",
      "  ml-agents-envs: 1.1.0,\n",
      "  Communicator API: 1.5.0,\n",
      "  PyTorch: 2.8.0\n",
      "[INFO] Connected to Unity environment with package version 4.0.0 and communication version 1.5.0\n",
      "[INFO] Connected new brain: Behavior_B?team=1\n",
      "[INFO] Connected new brain: Behavior_A?team=0\n",
      "[INFO] Hyperparameters for behavior name Behavior_A: \n",
      "\ttrainer_type:\tppo\n",
      "\thyperparameters:\t\n",
      "\t  batch_size:\t128\n",
      "\t  buffer_size:\t1280\n",
      "\t  learning_rate:\t0.0003\n",
      "\t  beta:\t0.005\n",
      "\t  epsilon:\t0.2\n",
      "\t  lambd:\t0.95\n",
      "\t  num_epoch:\t3\n",
      "\t  shared_critic:\tFalse\n",
      "\t  learning_rate_schedule:\tlinear\n",
      "\t  beta_schedule:\tlinear\n",
      "\t  epsilon_schedule:\tlinear\n",
      "\tcheckpoint_interval:\t500000\n",
      "\tnetwork_settings:\t\n",
      "\t  normalize:\tTrue\n",
      "\t  hidden_units:\t128\n",
      "\t  num_layers:\t2\n",
      "\t  vis_encode_type:\tsimple\n",
      "\t  memory:\tNone\n",
      "\t  goal_conditioning_type:\thyper\n",
      "\t  deterministic:\tFalse\n",
      "\treward_signals:\t\n",
      "\t  extrinsic:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t1.0\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t128\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\tinit_path:\tNone\n",
      "\tkeep_checkpoints:\t5\n",
      "\teven_checkpoints:\tFalse\n",
      "\tmax_steps:\t1000000\n",
      "\ttime_horizon:\t1000\n",
      "\tsummary_freq:\t20000\n",
      "\tthreaded:\tFalse\n",
      "\tself_play:\t\n",
      "\t  save_steps:\t10000\n",
      "\t  team_change:\t40000\n",
      "\t  swap_steps:\t10000\n",
      "\t  window:\t10\n",
      "\t  play_against_latest_model_ratio:\t0.5\n",
      "\t  initial_elo:\t1200.0\n",
      "\tbehavioral_cloning:\tNone\n",
      "[INFO] Hyperparameters for behavior name Behavior_B: \n",
      "\ttrainer_type:\tsac\n",
      "\thyperparameters:\t\n",
      "\t  learning_rate:\t0.0003\n",
      "\t  learning_rate_schedule:\tlinear\n",
      "\t  batch_size:\t128\n",
      "\t  buffer_size:\t1280\n",
      "\t  buffer_init_steps:\t0\n",
      "\t  tau:\t0.005\n",
      "\t  steps_per_update:\t10.0\n",
      "\t  save_replay_buffer:\tFalse\n",
      "\t  init_entcoef:\t0.5\n",
      "\t  reward_signal_steps_per_update:\t10.0\n",
      "\tcheckpoint_interval:\t500000\n",
      "\tnetwork_settings:\t\n",
      "\t  normalize:\tTrue\n",
      "\t  hidden_units:\t128\n",
      "\t  num_layers:\t2\n",
      "\t  vis_encode_type:\tsimple\n",
      "\t  memory:\tNone\n",
      "\t  goal_conditioning_type:\thyper\n",
      "\t  deterministic:\tFalse\n",
      "\treward_signals:\t\n",
      "\t  extrinsic:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t1.0\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t128\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\tinit_path:\tNone\n",
      "\tkeep_checkpoints:\t5\n",
      "\teven_checkpoints:\tFalse\n",
      "\tmax_steps:\t1000000\n",
      "\ttime_horizon:\t1000\n",
      "\tsummary_freq:\t20000\n",
      "\tthreaded:\tFalse\n",
      "\tself_play:\t\n",
      "\t  save_steps:\t10000\n",
      "\t  team_change:\t40000\n",
      "\t  swap_steps:\t10000\n",
      "\t  window:\t10\n",
      "\t  play_against_latest_model_ratio:\t0.5\n",
      "\t  initial_elo:\t1200.0\n",
      "\tbehavioral_cloning:\tNone\n",
      "[INFO] Behavior_A. Step: 20000. Time Elapsed: 59.175 s. Mean Reward: 0.279. Std of Reward: 1.131. Training. ELO: 1200.852.\n",
      "[INFO] Behavior_A. Step: 40000. Time Elapsed: 110.067 s. Mean Reward: 0.863. Std of Reward: 1.022. Training. ELO: 1235.364.\n",
      "/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/site-packages/mlagents/trainers/torch_entities/utils.py:289: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:4424.)\n",
      "  torch.nn.functional.one_hot(_act.T, action_size[i]).float()\n",
      "[INFO] Behavior_B. Step: 20000. Time Elapsed: 177.399 s. Mean Reward: -0.049. Std of Reward: 1.066. Training. ELO: 1166.172.\n",
      "[INFO] Behavior_B. Step: 40000. Time Elapsed: 243.635 s. Mean Reward: -0.451. Std of Reward: 0.878. Training. ELO: 1148.482.\n",
      "[INFO] Behavior_A. Step: 60000. Time Elapsed: 296.844 s. Mean Reward: 0.801. Std of Reward: 1.018. Training. ELO: 1283.224.\n",
      "[INFO] Behavior_A. Step: 80000. Time Elapsed: 349.647 s. Mean Reward: 1.105. Std of Reward: 0.897. Training. ELO: 1302.662.\n",
      "[INFO] Behavior_B. Step: 60000. Time Elapsed: 415.863 s. Mean Reward: -0.195. Std of Reward: 1.011. Training. ELO: 1121.218.\n",
      "[INFO] Behavior_B. Step: 80000. Time Elapsed: 481.357 s. Mean Reward: -0.066. Std of Reward: 1.179. Training. ELO: 1126.960.\n",
      "[INFO] Behavior_A. Step: 100000. Time Elapsed: 534.482 s. Mean Reward: 1.128. Std of Reward: 0.839. Training. ELO: 1327.942.\n",
      "[INFO] Behavior_A. Step: 120000. Time Elapsed: 587.171 s. Mean Reward: 1.287. Std of Reward: 0.841. Training. ELO: 1346.905.\n",
      "[INFO] Behavior_B. Step: 100000. Time Elapsed: 651.970 s. Mean Reward: -0.031. Std of Reward: 1.227. Training. ELO: 1120.753.\n",
      "[INFO] Behavior_B. Step: 120000. Time Elapsed: 717.363 s. Mean Reward: 0.442. Std of Reward: 1.625. Training. ELO: 1124.859.\n",
      "[INFO] Behavior_A. Step: 140000. Time Elapsed: 771.845 s. Mean Reward: 1.163. Std of Reward: 0.801. Training. ELO: 1359.288.\n",
      "[INFO] Behavior_A. Step: 160000. Time Elapsed: 824.918 s. Mean Reward: 1.226. Std of Reward: 0.629. Training. ELO: 1378.705.\n",
      "[INFO] Behavior_B. Step: 140000. Time Elapsed: 889.281 s. Mean Reward: 0.181. Std of Reward: 1.796. Training. ELO: 1103.969.\n",
      "[INFO] Behavior_B. Step: 160000. Time Elapsed: 954.382 s. Mean Reward: 2.263. Std of Reward: 2.917. Training. ELO: 1104.690.\n",
      "[INFO] Behavior_A. Step: 180000. Time Elapsed: 1010.687 s. Mean Reward: 1.468. Std of Reward: 1.080. Training. ELO: 1388.377.\n",
      "[INFO] Behavior_A. Step: 200000. Time Elapsed: 1063.175 s. Mean Reward: 1.948. Std of Reward: 1.213. Training. ELO: 1392.277.\n",
      "[INFO] Behavior_B. Step: 180000. Time Elapsed: 1128.046 s. Mean Reward: 3.833. Std of Reward: 3.880. Training. ELO: 1108.877.\n",
      "[INFO] Behavior_B. Step: 200000. Time Elapsed: 1194.497 s. Mean Reward: 0.929. Std of Reward: 3.506. Training. ELO: 1106.861.\n",
      "[INFO] Behavior_A. Step: 220000. Time Elapsed: 1246.216 s. Mean Reward: 2.909. Std of Reward: 3.595. Training. ELO: 1395.281.\n",
      "[INFO] Behavior_A. Step: 240000. Time Elapsed: 1298.985 s. Mean Reward: 2.232. Std of Reward: 2.087. Training. ELO: 1396.970.\n",
      "[INFO] Behavior_B. Step: 220000. Time Elapsed: 1363.371 s. Mean Reward: 1.419. Std of Reward: 2.572. Training. ELO: 1108.819.\n",
      "[INFO] Behavior_B. Step: 240000. Time Elapsed: 1427.992 s. Mean Reward: 6.250. Std of Reward: 5.184. Training. ELO: 1109.007.\n",
      "[INFO] Behavior_A. Step: 260000. Time Elapsed: 1482.588 s. Mean Reward: 2.314. Std of Reward: 2.361. Training. ELO: 1402.759.\n",
      "[INFO] Behavior_A. Step: 280000. Time Elapsed: 1533.738 s. Mean Reward: 7.150. Std of Reward: 4.148. Training. ELO: 1404.280.\n",
      "[INFO] Behavior_B. Step: 260000. Time Elapsed: 1601.045 s. Mean Reward: 2.818. Std of Reward: 5.557. Training. ELO: 1110.148.\n",
      "[INFO] Behavior_B. Step: 280000. Time Elapsed: 1663.972 s. Mean Reward: 3.667. Std of Reward: 5.807. Training. ELO: 1109.694.\n",
      "[INFO] Behavior_A. Step: 300000. Time Elapsed: 1720.538 s. Mean Reward: 5.278. Std of Reward: 4.360. Training. ELO: 1403.404.\n",
      "[INFO] Behavior_A. Step: 320000. Time Elapsed: 1771.575 s. Mean Reward: 11.100. Std of Reward: 4.017. Training. ELO: 1401.876.\n",
      "[INFO] Behavior_B. Step: 300000. Time Elapsed: 1836.666 s. Mean Reward: 5.091. Std of Reward: 5.616. Training. ELO: 1111.435.\n",
      "[INFO] Behavior_B. Step: 320000. Time Elapsed: 1900.581 s. Mean Reward: -0.354. Std of Reward: 1.496. Training. ELO: 1106.981.\n",
      "[INFO] Behavior_A. Step: 340000. Time Elapsed: 1954.561 s. Mean Reward: 2.071. Std of Reward: 2.034. Training. ELO: 1403.746.\n",
      "[INFO] Behavior_A. Step: 360000. Time Elapsed: 2007.170 s. Mean Reward: 3.605. Std of Reward: 3.848. Training. ELO: 1404.575.\n",
      "[INFO] Behavior_B. Step: 340000. Time Elapsed: 2070.352 s. Mean Reward: 0.810. Std of Reward: 2.805. Training. ELO: 1102.160.\n",
      "[INFO] Behavior_B. Step: 360000. Time Elapsed: 2136.634 s. Mean Reward: 1.438. Std of Reward: 2.599. Training. ELO: 1101.544.\n",
      "[INFO] Behavior_A. Step: 380000. Time Elapsed: 2192.296 s. Mean Reward: 3.885. Std of Reward: 4.175. Training. ELO: 1406.591.\n",
      "[INFO] Behavior_A. Step: 400000. Time Elapsed: 2245.056 s. Mean Reward: 2.271. Std of Reward: 2.412. Training. ELO: 1404.495.\n",
      "[INFO] Behavior_B. Step: 380000. Time Elapsed: 2308.443 s. Mean Reward: 0.395. Std of Reward: 2.344. Training. ELO: 1105.080.\n",
      "[INFO] Behavior_B. Step: 400000. Time Elapsed: 2374.966 s. Mean Reward: -0.178. Std of Reward: 1.278. Training. ELO: 1103.282.\n",
      "[INFO] Behavior_A. Step: 420000. Time Elapsed: 2429.137 s. Mean Reward: 1.735. Std of Reward: 1.360. Training. ELO: 1408.168.\n",
      "[INFO] Behavior_A. Step: 440000. Time Elapsed: 2481.847 s. Mean Reward: 3.125. Std of Reward: 2.818. Training. ELO: 1412.262.\n",
      "[INFO] Behavior_B. Step: 420000. Time Elapsed: 2547.833 s. Mean Reward: -0.449. Std of Reward: 0.813. Training. ELO: 1097.855.\n",
      "[INFO] Behavior_B. Step: 440000. Time Elapsed: 2612.869 s. Mean Reward: 0.295. Std of Reward: 2.270. Training. ELO: 1091.205.\n",
      "[INFO] Behavior_A. Step: 460000. Time Elapsed: 2666.832 s. Mean Reward: 1.719. Std of Reward: 1.213. Training. ELO: 1425.364.\n",
      "[INFO] Behavior_A. Step: 480000. Time Elapsed: 2721.504 s. Mean Reward: 2.500. Std of Reward: 2.503. Training. ELO: 1428.334.\n",
      "[INFO] Behavior_B. Step: 460000. Time Elapsed: 2783.952 s. Mean Reward: 0.661. Std of Reward: 2.560. Training. ELO: 1085.863.\n",
      "[INFO] Behavior_B. Step: 480000. Time Elapsed: 2850.149 s. Mean Reward: 0.095. Std of Reward: 2.098. Training. ELO: 1082.508.\n",
      "[INFO] Behavior_A. Step: 500000. Time Elapsed: 2904.532 s. Mean Reward: 1.694. Std of Reward: 1.168. Training. ELO: 1435.707.\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_PPO_SAC/Behavior_A/Behavior_A-499358.onnx\n",
      "[INFO] Behavior_A. Step: 520000. Time Elapsed: 2959.090 s. Mean Reward: 3.250. Std of Reward: 3.953. Training. ELO: 1440.262.\n",
      "[INFO] Behavior_B. Step: 500000. Time Elapsed: 3021.765 s. Mean Reward: 1.643. Std of Reward: 4.126. Training. ELO: 1082.193.\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_PPO_SAC/Behavior_B/Behavior_B-499277.onnx\n",
      "[INFO] Behavior_B. Step: 520000. Time Elapsed: 3087.134 s. Mean Reward: 4.000. Std of Reward: 4.661. Training. ELO: 1081.614.\n",
      "[INFO] Behavior_A. Step: 540000. Time Elapsed: 3142.442 s. Mean Reward: 2.000. Std of Reward: 1.482. Training. ELO: 1439.296.\n",
      "[INFO] Behavior_A. Step: 560000. Time Elapsed: 3195.326 s. Mean Reward: 3.781. Std of Reward: 3.592. Training. ELO: 1441.559.\n",
      "[INFO] Behavior_B. Step: 540000. Time Elapsed: 3258.772 s. Mean Reward: 0.790. Std of Reward: 1.331. Training. ELO: 1082.194.\n",
      "[INFO] Behavior_B. Step: 560000. Time Elapsed: 3325.819 s. Mean Reward: 3.250. Std of Reward: 4.684. Training. ELO: 1082.329.\n",
      "[INFO] Behavior_A. Step: 580000. Time Elapsed: 3379.028 s. Mean Reward: 4.118. Std of Reward: 3.771. Training. ELO: 1441.726.\n",
      "[INFO] Behavior_A. Step: 600000. Time Elapsed: 3433.557 s. Mean Reward: 3.917. Std of Reward: 3.845. Training. ELO: 1441.450.\n",
      "[INFO] Behavior_B. Step: 580000. Time Elapsed: 3497.069 s. Mean Reward: 5.875. Std of Reward: 6.153. Training. ELO: 1083.937.\n",
      "[INFO] Behavior_B. Step: 600000. Time Elapsed: 3562.148 s. Mean Reward: 0.487. Std of Reward: 2.763. Training. ELO: 1083.841.\n",
      "[INFO] Behavior_A. Step: 620000. Time Elapsed: 3618.212 s. Mean Reward: 2.203. Std of Reward: 1.749. Training. ELO: 1441.476.\n",
      "[INFO] Behavior_A. Step: 640000. Time Elapsed: 3670.182 s. Mean Reward: 3.045. Std of Reward: 3.680. Training. ELO: 1440.680.\n",
      "[INFO] Behavior_B. Step: 620000. Time Elapsed: 3734.366 s. Mean Reward: -0.315. Std of Reward: 1.063. Training. ELO: 1080.250.\n",
      "[INFO] Behavior_B. Step: 640000. Time Elapsed: 3799.102 s. Mean Reward: -0.609. Std of Reward: 1.496. Training. ELO: 1073.854.\n",
      "[INFO] Behavior_A. Step: 660000. Time Elapsed: 3855.209 s. Mean Reward: 1.256. Std of Reward: 1.181. Training. ELO: 1447.216.\n",
      "[INFO] Behavior_A. Step: 680000. Time Elapsed: 3907.906 s. Mean Reward: 1.320. Std of Reward: 0.953. Training. ELO: 1456.075.\n",
      "[INFO] Behavior_B. Step: 660000. Time Elapsed: 3971.576 s. Mean Reward: -0.755. Std of Reward: 0.557. Training. ELO: 1055.426.\n",
      "[INFO] Behavior_B. Step: 680000. Time Elapsed: 4038.001 s. Mean Reward: 0.287. Std of Reward: 2.534. Training. ELO: 1046.747.\n",
      "[INFO] Behavior_A. Step: 700000. Time Elapsed: 4094.599 s. Mean Reward: 1.536. Std of Reward: 1.362. Training. ELO: 1464.565.\n",
      "[INFO] Behavior_A. Step: 720000. Time Elapsed: 4147.173 s. Mean Reward: 1.820. Std of Reward: 1.501. Training. ELO: 1470.488.\n",
      "[INFO] Behavior_B. Step: 700000. Time Elapsed: 4210.417 s. Mean Reward: -0.441. Std of Reward: 1.682. Training. ELO: 1039.058.\n",
      "[INFO] Behavior_B. Step: 720000. Time Elapsed: 4277.579 s. Mean Reward: 0.145. Std of Reward: 2.722. Training. ELO: 1036.217.\n",
      "[INFO] Behavior_A. Step: 740000. Time Elapsed: 4334.264 s. Mean Reward: 2.000. Std of Reward: 1.725. Training. ELO: 1474.322.\n",
      "[INFO] Behavior_A. Step: 760000. Time Elapsed: 4387.846 s. Mean Reward: 3.765. Std of Reward: 3.139. Training. ELO: 1475.384.\n",
      "[INFO] Behavior_B. Step: 740000. Time Elapsed: 4445.862 s. Mean Reward: 3.167. Std of Reward: 4.905. Training. ELO: 1034.803.\n",
      "[INFO] Behavior_B. Step: 760000. Time Elapsed: 4511.416 s. Mean Reward: 11.875. Std of Reward: 1.709. Training.\n",
      "[INFO] Behavior_A. Step: 780000. Time Elapsed: 4572.796 s. Mean Reward: 5.778. Std of Reward: 4.607. Training. ELO: 1474.565.\n",
      "[INFO] Behavior_A. Step: 800000. Time Elapsed: 4624.938 s. Mean Reward: 2.250. Std of Reward: 2.364. Training. ELO: 1473.505.\n",
      "[INFO] Behavior_B. Step: 780000. Time Elapsed: 4683.419 s. Mean Reward: 2.654. Std of Reward: 4.900. Training. ELO: 1036.511.\n",
      "[INFO] Behavior_B. Step: 800000. Time Elapsed: 4750.204 s. Mean Reward: 3.611. Std of Reward: 4.162. Training. ELO: 1035.985.\n",
      "[INFO] Behavior_A. Step: 820000. Time Elapsed: 4809.738 s. Mean Reward: 4.200. Std of Reward: 3.809. Training. ELO: 1474.531.\n",
      "[INFO] Behavior_A. Step: 840000. Time Elapsed: 4861.946 s. Mean Reward: 3.733. Std of Reward: 4.370. Training. ELO: 1474.873.\n",
      "[INFO] Behavior_B. Step: 820000. Time Elapsed: 4921.627 s. Mean Reward: 0.825. Std of Reward: 3.658. Training. ELO: 1035.495.\n",
      "[INFO] Behavior_B. Step: 840000. Time Elapsed: 4989.636 s. Mean Reward: 7.286. Std of Reward: 5.981. Training. ELO: 1035.511.\n",
      "[INFO] Behavior_A. Step: 860000. Time Elapsed: 5049.351 s. Mean Reward: 4.200. Std of Reward: 4.118. Training. ELO: 1475.575.\n",
      "[INFO] Behavior_A. Step: 880000. Time Elapsed: 5101.046 s. Mean Reward: 7.214. Std of Reward: 4.883. Training. ELO: 1476.154.\n",
      "[INFO] Behavior_B. Step: 860000. Time Elapsed: 5161.488 s. Mean Reward: 8.214. Std of Reward: 4.605. Training. ELO: 1036.664.\n",
      "[INFO] Behavior_B. Step: 880000. Time Elapsed: 5224.573 s. Mean Reward: 0.452. Std of Reward: 2.182. Training. ELO: 1036.105.\n",
      "[INFO] Behavior_A. Step: 900000. Time Elapsed: 5286.908 s. Mean Reward: 7.062. Std of Reward: 3.964. Training. ELO: 1477.668.\n",
      "[INFO] Behavior_A. Step: 920000. Time Elapsed: 5340.923 s. Mean Reward: 3.250. Std of Reward: 3.548. Training. ELO: 1478.350.\n",
      "[INFO] Behavior_B. Step: 900000. Time Elapsed: 5399.791 s. Mean Reward: 0.394. Std of Reward: 3.286. Training. ELO: 1034.200.\n",
      "[INFO] Behavior_B. Step: 920000. Time Elapsed: 5464.537 s. Mean Reward: 0.393. Std of Reward: 2.995. Training. ELO: 1031.766.\n",
      "[INFO] Behavior_A. Step: 940000. Time Elapsed: 5526.769 s. Mean Reward: 4.846. Std of Reward: 4.180. Training. ELO: 1478.875.\n",
      "[INFO] Behavior_A. Step: 960000. Time Elapsed: 5579.377 s. Mean Reward: 1.912. Std of Reward: 1.243. Training. ELO: 1480.029.\n",
      "[INFO] Behavior_B. Step: 940000. Time Elapsed: 5636.070 s. Mean Reward: 3.450. Std of Reward: 5.012. Training. ELO: 1032.238.\n",
      "[INFO] Behavior_B. Step: 960000. Time Elapsed: 5701.565 s. Mean Reward: 12.250. Std of Reward: 1.436. Training.\n",
      "[INFO] Behavior_A. Step: 980000. Time Elapsed: 5765.172 s. Mean Reward: 5.200. Std of Reward: 4.377. Training. ELO: 1482.071.\n",
      "[INFO] Behavior_A. Step: 1000000. Time Elapsed: 5817.797 s. Mean Reward: 3.167. Std of Reward: 3.532. Training. ELO: 1482.019.\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_PPO_SAC/Behavior_A/Behavior_A-999866.onnx\n",
      "[INFO] Behavior_B. Step: 980000. Time Elapsed: 5871.713 s. Mean Reward: 6.000. Std of Reward: 5.534. Training. ELO: 1032.607.\n",
      "[INFO] Behavior_B. Step: 1000000. Time Elapsed: 5939.024 s. Mean Reward: 5.214. Std of Reward: 5.161. Training. ELO: 1033.471.\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_PPO_SAC/Behavior_B/Behavior_B-999841.onnx\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_PPO_SAC/Behavior_A/Behavior_A-1007583.onnx\n",
      "[INFO] Copied /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_PPO_SAC/Behavior_A/Behavior_A-1007583.onnx to /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_PPO_SAC/Behavior_A.onnx.\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_PPO_SAC/Behavior_B/Behavior_B-1000841.onnx\n",
      "[INFO] Copied /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_PPO_SAC/Behavior_B/Behavior_B-1000841.onnx to /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_PPO_SAC/Behavior_B.onnx.\n"
     ]
    }
   ],
   "source": [
    "config_sac_fp = os.path.join(cur_dir, \"config\", \"Pong_adversarial_ppo_sac.yaml\")\n",
    "run_sac_id = \"Pong_Adv_PPO_SAC\"\n",
    "print(config_sac_fp)\n",
    "print(run_sac_id)\n",
    "\n",
    "!mlagents-learn $config_sac_fp \\\n",
    "               --env=$env_fp \\\n",
    "               --results-dir=$output_dir \\\n",
    "               --run-id=$run_sac_id --base-port=$baseport\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d748d1",
   "metadata": {},
   "source": [
    "### Testing PPO-SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f38a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            ┐  ╖\n",
      "        ╓╖╬│╡  ││╬╖╖\n",
      "    ╓╖╬│││││┘  ╬│││││╬╖\n",
      " ╖╬│││││╬╜        ╙╬│││││╖╖                               ╗╗╗\n",
      " ╬╬╬╬╖││╦╖        ╖╬││╗╣╣╣╬      ╟╣╣╬    ╟╣╣╣             ╜╜╜  ╟╣╣\n",
      " ╬╬╬╬╬╬╬╬╖│╬╖╖╓╬╪│╓╣╣╣╣╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╒╣╣╖╗╣╣╣╗   ╣╣╣ ╣╣╣╣╣╣ ╟╣╣╖   ╣╣╣\n",
      " ╬╬╬╬┐  ╙╬╬╬╬│╓╣╣╣╝╜  ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╣╙ ╙╣╣╣  ╣╣╣ ╙╟╣╣╜╙  ╫╣╣  ╟╣╣\n",
      " ╬╬╬╬┐     ╙╬╬╣╣      ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣     ╣╣╣┌╣╣╜\n",
      " ╬╬╬╜       ╬╬╣╣      ╙╝╣╣╬      ╙╣╣╣╗╖╓╗╣╣╣╜ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣╦╓    ╣╣╣╣╣\n",
      " ╙   ╓╦╖    ╬╬╣╣   ╓╗╗╖            ╙╝╣╣╣╣╝╜   ╘╝╝╜   ╝╝╝  ╝╝╝   ╙╣╣╣    ╟╣╣╣\n",
      "   ╩╬╬╬╬╬╬╦╦╬╬╣╣╗╣╣╣╣╣╣╣╝                                             ╫╣╣╣╣\n",
      "      ╙╬╬╬╬╬╬╬╣╣╣╣╣╣╝╜\n",
      "          ╙╬╬╬╣╣╣╜\n",
      "             ╙\n",
      "        \n",
      " Version information:\n",
      "  ml-agents: 1.1.0,\n",
      "  ml-agents-envs: 1.1.0,\n",
      "  Communicator API: 1.5.0,\n",
      "  PyTorch: 2.8.0\n",
      "[INFO] Connected to Unity environment with package version 4.0.0 and communication version 1.5.0\n",
      "[INFO] Connected new brain: Behavior_B?team=1\n",
      "[INFO] Connected new brain: Behavior_A?team=0\n",
      "[INFO] Hyperparameters for behavior name Behavior_B: \n",
      "\ttrainer_type:\tsac\n",
      "\thyperparameters:\t\n",
      "\t  learning_rate:\t0.0003\n",
      "\t  learning_rate_schedule:\tlinear\n",
      "\t  batch_size:\t128\n",
      "\t  buffer_size:\t1280\n",
      "\t  buffer_init_steps:\t0\n",
      "\t  tau:\t0.005\n",
      "\t  steps_per_update:\t10.0\n",
      "\t  save_replay_buffer:\tFalse\n",
      "\t  init_entcoef:\t0.5\n",
      "\t  reward_signal_steps_per_update:\t10.0\n",
      "\tcheckpoint_interval:\t500000\n",
      "\tnetwork_settings:\t\n",
      "\t  normalize:\tTrue\n",
      "\t  hidden_units:\t128\n",
      "\t  num_layers:\t2\n",
      "\t  vis_encode_type:\tsimple\n",
      "\t  memory:\tNone\n",
      "\t  goal_conditioning_type:\thyper\n",
      "\t  deterministic:\tFalse\n",
      "\treward_signals:\t\n",
      "\t  extrinsic:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t1.0\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t128\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\tinit_path:\tNone\n",
      "\tkeep_checkpoints:\t5\n",
      "\teven_checkpoints:\tFalse\n",
      "\tmax_steps:\t1000000\n",
      "\ttime_horizon:\t1000\n",
      "\tsummary_freq:\t20000\n",
      "\tthreaded:\tFalse\n",
      "\tself_play:\t\n",
      "\t  save_steps:\t10000\n",
      "\t  team_change:\t40000\n",
      "\t  swap_steps:\t10000\n",
      "\t  window:\t10\n",
      "\t  play_against_latest_model_ratio:\t0.5\n",
      "\t  initial_elo:\t1200.0\n",
      "\tbehavioral_cloning:\tNone\n",
      "[INFO] Resuming from /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_PPO_SAC/Behavior_B.\n",
      "[INFO] Resuming training from step 1000841.\n",
      "[INFO] Hyperparameters for behavior name Behavior_A: \n",
      "\ttrainer_type:\tppo\n",
      "\thyperparameters:\t\n",
      "\t  batch_size:\t128\n",
      "\t  buffer_size:\t1280\n",
      "\t  learning_rate:\t0.0003\n",
      "\t  beta:\t0.005\n",
      "\t  epsilon:\t0.2\n",
      "\t  lambd:\t0.95\n",
      "\t  num_epoch:\t3\n",
      "\t  shared_critic:\tFalse\n",
      "\t  learning_rate_schedule:\tlinear\n",
      "\t  beta_schedule:\tlinear\n",
      "\t  epsilon_schedule:\tlinear\n",
      "\tcheckpoint_interval:\t500000\n",
      "\tnetwork_settings:\t\n",
      "\t  normalize:\tTrue\n",
      "\t  hidden_units:\t128\n",
      "\t  num_layers:\t2\n",
      "\t  vis_encode_type:\tsimple\n",
      "\t  memory:\tNone\n",
      "\t  goal_conditioning_type:\thyper\n",
      "\t  deterministic:\tFalse\n",
      "\treward_signals:\t\n",
      "\t  extrinsic:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t1.0\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t128\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\tinit_path:\tNone\n",
      "\tkeep_checkpoints:\t5\n",
      "\teven_checkpoints:\tFalse\n",
      "\tmax_steps:\t1000000\n",
      "\ttime_horizon:\t1000\n",
      "\tsummary_freq:\t20000\n",
      "\tthreaded:\tFalse\n",
      "\tself_play:\t\n",
      "\t  save_steps:\t10000\n",
      "\t  team_change:\t40000\n",
      "\t  swap_steps:\t10000\n",
      "\t  window:\t10\n",
      "\t  play_against_latest_model_ratio:\t0.5\n",
      "\t  initial_elo:\t1200.0\n",
      "\tbehavioral_cloning:\tNone\n",
      "[INFO] Resuming from /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_PPO_SAC/Behavior_A.\n",
      "[INFO] Resuming training from step 1007583.\n",
      "^C\n",
      "Exception ignored in atexit callback: <function _exit_function at 0x14250eb90>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/multiprocessing/util.py\", line 357, in _exit_function\n",
      "    p.join()\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/multiprocessing/popen_fork.py\", line 43, in wait\n",
      "    return self.poll(os.WNOHANG if timeout == 0.0 else 0)\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/multiprocessing/popen_fork.py\", line 27, in poll\n",
      "    pid, sts = os.waitpid(self.pid, flag)\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "# config_sac_fp = os.path.join(cur_dir, \"config\", \"Pong_adversarial_ppo_sac.yaml\")\n",
    "# run_sac_id = \"Pong_Adv_PPO_SAC\"\n",
    "\n",
    "# !mlagents-learn $config_sac_fp \\\n",
    "#                --env=$env_fp \\\n",
    "#                --results-dir=$output_dir \\\n",
    "#                --run-id=$run_sac_id --base-port=$baseport --resume --inference --time-scale=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40c13bd",
   "metadata": {},
   "source": [
    "## Training PPO-POCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694c6520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/config/Pong_adversarial_ppo_poca.yaml\n",
      "Pong_Adv_PPO_POCA\n",
      "\n",
      "            ┐  ╖\n",
      "        ╓╖╬│╡  ││╬╖╖\n",
      "    ╓╖╬│││││┘  ╬│││││╬╖\n",
      " ╖╬│││││╬╜        ╙╬│││││╖╖                               ╗╗╗\n",
      " ╬╬╬╬╖││╦╖        ╖╬││╗╣╣╣╬      ╟╣╣╬    ╟╣╣╣             ╜╜╜  ╟╣╣\n",
      " ╬╬╬╬╬╬╬╬╖│╬╖╖╓╬╪│╓╣╣╣╣╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╒╣╣╖╗╣╣╣╗   ╣╣╣ ╣╣╣╣╣╣ ╟╣╣╖   ╣╣╣\n",
      " ╬╬╬╬┐  ╙╬╬╬╬│╓╣╣╣╝╜  ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╣╙ ╙╣╣╣  ╣╣╣ ╙╟╣╣╜╙  ╫╣╣  ╟╣╣\n",
      " ╬╬╬╬┐     ╙╬╬╣╣      ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣     ╣╣╣┌╣╣╜\n",
      " ╬╬╬╜       ╬╬╣╣      ╙╝╣╣╬      ╙╣╣╣╗╖╓╗╣╣╣╜ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣╦╓    ╣╣╣╣╣\n",
      " ╙   ╓╦╖    ╬╬╣╣   ╓╗╗╖            ╙╝╣╣╣╣╝╜   ╘╝╝╜   ╝╝╝  ╝╝╝   ╙╣╣╣    ╟╣╣╣\n",
      "   ╩╬╬╬╬╬╬╦╦╬╬╣╣╗╣╣╣╣╣╣╣╝                                             ╫╣╣╣╣\n",
      "      ╙╬╬╬╬╬╬╬╣╣╣╣╣╣╝╜\n",
      "          ╙╬╬╬╣╣╣╜\n",
      "             ╙\n",
      "        \n",
      " Version information:\n",
      "  ml-agents: 1.1.0,\n",
      "  ml-agents-envs: 1.1.0,\n",
      "  Communicator API: 1.5.0,\n",
      "  PyTorch: 2.8.0\n",
      "[INFO] Connected to Unity environment with package version 4.0.0 and communication version 1.5.0\n",
      "[INFO] Connected new brain: Behavior_B?team=1\n",
      "[INFO] Connected new brain: Behavior_A?team=0\n",
      "[INFO] Hyperparameters for behavior name Behavior_B: \n",
      "\ttrainer_type:\tpoca\n",
      "\thyperparameters:\t\n",
      "\t  batch_size:\t128\n",
      "\t  buffer_size:\t1280\n",
      "\t  learning_rate:\t0.0003\n",
      "\t  beta:\t0.005\n",
      "\t  epsilon:\t0.2\n",
      "\t  lambd:\t0.95\n",
      "\t  num_epoch:\t3\n",
      "\t  learning_rate_schedule:\tlinear\n",
      "\t  beta_schedule:\tlinear\n",
      "\t  epsilon_schedule:\tlinear\n",
      "\tcheckpoint_interval:\t500000\n",
      "\tnetwork_settings:\t\n",
      "\t  normalize:\tTrue\n",
      "\t  hidden_units:\t128\n",
      "\t  num_layers:\t2\n",
      "\t  vis_encode_type:\tsimple\n",
      "\t  memory:\tNone\n",
      "\t  goal_conditioning_type:\thyper\n",
      "\t  deterministic:\tFalse\n",
      "\treward_signals:\t\n",
      "\t  extrinsic:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t1.0\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t128\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\tinit_path:\tNone\n",
      "\tkeep_checkpoints:\t5\n",
      "\teven_checkpoints:\tFalse\n",
      "\tmax_steps:\t1000000\n",
      "\ttime_horizon:\t1000\n",
      "\tsummary_freq:\t20000\n",
      "\tthreaded:\tFalse\n",
      "\tself_play:\t\n",
      "\t  save_steps:\t10000\n",
      "\t  team_change:\t40000\n",
      "\t  swap_steps:\t10000\n",
      "\t  window:\t10\n",
      "\t  play_against_latest_model_ratio:\t0.5\n",
      "\t  initial_elo:\t1200.0\n",
      "\tbehavioral_cloning:\tNone\n",
      "[INFO] Hyperparameters for behavior name Behavior_A: \n",
      "\ttrainer_type:\tppo\n",
      "\thyperparameters:\t\n",
      "\t  batch_size:\t128\n",
      "\t  buffer_size:\t1280\n",
      "\t  learning_rate:\t0.0003\n",
      "\t  beta:\t0.005\n",
      "\t  epsilon:\t0.2\n",
      "\t  lambd:\t0.95\n",
      "\t  num_epoch:\t3\n",
      "\t  shared_critic:\tFalse\n",
      "\t  learning_rate_schedule:\tlinear\n",
      "\t  beta_schedule:\tlinear\n",
      "\t  epsilon_schedule:\tlinear\n",
      "\tcheckpoint_interval:\t500000\n",
      "\tnetwork_settings:\t\n",
      "\t  normalize:\tTrue\n",
      "\t  hidden_units:\t128\n",
      "\t  num_layers:\t2\n",
      "\t  vis_encode_type:\tsimple\n",
      "\t  memory:\tNone\n",
      "\t  goal_conditioning_type:\thyper\n",
      "\t  deterministic:\tFalse\n",
      "\treward_signals:\t\n",
      "\t  extrinsic:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t1.0\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t128\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\tinit_path:\tNone\n",
      "\tkeep_checkpoints:\t5\n",
      "\teven_checkpoints:\tFalse\n",
      "\tmax_steps:\t1000000\n",
      "\ttime_horizon:\t1000\n",
      "\tsummary_freq:\t20000\n",
      "\tthreaded:\tFalse\n",
      "\tself_play:\t\n",
      "\t  save_steps:\t10000\n",
      "\t  team_change:\t40000\n",
      "\t  swap_steps:\t10000\n",
      "\t  window:\t10\n",
      "\t  play_against_latest_model_ratio:\t0.5\n",
      "\t  initial_elo:\t1200.0\n",
      "\tbehavioral_cloning:\tNone\n",
      "[INFO] Behavior_B. Step: 20000. Time Elapsed: 59.848 s. Mean Reward: 0.138. Mean Group Reward: 0.000. Training. ELO: 1201.973.\n",
      "[INFO] Behavior_B. Step: 40000. Time Elapsed: 114.717 s. Mean Reward: 0.642. Mean Group Reward: 0.000. Training. ELO: 1219.536.\n",
      "[INFO] Behavior_A. Step: 20000. Time Elapsed: 168.161 s. Mean Reward: 0.038. Std of Reward: 1.073. Training. ELO: 1190.720.\n",
      "[INFO] Behavior_A. Step: 40000. Time Elapsed: 220.827 s. Mean Reward: 0.637. Std of Reward: 1.069. Training. ELO: 1204.593.\n",
      "[INFO] Behavior_B. Step: 60000. Time Elapsed: 276.133 s. Mean Reward: -0.096. Mean Group Reward: 0.000. Training. ELO: 1218.166.\n",
      "[INFO] Behavior_B. Step: 80000. Time Elapsed: 330.262 s. Mean Reward: 0.938. Mean Group Reward: 0.000. Training. ELO: 1220.663.\n",
      "[INFO] Behavior_A. Step: 60000. Time Elapsed: 383.071 s. Mean Reward: 0.967. Std of Reward: 1.211. Training. ELO: 1245.079.\n",
      "[INFO] Behavior_A. Step: 80000. Time Elapsed: 437.607 s. Mean Reward: 1.061. Std of Reward: 1.187. Training. ELO: 1264.010.\n",
      "[INFO] Behavior_B. Step: 100000. Time Elapsed: 491.255 s. Mean Reward: 1.370. Mean Group Reward: 0.000. Training. ELO: 1227.216.\n",
      "[INFO] Behavior_B. Step: 120000. Time Elapsed: 547.681 s. Mean Reward: 1.250. Mean Group Reward: 0.000. Training. ELO: 1241.775.\n",
      "[INFO] Behavior_A. Step: 100000. Time Elapsed: 598.726 s. Mean Reward: 1.299. Std of Reward: 1.298. Training. ELO: 1278.094.\n",
      "[INFO] Behavior_A. Step: 120000. Time Elapsed: 653.010 s. Mean Reward: 3.194. Std of Reward: 2.959. Training. ELO: 1286.952.\n",
      "[INFO] Behavior_B. Step: 140000. Time Elapsed: 706.888 s. Mean Reward: 2.000. Mean Group Reward: 0.000. Training. ELO: 1253.408.\n",
      "[INFO] Behavior_B. Step: 160000. Time Elapsed: 759.520 s. Mean Reward: 2.848. Mean Group Reward: 0.000. Training. ELO: 1257.916.\n",
      "[INFO] Behavior_A. Step: 140000. Time Elapsed: 814.644 s. Mean Reward: 3.450. Std of Reward: 2.987. Training. ELO: 1286.550.\n",
      "[INFO] Behavior_A. Step: 160000. Time Elapsed: 865.385 s. Mean Reward: 3.278. Std of Reward: 3.284. Training. ELO: 1285.353.\n",
      "[INFO] Behavior_B. Step: 180000. Time Elapsed: 923.691 s. Mean Reward: 3.917. Mean Group Reward: 0.000. Training. ELO: 1261.894.\n",
      "[INFO] Behavior_B. Step: 200000. Time Elapsed: 976.551 s. Mean Reward: 2.370. Mean Group Reward: 0.000. Training. ELO: 1263.362.\n",
      "[INFO] Behavior_A. Step: 180000. Time Elapsed: 1029.433 s. Mean Reward: 3.375. Std of Reward: 3.110. Training. ELO: 1288.523.\n",
      "[INFO] Behavior_A. Step: 200000. Time Elapsed: 1081.377 s. Mean Reward: 1.950. Std of Reward: 1.890. Training. ELO: 1289.255.\n",
      "[INFO] Behavior_B. Step: 220000. Time Elapsed: 1138.714 s. Mean Reward: 1.978. Mean Group Reward: 0.000. Training. ELO: 1266.966.\n",
      "[INFO] Behavior_B. Step: 240000. Time Elapsed: 1191.802 s. Mean Reward: 3.667. Mean Group Reward: 0.000. Training. ELO: 1270.101.\n",
      "[INFO] Behavior_A. Step: 220000. Time Elapsed: 1244.280 s. Mean Reward: 2.789. Std of Reward: 3.172. Training. ELO: 1290.536.\n",
      "[INFO] Behavior_A. Step: 240000. Time Elapsed: 1296.528 s. Mean Reward: 4.423. Std of Reward: 5.413. Training. ELO: 1290.883.\n",
      "[INFO] Behavior_B. Step: 260000. Time Elapsed: 1351.232 s. Mean Reward: 3.450. Mean Group Reward: 0.000. Training. ELO: 1271.403.\n",
      "[INFO] Behavior_B. Step: 280000. Time Elapsed: 1407.537 s. Mean Reward: 2.304. Mean Group Reward: 0.000. Training. ELO: 1274.696.\n",
      "[INFO] Behavior_A. Step: 260000. Time Elapsed: 1457.029 s. Mean Reward: 3.250. Std of Reward: 4.910. Training. ELO: 1287.555.\n",
      "[INFO] Behavior_A. Step: 280000. Time Elapsed: 1510.306 s. Mean Reward: 1.444. Std of Reward: 2.488. Training. ELO: 1282.571.\n",
      "[INFO] Behavior_B. Step: 300000. Time Elapsed: 1565.082 s. Mean Reward: 3.235. Mean Group Reward: 0.000. Training. ELO: 1280.630.\n",
      "[INFO] Behavior_B. Step: 320000. Time Elapsed: 1619.518 s. Mean Reward: 3.656. Mean Group Reward: 0.000. Training. ELO: 1283.880.\n",
      "[INFO] Behavior_A. Step: 300000. Time Elapsed: 1672.726 s. Mean Reward: 5.000. Std of Reward: 2.160. Training. ELO: 1278.004.\n",
      "[INFO] Behavior_A. Step: 320000. Time Elapsed: 1723.490 s. Mean Reward: 3.958. Std of Reward: 5.911. Training. ELO: 1275.539.\n",
      "[INFO] Behavior_B. Step: 340000. Time Elapsed: 1781.615 s. Mean Reward: 3.077. Mean Group Reward: 0.000. Training. ELO: 1284.083.\n",
      "[INFO] Behavior_B. Step: 360000. Time Elapsed: 1836.558 s. Mean Reward: 4.955. Mean Group Reward: 0.000. Training. ELO: 1283.859.\n",
      "[INFO] Behavior_A. Step: 340000. Time Elapsed: 1888.689 s. Mean Reward: 7.286. Std of Reward: 4.832. Training. ELO: 1274.537.\n",
      "[INFO] Behavior_A. Step: 360000. Time Elapsed: 1940.683 s. Mean Reward: 4.083. Std of Reward: 3.322. Training. ELO: 1276.188.\n",
      "[INFO] Behavior_B. Step: 380000. Time Elapsed: 1996.573 s. Mean Reward: 4.500. Mean Group Reward: 0.000. Training. ELO: 1281.715.\n",
      "[INFO] Behavior_B. Step: 400000. Time Elapsed: 2050.669 s. Mean Reward: 4.269. Mean Group Reward: 0.000. Training. ELO: 1282.648.\n",
      "[INFO] Behavior_A. Step: 380000. Time Elapsed: 2101.994 s. Mean Reward: 6.500. Std of Reward: 4.472. Training. ELO: 1277.475.\n",
      "[INFO] Behavior_A. Step: 400000. Time Elapsed: 2155.541 s. Mean Reward: 4.455. Std of Reward: 4.624. Training. ELO: 1277.859.\n",
      "[INFO] Behavior_B. Step: 420000. Time Elapsed: 2211.671 s. Mean Reward: 6.500. Mean Group Reward: 0.000. Training. ELO: 1282.996.\n",
      "[INFO] Behavior_B. Step: 440000. Time Elapsed: 2266.747 s. Mean Reward: 5.500. Mean Group Reward: 0.000. Training. ELO: 1282.930.\n",
      "[INFO] Behavior_A. Step: 420000. Time Elapsed: 2315.030 s. Mean Reward: 5.188. Std of Reward: 4.782. Training. ELO: 1277.977.\n",
      "[INFO] Behavior_A. Step: 440000. Time Elapsed: 2368.342 s. Mean Reward: 5.450. Std of Reward: 4.552. Training. ELO: 1278.613.\n",
      "[INFO] Behavior_B. Step: 460000. Time Elapsed: 2427.164 s. Mean Reward: 3.000. Mean Group Reward: 0.000. Training. ELO: 1282.412.\n",
      "[INFO] Behavior_B. Step: 480000. Time Elapsed: 2482.859 s. Mean Reward: 3.469. Mean Group Reward: 0.000. Training. ELO: 1282.605.\n",
      "[INFO] Behavior_A. Step: 460000. Time Elapsed: 2530.125 s. Mean Reward: 6.643. Std of Reward: 3.943. Training. ELO: 1276.796.\n",
      "[INFO] Behavior_A. Step: 480000. Time Elapsed: 2583.338 s. Mean Reward: 7.417. Std of Reward: 3.576. Training. ELO: 1278.576.\n",
      "[INFO] Behavior_B. Step: 500000. Time Elapsed: 2642.306 s. Mean Reward: 5.143. Mean Group Reward: 0.000. Training. ELO: 1282.859.\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_PPO_POCA/Behavior_B/Behavior_B-499321.onnx\n",
      "[INFO] Behavior_B. Step: 520000. Time Elapsed: 2696.864 s. Mean Reward: 10.400. Mean Group Reward: 0.000. Training. ELO: 1281.888.\n",
      "[INFO] Behavior_A. Step: 500000. Time Elapsed: 2745.880 s. Mean Reward: 5.500. Std of Reward: 4.564. Training. ELO: 1279.851.\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_PPO_POCA/Behavior_A/Behavior_A-499309.onnx\n",
      "[INFO] Behavior_A. Step: 520000. Time Elapsed: 2797.997 s. Mean Reward: 5.400. Std of Reward: 3.192. Training. ELO: 1281.907.\n",
      "[INFO] Behavior_B. Step: 540000. Time Elapsed: 2856.318 s. Mean Reward: 3.650. Mean Group Reward: 0.000. Training. ELO: 1277.676.\n",
      "[INFO] Behavior_B. Step: 560000. Time Elapsed: 2910.341 s. Mean Reward: 6.200. Mean Group Reward: 0.000. Training. ELO: 1275.987.\n",
      "[INFO] Behavior_A. Step: 540000. Time Elapsed: 2959.972 s. Mean Reward: 5.188. Std of Reward: 4.077. Training. ELO: 1286.271.\n",
      "[INFO] Behavior_A. Step: 560000. Time Elapsed: 3013.063 s. Mean Reward: 11.700. Std of Reward: 2.421. Training. ELO: 1288.921.\n",
      "[INFO] Behavior_B. Step: 580000. Time Elapsed: 3071.046 s. Mean Reward: 6.812. Mean Group Reward: 0.000. Training. ELO: 1275.692.\n",
      "[INFO] Behavior_B. Step: 600000. Time Elapsed: 3126.521 s. Mean Reward: 3.050. Mean Group Reward: 0.000. Training. ELO: 1274.209.\n",
      "[INFO] Behavior_A. Step: 580000. Time Elapsed: 3177.317 s. Mean Reward: 3.147. Std of Reward: 2.611. Training. ELO: 1291.836.\n",
      "[INFO] Behavior_A. Step: 600000. Time Elapsed: 3228.333 s. Mean Reward: 4.875. Std of Reward: 4.649. Training. ELO: 1295.289.\n",
      "[INFO] Behavior_B. Step: 620000. Time Elapsed: 3286.647 s. Mean Reward: 7.000. Mean Group Reward: 0.000. Training. ELO: 1272.577.\n",
      "[INFO] Behavior_B. Step: 640000. Time Elapsed: 3340.476 s. Mean Reward: 8.167. Mean Group Reward: 0.000. Training. ELO: 1272.297.\n",
      "[INFO] Behavior_A. Step: 620000. Time Elapsed: 3389.971 s. Mean Reward: 4.333. Std of Reward: 5.197. Training. ELO: 1295.224.\n",
      "[INFO] Behavior_A. Step: 640000. Time Elapsed: 3443.784 s. Mean Reward: 6.714. Std of Reward: 5.731. Training. ELO: 1295.228.\n",
      "[INFO] Behavior_B. Step: 660000. Time Elapsed: 3503.130 s. Mean Reward: 2.731. Mean Group Reward: 0.000. Training. ELO: 1268.695.\n",
      "[INFO] Behavior_B. Step: 680000. Time Elapsed: 3554.775 s. Mean Reward: 2.789. Mean Group Reward: 0.000. Training. ELO: 1268.069.\n",
      "[INFO] Behavior_A. Step: 660000. Time Elapsed: 3605.023 s. Mean Reward: 6.071. Std of Reward: 3.895. Training. ELO: 1297.149.\n",
      "[INFO] Behavior_A. Step: 680000. Time Elapsed: 3658.255 s. Mean Reward: 4.550. Std of Reward: 4.480. Training. ELO: 1297.645.\n",
      "[INFO] Behavior_B. Step: 700000. Time Elapsed: 3716.265 s. Mean Reward: 7.357. Mean Group Reward: 0.000. Training. ELO: 1267.657.\n",
      "[INFO] Behavior_B. Step: 720000. Time Elapsed: 3768.899 s. Mean Reward: 6.600. Mean Group Reward: 0.000. Training. ELO: 1266.124.\n",
      "[INFO] Behavior_A. Step: 700000. Time Elapsed: 3822.470 s. Mean Reward: 5.167. Std of Reward: 3.979. Training. ELO: 1299.413.\n",
      "[INFO] Behavior_A. Step: 720000. Time Elapsed: 3873.332 s. Mean Reward: 8.250. Std of Reward: 2.479. Training. ELO: 1302.274.\n",
      "[INFO] Behavior_B. Step: 740000. Time Elapsed: 3929.839 s. Mean Reward: 2.962. Mean Group Reward: 0.000. Training. ELO: 1263.643.\n",
      "[INFO] Behavior_B. Step: 760000. Time Elapsed: 3984.406 s. Mean Reward: 4.312. Mean Group Reward: 0.000. Training. ELO: 1260.815.\n",
      "[INFO] Behavior_A. Step: 740000. Time Elapsed: 4036.445 s. Mean Reward: 2.575. Std of Reward: 2.198. Training. ELO: 1303.092.\n",
      "[INFO] Behavior_A. Step: 760000. Time Elapsed: 4090.460 s. Mean Reward: 6.917. Std of Reward: 3.856. Training. ELO: 1303.904.\n",
      "[INFO] Behavior_B. Step: 780000. Time Elapsed: 4145.550 s. Mean Reward: 8.000. Mean Group Reward: 0.000. Training. ELO: 1258.610.\n",
      "[INFO] Behavior_B. Step: 800000. Time Elapsed: 4197.442 s. Mean Reward: 4.375. Mean Group Reward: 0.000. Training. ELO: 1257.862.\n",
      "[INFO] Behavior_A. Step: 780000. Time Elapsed: 4253.005 s. Mean Reward: 6.222. Std of Reward: 3.575. Training. ELO: 1305.613.\n",
      "[INFO] Behavior_A. Step: 800000. Time Elapsed: 4305.809 s. Mean Reward: 3.846. Std of Reward: 4.383. Training. ELO: 1307.890.\n",
      "[INFO] Behavior_B. Step: 820000. Time Elapsed: 4361.595 s. Mean Reward: 7.583. Mean Group Reward: 0.000. Training. ELO: 1255.217.\n",
      "[INFO] Behavior_B. Step: 840000. Time Elapsed: 4414.354 s. Mean Reward: 6.143. Mean Group Reward: 0.000. Training. ELO: 1253.698.\n",
      "[INFO] Behavior_A. Step: 820000. Time Elapsed: 4467.239 s. Mean Reward: 4.889. Std of Reward: 3.703. Training. ELO: 1309.845.\n",
      "[INFO] Behavior_A. Step: 840000. Time Elapsed: 4518.724 s. Mean Reward: 6.056. Std of Reward: 5.490. Training. ELO: 1309.603.\n",
      "[INFO] Behavior_B. Step: 860000. Time Elapsed: 4576.121 s. Mean Reward: 7.250. Mean Group Reward: 0.000. Training. ELO: 1253.292.\n",
      "[INFO] Behavior_B. Step: 880000. Time Elapsed: 4630.644 s. Mean Reward: 10.250. Mean Group Reward: 0.000. Training. ELO: 1253.727.\n",
      "[INFO] Behavior_A. Step: 860000. Time Elapsed: 4684.676 s. Mean Reward: 6.071. Std of Reward: 3.639. Training. ELO: 1308.187.\n",
      "[INFO] Behavior_A. Step: 880000. Time Elapsed: 4736.613 s. Mean Reward: 7.643. Std of Reward: 6.390. Training. ELO: 1306.575.\n",
      "[INFO] Behavior_B. Step: 900000. Time Elapsed: 4793.458 s. Mean Reward: 5.056. Mean Group Reward: 0.000. Training. ELO: 1256.303.\n",
      "[INFO] Behavior_B. Step: 920000. Time Elapsed: 4848.018 s. Mean Reward: 6.714. Mean Group Reward: 0.000. Training. ELO: 1257.777.\n",
      "[INFO] Behavior_A. Step: 900000. Time Elapsed: 4896.727 s. Mean Reward: 7.917. Std of Reward: 4.137. Training. ELO: 1306.297.\n",
      "[INFO] Behavior_A. Step: 920000. Time Elapsed: 4950.121 s. Mean Reward: 8.083. Std of Reward: 3.791. Training. ELO: 1307.580.\n",
      "[INFO] Behavior_B. Step: 940000. Time Elapsed: 5007.246 s. Mean Reward: 3.417. Mean Group Reward: 0.000. Training. ELO: 1257.707.\n",
      "[INFO] Behavior_B. Step: 960000. Time Elapsed: 5063.235 s. Mean Reward: 8.286. Mean Group Reward: 0.000. Training. ELO: 1256.975.\n",
      "[INFO] Behavior_A. Step: 940000. Time Elapsed: 5112.838 s. Mean Reward: 6.700. Std of Reward: 3.400. Training. ELO: 1307.844.\n",
      "[INFO] Behavior_A. Step: 960000. Time Elapsed: 5167.504 s. Mean Reward: 8.900. Std of Reward: 3.426. Training. ELO: 1308.366.\n",
      "[INFO] Behavior_B. Step: 980000. Time Elapsed: 5222.495 s. Mean Reward: 5.333. Mean Group Reward: 0.000. Training. ELO: 1257.180.\n",
      "[INFO] Behavior_B. Step: 1000000. Time Elapsed: 5277.790 s. Mean Reward: 7.125. Mean Group Reward: 0.000. Training. ELO: 1257.186.\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_PPO_POCA/Behavior_B/Behavior_B-999880.onnx\n",
      "[INFO] Behavior_A. Step: 980000. Time Elapsed: 5326.462 s. Mean Reward: 3.688. Std of Reward: 1.731. Training. ELO: 1307.672.\n",
      "[INFO] Behavior_A. Step: 1000000. Time Elapsed: 5379.120 s. Mean Reward: 6.286. Std of Reward: 3.149. Training. ELO: 1307.361.\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_PPO_POCA/Behavior_A/Behavior_A-999118.onnx\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_PPO_POCA/Behavior_B/Behavior_B-1011376.onnx\n",
      "[INFO] Copied /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_PPO_POCA/Behavior_B/Behavior_B-1011376.onnx to /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_PPO_POCA/Behavior_B.onnx.\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_PPO_POCA/Behavior_A/Behavior_A-1000118.onnx\n",
      "[INFO] Copied /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_PPO_POCA/Behavior_A/Behavior_A-1000118.onnx to /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_PPO_POCA/Behavior_A.onnx.\n"
     ]
    }
   ],
   "source": [
    "config_poca_fp = os.path.join(cur_dir, \"config\", \"Pong_adversarial_ppo_poca.yaml\")\n",
    "run_poca_id = \"Pong_Adv_PPO_POCA\"\n",
    "print(config_poca_fp)\n",
    "print(run_poca_id)\n",
    "\n",
    "!mlagents-learn $config_poca_fp \\\n",
    "               --env=$env_fp \\\n",
    "               --results-dir=$output_dir \\\n",
    "               --run-id=$run_poca_id --base-port=$baseport"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0556fca",
   "metadata": {},
   "source": [
    "### Testing PPO-POCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e109ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            ┐  ╖\n",
      "        ╓╖╬│╡  ││╬╖╖\n",
      "    ╓╖╬│││││┘  ╬│││││╬╖\n",
      " ╖╬│││││╬╜        ╙╬│││││╖╖                               ╗╗╗\n",
      " ╬╬╬╬╖││╦╖        ╖╬││╗╣╣╣╬      ╟╣╣╬    ╟╣╣╣             ╜╜╜  ╟╣╣\n",
      " ╬╬╬╬╬╬╬╬╖│╬╖╖╓╬╪│╓╣╣╣╣╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╒╣╣╖╗╣╣╣╗   ╣╣╣ ╣╣╣╣╣╣ ╟╣╣╖   ╣╣╣\n",
      " ╬╬╬╬┐  ╙╬╬╬╬│╓╣╣╣╝╜  ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╣╙ ╙╣╣╣  ╣╣╣ ╙╟╣╣╜╙  ╫╣╣  ╟╣╣\n",
      " ╬╬╬╬┐     ╙╬╬╣╣      ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣     ╣╣╣┌╣╣╜\n",
      " ╬╬╬╜       ╬╬╣╣      ╙╝╣╣╬      ╙╣╣╣╗╖╓╗╣╣╣╜ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣╦╓    ╣╣╣╣╣\n",
      " ╙   ╓╦╖    ╬╬╣╣   ╓╗╗╖            ╙╝╣╣╣╣╝╜   ╘╝╝╜   ╝╝╝  ╝╝╝   ╙╣╣╣    ╟╣╣╣\n",
      "   ╩╬╬╬╬╬╬╦╦╬╬╣╣╗╣╣╣╣╣╣╣╝                                             ╫╣╣╣╣\n",
      "      ╙╬╬╬╬╬╬╬╣╣╣╣╣╣╝╜\n",
      "          ╙╬╬╬╣╣╣╜\n",
      "             ╙\n",
      "        \n",
      " Version information:\n",
      "  ml-agents: 1.1.0,\n",
      "  ml-agents-envs: 1.1.0,\n",
      "  Communicator API: 1.5.0,\n",
      "  PyTorch: 2.8.0\n",
      "[INFO] Connected to Unity environment with package version 4.0.0 and communication version 1.5.0\n",
      "[INFO] Connected new brain: Behavior_B?team=1\n",
      "[INFO] Connected new brain: Behavior_A?team=0\n",
      "[INFO] Hyperparameters for behavior name Behavior_A: \n",
      "\ttrainer_type:\tppo\n",
      "\thyperparameters:\t\n",
      "\t  batch_size:\t128\n",
      "\t  buffer_size:\t1280\n",
      "\t  learning_rate:\t0.0003\n",
      "\t  beta:\t0.005\n",
      "\t  epsilon:\t0.2\n",
      "\t  lambd:\t0.95\n",
      "\t  num_epoch:\t3\n",
      "\t  shared_critic:\tFalse\n",
      "\t  learning_rate_schedule:\tlinear\n",
      "\t  beta_schedule:\tlinear\n",
      "\t  epsilon_schedule:\tlinear\n",
      "\tcheckpoint_interval:\t500000\n",
      "\tnetwork_settings:\t\n",
      "\t  normalize:\tTrue\n",
      "\t  hidden_units:\t128\n",
      "\t  num_layers:\t2\n",
      "\t  vis_encode_type:\tsimple\n",
      "\t  memory:\tNone\n",
      "\t  goal_conditioning_type:\thyper\n",
      "\t  deterministic:\tFalse\n",
      "\treward_signals:\t\n",
      "\t  extrinsic:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t1.0\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t128\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\tinit_path:\tNone\n",
      "\tkeep_checkpoints:\t5\n",
      "\teven_checkpoints:\tFalse\n",
      "\tmax_steps:\t1000000\n",
      "\ttime_horizon:\t1000\n",
      "\tsummary_freq:\t20000\n",
      "\tthreaded:\tFalse\n",
      "\tself_play:\t\n",
      "\t  save_steps:\t10000\n",
      "\t  team_change:\t40000\n",
      "\t  swap_steps:\t10000\n",
      "\t  window:\t10\n",
      "\t  play_against_latest_model_ratio:\t0.5\n",
      "\t  initial_elo:\t1200.0\n",
      "\tbehavioral_cloning:\tNone\n",
      "[INFO] Resuming from /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_PPO_POCA/Behavior_A.\n",
      "[INFO] Resuming training from step 1000118.\n",
      "[INFO] Hyperparameters for behavior name Behavior_B: \n",
      "\ttrainer_type:\tpoca\n",
      "\thyperparameters:\t\n",
      "\t  batch_size:\t128\n",
      "\t  buffer_size:\t1280\n",
      "\t  learning_rate:\t0.0003\n",
      "\t  beta:\t0.005\n",
      "\t  epsilon:\t0.2\n",
      "\t  lambd:\t0.95\n",
      "\t  num_epoch:\t3\n",
      "\t  learning_rate_schedule:\tlinear\n",
      "\t  beta_schedule:\tlinear\n",
      "\t  epsilon_schedule:\tlinear\n",
      "\tcheckpoint_interval:\t500000\n",
      "\tnetwork_settings:\t\n",
      "\t  normalize:\tTrue\n",
      "\t  hidden_units:\t128\n",
      "\t  num_layers:\t2\n",
      "\t  vis_encode_type:\tsimple\n",
      "\t  memory:\tNone\n",
      "\t  goal_conditioning_type:\thyper\n",
      "\t  deterministic:\tFalse\n",
      "\treward_signals:\t\n",
      "\t  extrinsic:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t1.0\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t128\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\tinit_path:\tNone\n",
      "\tkeep_checkpoints:\t5\n",
      "\teven_checkpoints:\tFalse\n",
      "\tmax_steps:\t1000000\n",
      "\ttime_horizon:\t1000\n",
      "\tsummary_freq:\t20000\n",
      "\tthreaded:\tFalse\n",
      "\tself_play:\t\n",
      "\t  save_steps:\t10000\n",
      "\t  team_change:\t40000\n",
      "\t  swap_steps:\t10000\n",
      "\t  window:\t10\n",
      "\t  play_against_latest_model_ratio:\t0.5\n",
      "\t  initial_elo:\t1200.0\n",
      "\tbehavioral_cloning:\tNone\n",
      "[INFO] Resuming from /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_PPO_POCA/Behavior_B.\n",
      "[INFO] Resuming training from step 1011376.\n",
      "^C\n",
      "Exception ignored in atexit callback: <function _exit_function at 0x13f80eb90>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/multiprocessing/util.py\", line 357, in _exit_function\n",
      "    p.join()\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/multiprocessing/popen_fork.py\", line 43, in wait\n",
      "    return self.poll(os.WNOHANG if timeout == 0.0 else 0)\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/multiprocessing/popen_fork.py\", line 27, in poll\n",
      "    pid, sts = os.waitpid(self.pid, flag)\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "# config_poca_fp = os.path.join(cur_dir, \"config\", \"Pong_adversarial_ppo_poca.yaml\")\n",
    "# run_poca_id = \"Pong_Adv_PPO_POCA\"\n",
    "# !mlagents-learn $config_poca_fp \\\n",
    "#                --env=$env_fp \\\n",
    "#                --results-dir=$output_dir \\\n",
    "#                --run-id=$run_poca_id --base-port=$baseport --resume --inference --time-scale=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eea4b5e",
   "metadata": {},
   "source": [
    "## Training SAC-POCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47866e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/config/Pong_adversarial_sac_poca.yaml\n",
      "Pong_Adv_SAC_POCA\n",
      "\n",
      "            ┐  ╖\n",
      "        ╓╖╬│╡  ││╬╖╖\n",
      "    ╓╖╬│││││┘  ╬│││││╬╖\n",
      " ╖╬│││││╬╜        ╙╬│││││╖╖                               ╗╗╗\n",
      " ╬╬╬╬╖││╦╖        ╖╬││╗╣╣╣╬      ╟╣╣╬    ╟╣╣╣             ╜╜╜  ╟╣╣\n",
      " ╬╬╬╬╬╬╬╬╖│╬╖╖╓╬╪│╓╣╣╣╣╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╒╣╣╖╗╣╣╣╗   ╣╣╣ ╣╣╣╣╣╣ ╟╣╣╖   ╣╣╣\n",
      " ╬╬╬╬┐  ╙╬╬╬╬│╓╣╣╣╝╜  ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╣╙ ╙╣╣╣  ╣╣╣ ╙╟╣╣╜╙  ╫╣╣  ╟╣╣\n",
      " ╬╬╬╬┐     ╙╬╬╣╣      ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣     ╣╣╣┌╣╣╜\n",
      " ╬╬╬╜       ╬╬╣╣      ╙╝╣╣╬      ╙╣╣╣╗╖╓╗╣╣╣╜ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣╦╓    ╣╣╣╣╣\n",
      " ╙   ╓╦╖    ╬╬╣╣   ╓╗╗╖            ╙╝╣╣╣╣╝╜   ╘╝╝╜   ╝╝╝  ╝╝╝   ╙╣╣╣    ╟╣╣╣\n",
      "   ╩╬╬╬╬╬╬╦╦╬╬╣╣╗╣╣╣╣╣╣╣╝                                             ╫╣╣╣╣\n",
      "      ╙╬╬╬╬╬╬╬╣╣╣╣╣╣╝╜\n",
      "          ╙╬╬╬╣╣╣╜\n",
      "             ╙\n",
      "        \n",
      " Version information:\n",
      "  ml-agents: 1.1.0,\n",
      "  ml-agents-envs: 1.1.0,\n",
      "  Communicator API: 1.5.0,\n",
      "  PyTorch: 2.8.0\n",
      "[INFO] Connected to Unity environment with package version 4.0.0 and communication version 1.5.0\n",
      "[INFO] Connected new brain: Behavior_B?team=1\n",
      "[INFO] Connected new brain: Behavior_A?team=0\n",
      "[INFO] Hyperparameters for behavior name Behavior_B: \n",
      "\ttrainer_type:\tpoca\n",
      "\thyperparameters:\t\n",
      "\t  batch_size:\t128\n",
      "\t  buffer_size:\t1280\n",
      "\t  learning_rate:\t0.0003\n",
      "\t  beta:\t0.005\n",
      "\t  epsilon:\t0.2\n",
      "\t  lambd:\t0.95\n",
      "\t  num_epoch:\t3\n",
      "\t  learning_rate_schedule:\tlinear\n",
      "\t  beta_schedule:\tlinear\n",
      "\t  epsilon_schedule:\tlinear\n",
      "\tcheckpoint_interval:\t500000\n",
      "\tnetwork_settings:\t\n",
      "\t  normalize:\tTrue\n",
      "\t  hidden_units:\t128\n",
      "\t  num_layers:\t2\n",
      "\t  vis_encode_type:\tsimple\n",
      "\t  memory:\tNone\n",
      "\t  goal_conditioning_type:\thyper\n",
      "\t  deterministic:\tFalse\n",
      "\treward_signals:\t\n",
      "\t  extrinsic:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t1.0\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t128\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\tinit_path:\tNone\n",
      "\tkeep_checkpoints:\t5\n",
      "\teven_checkpoints:\tFalse\n",
      "\tmax_steps:\t1000000\n",
      "\ttime_horizon:\t1000\n",
      "\tsummary_freq:\t20000\n",
      "\tthreaded:\tFalse\n",
      "\tself_play:\t\n",
      "\t  save_steps:\t10000\n",
      "\t  team_change:\t40000\n",
      "\t  swap_steps:\t10000\n",
      "\t  window:\t10\n",
      "\t  play_against_latest_model_ratio:\t0.5\n",
      "\t  initial_elo:\t1200.0\n",
      "\tbehavioral_cloning:\tNone\n",
      "[INFO] Hyperparameters for behavior name Behavior_A: \n",
      "\ttrainer_type:\tsac\n",
      "\thyperparameters:\t\n",
      "\t  learning_rate:\t0.0003\n",
      "\t  learning_rate_schedule:\tlinear\n",
      "\t  batch_size:\t128\n",
      "\t  buffer_size:\t1280\n",
      "\t  buffer_init_steps:\t0\n",
      "\t  tau:\t0.005\n",
      "\t  steps_per_update:\t10.0\n",
      "\t  save_replay_buffer:\tFalse\n",
      "\t  init_entcoef:\t0.5\n",
      "\t  reward_signal_steps_per_update:\t10.0\n",
      "\tcheckpoint_interval:\t500000\n",
      "\tnetwork_settings:\t\n",
      "\t  normalize:\tTrue\n",
      "\t  hidden_units:\t128\n",
      "\t  num_layers:\t2\n",
      "\t  vis_encode_type:\tsimple\n",
      "\t  memory:\tNone\n",
      "\t  goal_conditioning_type:\thyper\n",
      "\t  deterministic:\tFalse\n",
      "\treward_signals:\t\n",
      "\t  extrinsic:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t1.0\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t128\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\tinit_path:\tNone\n",
      "\tkeep_checkpoints:\t5\n",
      "\teven_checkpoints:\tFalse\n",
      "\tmax_steps:\t1000000\n",
      "\ttime_horizon:\t1000\n",
      "\tsummary_freq:\t20000\n",
      "\tthreaded:\tFalse\n",
      "\tself_play:\t\n",
      "\t  save_steps:\t10000\n",
      "\t  team_change:\t40000\n",
      "\t  swap_steps:\t10000\n",
      "\t  window:\t10\n",
      "\t  play_against_latest_model_ratio:\t0.5\n",
      "\t  initial_elo:\t1200.0\n",
      "\tbehavioral_cloning:\tNone\n",
      "[INFO] Behavior_B. Step: 20000. Time Elapsed: 60.732 s. Mean Reward: 0.098. Mean Group Reward: 0.000. Training. ELO: 1198.383.\n",
      "[INFO] Behavior_B. Step: 40000. Time Elapsed: 115.407 s. Mean Reward: 0.190. Mean Group Reward: 0.000. Training. ELO: 1201.720.\n",
      "/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/site-packages/mlagents/trainers/torch_entities/utils.py:289: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:4424.)\n",
      "  torch.nn.functional.one_hot(_act.T, action_size[i]).float()\n",
      "[INFO] Behavior_A. Step: 20000. Time Elapsed: 181.877 s. Mean Reward: -0.075. Std of Reward: 1.064. Training. ELO: 1194.186.\n",
      "[INFO] Behavior_A. Step: 40000. Time Elapsed: 248.526 s. Mean Reward: -0.041. Std of Reward: 1.047. Training. ELO: 1177.306.\n",
      "[INFO] Behavior_B. Step: 60000. Time Elapsed: 303.323 s. Mean Reward: 0.568. Mean Group Reward: 0.000. Training. ELO: 1229.185.\n",
      "[INFO] Behavior_B. Step: 80000. Time Elapsed: 358.536 s. Mean Reward: 0.961. Mean Group Reward: 0.000. Training. ELO: 1264.104.\n",
      "[INFO] Behavior_A. Step: 60000. Time Elapsed: 424.533 s. Mean Reward: -0.321. Std of Reward: 0.977. Training. ELO: 1130.685.\n",
      "[INFO] Behavior_A. Step: 80000. Time Elapsed: 490.873 s. Mean Reward: 0.139. Std of Reward: 1.101. Training. ELO: 1140.450.\n",
      "[INFO] Behavior_B. Step: 100000. Time Elapsed: 546.558 s. Mean Reward: 0.967. Mean Group Reward: 0.000. Training. ELO: 1286.286.\n",
      "[INFO] Behavior_B. Step: 120000. Time Elapsed: 600.847 s. Mean Reward: 1.088. Mean Group Reward: 0.000. Training. ELO: 1308.373.\n",
      "[INFO] Behavior_A. Step: 100000. Time Elapsed: 667.948 s. Mean Reward: -0.140. Std of Reward: 1.382. Training. ELO: 1131.658.\n",
      "[INFO] Behavior_A. Step: 120000. Time Elapsed: 732.196 s. Mean Reward: 0.195. Std of Reward: 1.602. Training. ELO: 1127.490.\n",
      "[INFO] Behavior_B. Step: 140000. Time Elapsed: 787.676 s. Mean Reward: 1.224. Mean Group Reward: 0.000. Training. ELO: 1324.886.\n",
      "[INFO] Behavior_B. Step: 160000. Time Elapsed: 842.809 s. Mean Reward: 1.256. Mean Group Reward: 0.000. Training. ELO: 1344.494.\n",
      "[INFO] Behavior_A. Step: 140000. Time Elapsed: 907.906 s. Mean Reward: -0.466. Std of Reward: 1.166. Training. ELO: 1122.409.\n",
      "[INFO] Behavior_A. Step: 160000. Time Elapsed: 973.883 s. Mean Reward: 0.529. Std of Reward: 2.373. Training. ELO: 1113.892.\n",
      "[INFO] Behavior_B. Step: 180000. Time Elapsed: 1029.501 s. Mean Reward: 1.673. Mean Group Reward: 0.000. Training. ELO: 1363.843.\n",
      "[INFO] Behavior_B. Step: 200000. Time Elapsed: 1083.823 s. Mean Reward: 1.020. Mean Group Reward: 0.000. Training. ELO: 1364.712.\n",
      "[INFO] Behavior_A. Step: 180000. Time Elapsed: 1148.839 s. Mean Reward: 0.398. Std of Reward: 1.959. Training. ELO: 1114.996.\n",
      "[INFO] Behavior_A. Step: 200000. Time Elapsed: 1215.651 s. Mean Reward: 0.580. Std of Reward: 2.492. Training. ELO: 1126.776.\n",
      "[INFO] Behavior_B. Step: 220000. Time Elapsed: 1270.047 s. Mean Reward: 1.629. Mean Group Reward: 0.000. Training. ELO: 1358.809.\n",
      "[INFO] Behavior_B. Step: 240000. Time Elapsed: 1324.375 s. Mean Reward: 1.911. Mean Group Reward: 0.000. Training. ELO: 1356.181.\n",
      "[INFO] Behavior_A. Step: 220000. Time Elapsed: 1390.488 s. Mean Reward: 1.040. Std of Reward: 2.634. Training. ELO: 1133.056.\n",
      "[INFO] Behavior_A. Step: 240000. Time Elapsed: 1455.044 s. Mean Reward: 1.304. Std of Reward: 3.074. Training. ELO: 1132.726.\n",
      "[INFO] Behavior_B. Step: 260000. Time Elapsed: 1512.664 s. Mean Reward: 2.044. Mean Group Reward: 0.000. Training. ELO: 1362.854.\n",
      "[INFO] Behavior_B. Step: 280000. Time Elapsed: 1566.705 s. Mean Reward: 2.221. Mean Group Reward: 0.000. Training. ELO: 1368.053.\n",
      "[INFO] Behavior_A. Step: 260000. Time Elapsed: 1630.521 s. Mean Reward: 0.889. Std of Reward: 2.888. Training. ELO: 1121.986.\n",
      "[INFO] Behavior_A. Step: 280000. Time Elapsed: 1695.833 s. Mean Reward: 0.362. Std of Reward: 2.437. Training. ELO: 1117.273.\n",
      "[INFO] Behavior_B. Step: 300000. Time Elapsed: 1754.624 s. Mean Reward: 1.688. Mean Group Reward: 0.000. Training. ELO: 1382.417.\n",
      "[INFO] Behavior_B. Step: 320000. Time Elapsed: 1808.912 s. Mean Reward: 1.989. Mean Group Reward: 0.000. Training. ELO: 1390.340.\n",
      "[INFO] Behavior_A. Step: 300000. Time Elapsed: 1870.896 s. Mean Reward: 5.556. Std of Reward: 4.153. Training. ELO: 1114.378.\n",
      "[INFO] Behavior_A. Step: 320000. Time Elapsed: 1936.746 s. Mean Reward: 0.008. Std of Reward: 1.500. Training. ELO: 1115.216.\n",
      "[INFO] Behavior_B. Step: 340000. Time Elapsed: 1996.809 s. Mean Reward: 2.038. Mean Group Reward: 0.000. Training. ELO: 1401.283.\n",
      "[INFO] Behavior_B. Step: 360000. Time Elapsed: 2049.164 s. Mean Reward: 2.648. Mean Group Reward: 0.000. Training. ELO: 1405.686.\n",
      "[INFO] Behavior_A. Step: 340000. Time Elapsed: 2113.355 s. Mean Reward: 0.796. Std of Reward: 3.065. Training. ELO: 1105.774.\n",
      "[INFO] Behavior_A. Step: 360000. Time Elapsed: 2178.863 s. Mean Reward: 3.650. Std of Reward: 2.855. Training. ELO: 1104.377.\n",
      "[INFO] Behavior_B. Step: 380000. Time Elapsed: 2235.024 s. Mean Reward: 2.759. Mean Group Reward: 0.000. Training. ELO: 1405.585.\n",
      "[INFO] Behavior_B. Step: 400000. Time Elapsed: 2290.225 s. Mean Reward: 1.795. Mean Group Reward: 0.000. Training. ELO: 1407.766.\n",
      "[INFO] Behavior_A. Step: 380000. Time Elapsed: 2354.558 s. Mean Reward: 3.393. Std of Reward: 4.294. Training. ELO: 1106.365.\n",
      "[INFO] Behavior_A. Step: 400000. Time Elapsed: 2420.075 s. Mean Reward: 0.932. Std of Reward: 2.862. Training. ELO: 1107.603.\n",
      "[INFO] Behavior_B. Step: 420000. Time Elapsed: 2477.774 s. Mean Reward: 1.534. Mean Group Reward: 0.000. Training. ELO: 1412.526.\n",
      "[INFO] Behavior_B. Step: 440000. Time Elapsed: 2531.994 s. Mean Reward: 3.068. Mean Group Reward: 0.000. Training. ELO: 1414.002.\n",
      "[INFO] Behavior_A. Step: 420000. Time Elapsed: 2596.927 s. Mean Reward: 1.750. Std of Reward: 4.120. Training. ELO: 1109.961.\n",
      "[INFO] Behavior_A. Step: 440000. Time Elapsed: 2662.032 s. Mean Reward: 0.167. Std of Reward: 1.556. Training. ELO: 1110.383.\n",
      "[INFO] Behavior_B. Step: 460000. Time Elapsed: 2721.622 s. Mean Reward: 2.204. Mean Group Reward: 0.000. Training. ELO: 1411.895.\n",
      "[INFO] Behavior_B. Step: 480000. Time Elapsed: 2774.458 s. Mean Reward: 2.261. Mean Group Reward: 0.000. Training. ELO: 1415.168.\n",
      "[INFO] Behavior_A. Step: 460000. Time Elapsed: 2838.579 s. Mean Reward: 0.518. Std of Reward: 1.617. Training. ELO: 1106.433.\n",
      "[INFO] Behavior_A. Step: 480000. Time Elapsed: 2904.533 s. Mean Reward: -0.451. Std of Reward: 0.945. Training. ELO: 1099.442.\n",
      "[INFO] Behavior_B. Step: 500000. Time Elapsed: 2964.045 s. Mean Reward: 1.678. Mean Group Reward: 0.000. Training. ELO: 1425.486.\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_SAC_POCA/Behavior_B/Behavior_B-499542.onnx\n",
      "[INFO] Behavior_B. Step: 520000. Time Elapsed: 3017.335 s. Mean Reward: 1.950. Mean Group Reward: 0.000. Training. ELO: 1432.823.\n",
      "[INFO] Behavior_A. Step: 500000. Time Elapsed: 3079.924 s. Mean Reward: -0.397. Std of Reward: 0.766. Training. ELO: 1088.072.\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_SAC_POCA/Behavior_A/Behavior_A-499849.onnx\n",
      "[INFO] Behavior_A. Step: 520000. Time Elapsed: 3145.927 s. Mean Reward: 0.796. Std of Reward: 3.116. Training. ELO: 1084.926.\n",
      "[INFO] Behavior_B. Step: 540000. Time Elapsed: 3205.736 s. Mean Reward: 3.294. Mean Group Reward: 0.000. Training. ELO: 1434.825.\n",
      "[INFO] Behavior_B. Step: 560000. Time Elapsed: 3259.755 s. Mean Reward: 2.000. Mean Group Reward: 0.000. Training. ELO: 1439.637.\n",
      "[INFO] Behavior_A. Step: 540000. Time Elapsed: 3320.333 s. Mean Reward: 8.625. Std of Reward: 2.945. Training. ELO: 1087.143.\n",
      "[INFO] Behavior_A. Step: 560000. Time Elapsed: 3385.686 s. Mean Reward: 6.375. Std of Reward: 4.980. Training. ELO: 1088.176.\n",
      "[INFO] Behavior_B. Step: 580000. Time Elapsed: 3446.574 s. Mean Reward: 2.224. Mean Group Reward: 0.000. Training. ELO: 1441.464.\n",
      "[INFO] Behavior_B. Step: 600000. Time Elapsed: 3501.761 s. Mean Reward: 3.222. Mean Group Reward: 0.000. Training. ELO: 1439.182.\n",
      "[INFO] Behavior_A. Step: 580000. Time Elapsed: 3562.445 s. Mean Reward: 3.909. Std of Reward: 4.738. Training. ELO: 1090.267.\n",
      "[INFO] Behavior_A. Step: 600000. Time Elapsed: 3625.742 s. Mean Reward: 8.583. Std of Reward: 4.004. Training. ELO: 1093.242.\n",
      "[INFO] Behavior_B. Step: 620000. Time Elapsed: 3687.917 s. Mean Reward: 3.200. Mean Group Reward: 0.000. Training. ELO: 1432.856.\n",
      "[INFO] Behavior_B. Step: 640000. Time Elapsed: 3744.133 s. Mean Reward: 2.932. Mean Group Reward: 0.000. Training. ELO: 1432.140.\n",
      "[INFO] Behavior_A. Step: 620000. Time Elapsed: 3802.368 s. Mean Reward: 7.083. Std of Reward: 4.495. Training. ELO: 1095.426.\n",
      "[INFO] Behavior_A. Step: 640000. Time Elapsed: 3869.865 s. Mean Reward: 4.200. Std of Reward: 4.122. Training. ELO: 1100.158.\n",
      "[INFO] Behavior_B. Step: 660000. Time Elapsed: 3931.064 s. Mean Reward: 3.458. Mean Group Reward: 0.000. Training. ELO: 1425.884.\n",
      "[INFO] Behavior_B. Step: 680000. Time Elapsed: 3986.054 s. Mean Reward: 8.500. Mean Group Reward: 0.000. Training. ELO: 1422.601.\n",
      "[INFO] Behavior_A. Step: 660000. Time Elapsed: 4045.649 s. Mean Reward: 4.650. Std of Reward: 4.822. Training. ELO: 1112.286.\n",
      "[INFO] Behavior_A. Step: 680000. Time Elapsed: 4110.678 s. Mean Reward: 2.417. Std of Reward: 4.243. Training. ELO: 1114.971.\n",
      "[INFO] Behavior_B. Step: 700000. Time Elapsed: 4170.644 s. Mean Reward: 2.538. Mean Group Reward: 0.000. Training. ELO: 1416.560.\n",
      "[INFO] Behavior_B. Step: 720000. Time Elapsed: 4226.411 s. Mean Reward: 3.196. Mean Group Reward: 0.000. Training. ELO: 1419.372.\n",
      "[INFO] Behavior_A. Step: 700000. Time Elapsed: 4287.456 s. Mean Reward: 4.100. Std of Reward: 4.598. Training. ELO: 1113.545.\n",
      "[INFO] Behavior_A. Step: 720000. Time Elapsed: 4353.879 s. Mean Reward: 0.960. Std of Reward: 3.130. Training. ELO: 1111.191.\n",
      "[INFO] Behavior_B. Step: 740000. Time Elapsed: 4411.385 s. Mean Reward: 4.143. Mean Group Reward: 0.000. Training. ELO: 1423.556.\n",
      "[INFO] Behavior_B. Step: 760000. Time Elapsed: 4468.653 s. Mean Reward: 5.727. Mean Group Reward: 0.000. Training. ELO: 1424.033.\n",
      "[INFO] Behavior_A. Step: 740000. Time Elapsed: 4531.213 s. Mean Reward: 5.333. Std of Reward: 4.546. Training. ELO: 1109.693.\n",
      "[INFO] Behavior_A. Step: 760000. Time Elapsed: 4594.993 s. Mean Reward: 2.474. Std of Reward: 3.868. Training. ELO: 1111.034.\n",
      "[INFO] Behavior_B. Step: 780000. Time Elapsed: 4654.253 s. Mean Reward: 4.600. Mean Group Reward: 0.000. Training. ELO: 1423.718.\n",
      "[INFO] Behavior_B. Step: 800000. Time Elapsed: 4708.125 s. Mean Reward: 3.071. Mean Group Reward: 0.000. Training. ELO: 1421.008.\n",
      "[INFO] Behavior_A. Step: 780000. Time Elapsed: 4771.180 s. Mean Reward: 7.200. Std of Reward: 4.643. Training. ELO: 1114.562.\n",
      "[INFO] Behavior_A. Step: 800000. Time Elapsed: 4838.910 s. Mean Reward: 5.500. Std of Reward: 4.613. Training. ELO: 1115.299.\n",
      "[INFO] Behavior_B. Step: 820000. Time Elapsed: 4894.456 s. Mean Reward: 2.917. Mean Group Reward: 0.000. Training. ELO: 1417.118.\n",
      "[INFO] Behavior_B. Step: 840000. Time Elapsed: 4948.758 s. Mean Reward: 6.786. Mean Group Reward: 0.000. Training. ELO: 1415.442.\n",
      "[INFO] Behavior_A. Step: 820000. Time Elapsed: 5013.939 s. Mean Reward: 4.700. Std of Reward: 4.302. Training. ELO: 1118.393.\n",
      "[INFO] Behavior_A. Step: 840000. Time Elapsed: 5079.565 s. Mean Reward: 0.333. Std of Reward: 2.914. Training. ELO: 1119.209.\n",
      "[INFO] Behavior_B. Step: 860000. Time Elapsed: 5135.580 s. Mean Reward: 1.931. Mean Group Reward: 0.000. Training. ELO: 1413.865.\n",
      "[INFO] Behavior_B. Step: 880000. Time Elapsed: 5191.422 s. Mean Reward: 1.818. Mean Group Reward: 0.000. Training. ELO: 1420.483.\n",
      "[INFO] Behavior_A. Step: 860000. Time Elapsed: 5255.826 s. Mean Reward: 0.683. Std of Reward: 2.631. Training. ELO: 1109.973.\n",
      "[INFO] Behavior_A. Step: 880000. Time Elapsed: 5321.178 s. Mean Reward: 0.581. Std of Reward: 2.566. Training. ELO: 1107.035.\n",
      "[INFO] Behavior_B. Step: 900000. Time Elapsed: 5380.480 s. Mean Reward: 7.500. Mean Group Reward: 0.000. Training. ELO: 1423.780.\n",
      "[INFO] Behavior_B. Step: 920000. Time Elapsed: 5435.538 s. Mean Reward: 7.417. Mean Group Reward: 0.000. Training. ELO: 1422.543.\n",
      "[INFO] Behavior_A. Step: 900000. Time Elapsed: 5495.848 s. Mean Reward: 9.000. Std of Reward: 5.074. Training. ELO: 1107.345.\n",
      "[INFO] Behavior_A. Step: 920000. Time Elapsed: 5561.327 s. Mean Reward: 12.625. Std of Reward: 0.650. Training.\n",
      "[INFO] Behavior_B. Step: 940000. Time Elapsed: 5620.225 s. Mean Reward: 4.321. Mean Group Reward: 0.000. Training. ELO: 1419.740.\n",
      "[INFO] Behavior_B. Step: 960000. Time Elapsed: 5677.529 s. Mean Reward: 1.880. Mean Group Reward: 0.000. Training. ELO: 1423.114.\n",
      "[INFO] Behavior_A. Step: 940000. Time Elapsed: 5740.154 s. Mean Reward: 7.500. Std of Reward: 5.470. Training. ELO: 1113.056.\n",
      "[INFO] Behavior_A. Step: 960000. Time Elapsed: 5806.145 s. Mean Reward: 5.278. Std of Reward: 5.094. Training. ELO: 1113.883.\n",
      "[INFO] Behavior_B. Step: 980000. Time Elapsed: 5863.136 s. Mean Reward: 11.250. Mean Group Reward: 0.000. Training.\n",
      "[INFO] Behavior_B. Step: 1000000. Time Elapsed: 5917.125 s. Mean Reward: 10.800. Mean Group Reward: 0.000. Training. ELO: 1424.603.\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_SAC_POCA/Behavior_B/Behavior_B-999369.onnx\n",
      "[INFO] Behavior_A. Step: 980000. Time Elapsed: 5981.544 s. Mean Reward: 4.429. Std of Reward: 5.858. Training. ELO: 1114.896.\n",
      "[INFO] Behavior_A. Step: 1000000. Time Elapsed: 6046.813 s. Mean Reward: 10.300. Std of Reward: 3.400. Training. ELO: 1115.607.\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_SAC_POCA/Behavior_A/Behavior_A-999549.onnx\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_SAC_POCA/Behavior_B/Behavior_B-1010369.onnx\n",
      "[INFO] Copied /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_SAC_POCA/Behavior_B/Behavior_B-1010369.onnx to /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_SAC_POCA/Behavior_B.onnx.\n",
      "[INFO] Exported /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_SAC_POCA/Behavior_A/Behavior_A-1000549.onnx\n",
      "[INFO] Copied /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_SAC_POCA/Behavior_A/Behavior_A-1000549.onnx to /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_SAC_POCA/Behavior_A.onnx.\n"
     ]
    }
   ],
   "source": [
    "config_poca_fp = os.path.join(cur_dir, \"config\", \"Pong_adversarial_sac_poca.yaml\")\n",
    "run_poca_id = \"Pong_Adv_SAC_POCA\"\n",
    "print(config_poca_fp)\n",
    "print(run_poca_id)\n",
    "\n",
    "!mlagents-learn $config_poca_fp \\\n",
    "               --env=$env_fp \\\n",
    "               --results-dir=$output_dir \\\n",
    "               --run-id=$run_poca_id --base-port=$baseport"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70319f5",
   "metadata": {},
   "source": [
    "### Testing SAC-POCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4279112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            ┐  ╖\n",
      "        ╓╖╬│╡  ││╬╖╖\n",
      "    ╓╖╬│││││┘  ╬│││││╬╖\n",
      " ╖╬│││││╬╜        ╙╬│││││╖╖                               ╗╗╗\n",
      " ╬╬╬╬╖││╦╖        ╖╬││╗╣╣╣╬      ╟╣╣╬    ╟╣╣╣             ╜╜╜  ╟╣╣\n",
      " ╬╬╬╬╬╬╬╬╖│╬╖╖╓╬╪│╓╣╣╣╣╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╒╣╣╖╗╣╣╣╗   ╣╣╣ ╣╣╣╣╣╣ ╟╣╣╖   ╣╣╣\n",
      " ╬╬╬╬┐  ╙╬╬╬╬│╓╣╣╣╝╜  ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╣╙ ╙╣╣╣  ╣╣╣ ╙╟╣╣╜╙  ╫╣╣  ╟╣╣\n",
      " ╬╬╬╬┐     ╙╬╬╣╣      ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣     ╣╣╣┌╣╣╜\n",
      " ╬╬╬╜       ╬╬╣╣      ╙╝╣╣╬      ╙╣╣╣╗╖╓╗╣╣╣╜ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣╦╓    ╣╣╣╣╣\n",
      " ╙   ╓╦╖    ╬╬╣╣   ╓╗╗╖            ╙╝╣╣╣╣╝╜   ╘╝╝╜   ╝╝╝  ╝╝╝   ╙╣╣╣    ╟╣╣╣\n",
      "   ╩╬╬╬╬╬╬╦╦╬╬╣╣╗╣╣╣╣╣╣╣╝                                             ╫╣╣╣╣\n",
      "      ╙╬╬╬╬╬╬╬╣╣╣╣╣╣╝╜\n",
      "          ╙╬╬╬╣╣╣╜\n",
      "             ╙\n",
      "        \n",
      " Version information:\n",
      "  ml-agents: 1.1.0,\n",
      "  ml-agents-envs: 1.1.0,\n",
      "  Communicator API: 1.5.0,\n",
      "  PyTorch: 2.8.0\n",
      "[INFO] Connected to Unity environment with package version 4.0.0 and communication version 1.5.0\n",
      "[INFO] Connected new brain: Behavior_B?team=1\n",
      "[INFO] Connected new brain: Behavior_A?team=0\n",
      "[INFO] Hyperparameters for behavior name Behavior_B: \n",
      "\ttrainer_type:\tpoca\n",
      "\thyperparameters:\t\n",
      "\t  batch_size:\t128\n",
      "\t  buffer_size:\t1280\n",
      "\t  learning_rate:\t0.0003\n",
      "\t  beta:\t0.005\n",
      "\t  epsilon:\t0.2\n",
      "\t  lambd:\t0.95\n",
      "\t  num_epoch:\t3\n",
      "\t  learning_rate_schedule:\tlinear\n",
      "\t  beta_schedule:\tlinear\n",
      "\t  epsilon_schedule:\tlinear\n",
      "\tcheckpoint_interval:\t500000\n",
      "\tnetwork_settings:\t\n",
      "\t  normalize:\tTrue\n",
      "\t  hidden_units:\t128\n",
      "\t  num_layers:\t2\n",
      "\t  vis_encode_type:\tsimple\n",
      "\t  memory:\tNone\n",
      "\t  goal_conditioning_type:\thyper\n",
      "\t  deterministic:\tFalse\n",
      "\treward_signals:\t\n",
      "\t  extrinsic:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t1.0\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t128\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\tinit_path:\tNone\n",
      "\tkeep_checkpoints:\t5\n",
      "\teven_checkpoints:\tFalse\n",
      "\tmax_steps:\t1000000\n",
      "\ttime_horizon:\t1000\n",
      "\tsummary_freq:\t20000\n",
      "\tthreaded:\tFalse\n",
      "\tself_play:\t\n",
      "\t  save_steps:\t10000\n",
      "\t  team_change:\t40000\n",
      "\t  swap_steps:\t10000\n",
      "\t  window:\t10\n",
      "\t  play_against_latest_model_ratio:\t0.5\n",
      "\t  initial_elo:\t1200.0\n",
      "\tbehavioral_cloning:\tNone\n",
      "[INFO] Resuming from /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_SAC_POCA/Behavior_B.\n",
      "[INFO] Resuming training from step 1010369.\n",
      "[INFO] Hyperparameters for behavior name Behavior_A: \n",
      "\ttrainer_type:\tsac\n",
      "\thyperparameters:\t\n",
      "\t  learning_rate:\t0.0003\n",
      "\t  learning_rate_schedule:\tlinear\n",
      "\t  batch_size:\t128\n",
      "\t  buffer_size:\t1280\n",
      "\t  buffer_init_steps:\t0\n",
      "\t  tau:\t0.005\n",
      "\t  steps_per_update:\t10.0\n",
      "\t  save_replay_buffer:\tFalse\n",
      "\t  init_entcoef:\t0.5\n",
      "\t  reward_signal_steps_per_update:\t10.0\n",
      "\tcheckpoint_interval:\t500000\n",
      "\tnetwork_settings:\t\n",
      "\t  normalize:\tTrue\n",
      "\t  hidden_units:\t128\n",
      "\t  num_layers:\t2\n",
      "\t  vis_encode_type:\tsimple\n",
      "\t  memory:\tNone\n",
      "\t  goal_conditioning_type:\thyper\n",
      "\t  deterministic:\tFalse\n",
      "\treward_signals:\t\n",
      "\t  extrinsic:\t\n",
      "\t    gamma:\t0.99\n",
      "\t    strength:\t1.0\n",
      "\t    network_settings:\t\n",
      "\t      normalize:\tFalse\n",
      "\t      hidden_units:\t128\n",
      "\t      num_layers:\t2\n",
      "\t      vis_encode_type:\tsimple\n",
      "\t      memory:\tNone\n",
      "\t      goal_conditioning_type:\thyper\n",
      "\t      deterministic:\tFalse\n",
      "\tinit_path:\tNone\n",
      "\tkeep_checkpoints:\t5\n",
      "\teven_checkpoints:\tFalse\n",
      "\tmax_steps:\t1000000\n",
      "\ttime_horizon:\t1000\n",
      "\tsummary_freq:\t20000\n",
      "\tthreaded:\tFalse\n",
      "\tself_play:\t\n",
      "\t  save_steps:\t10000\n",
      "\t  team_change:\t40000\n",
      "\t  swap_steps:\t10000\n",
      "\t  window:\t10\n",
      "\t  play_against_latest_model_ratio:\t0.5\n",
      "\t  initial_elo:\t1200.0\n",
      "\tbehavioral_cloning:\tNone\n",
      "[INFO] Resuming from /Users/hyunjae.k/110_HyunJae_Git/2025_Playgrounds/Unity_Robotics_Playgrounds/Agent_Scripts/temp/mlagents_learn_output/Pong_Adv_SAC_POCA/Behavior_A.\n",
      "[INFO] Resuming training from step 1000549.\n",
      "^C\n",
      "Exception ignored in atexit callback: <function _exit_function at 0x16310eb90>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/multiprocessing/util.py\", line 357, in _exit_function\n",
      "    p.join()\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/multiprocessing/popen_fork.py\", line 43, in wait\n",
      "    return self.poll(os.WNOHANG if timeout == 0.0 else 0)\n",
      "  File \"/Users/hyunjae.k/anaconda3/envs/mlagents/lib/python3.10/multiprocessing/popen_fork.py\", line 27, in poll\n",
      "    pid, sts = os.waitpid(self.pid, flag)\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "# config_poca_fp = os.path.join(cur_dir, \"config\", \"Pong_adversarial_sac_poca.yaml\")\n",
    "# run_poca_id = \"Pong_Adv_SAC_POCA\"\n",
    "\n",
    "# !mlagents-learn $config_poca_fp \\\n",
    "#                --env=$env_fp \\\n",
    "#                --results-dir=$output_dir \\\n",
    "#                --run-id=$run_poca_id --base-port=$baseport --resume --inference --time-scale=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
